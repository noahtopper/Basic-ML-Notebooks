{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9847915c",
   "metadata": {},
   "source": [
    "# Credit Card Default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4cb513",
   "metadata": {},
   "source": [
    "I work with the Credit Card Default dataset to classify how risky different customers are. We have data on various attributes, plus whether the customer defaulted on their payment next month or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e27690e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_excel('data/default.xls', index_col=0, header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6b460",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d8a6f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "ID                                                                         \n",
       "1       20000    2          2         1   24      2      2     -1     -1   \n",
       "2      120000    2          2         2   26     -1      2      0      0   \n",
       "3       90000    2          2         2   34      0      0      0      0   \n",
       "4       50000    2          2         1   37      0      0      0      0   \n",
       "5       50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "    PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "ID         ...                                                                  \n",
       "1      -2  ...          0          0          0         0       689         0   \n",
       "2       0  ...       3272       3455       3261         0      1000      1000   \n",
       "3       0  ...      14331      14948      15549      1518      1500      1000   \n",
       "4       0  ...      28314      28959      29547      2000      2019      1200   \n",
       "5       0  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "    PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "ID                                                            \n",
       "1          0         0         0                           1  \n",
       "2       1000         0      2000                           1  \n",
       "3       1000      1000      5000                           0  \n",
       "4       1100      1069      1000                           0  \n",
       "5       9000       689       679                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b47cf7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   LIMIT_BAL  30000 non-null  int64\n",
      " 1   SEX        30000 non-null  int64\n",
      " 2   EDUCATION  30000 non-null  int64\n",
      " 3   MARRIAGE   30000 non-null  int64\n",
      " 4   AGE        30000 non-null  int64\n",
      " 5   PAY_0      30000 non-null  int64\n",
      " 6   PAY_2      30000 non-null  int64\n",
      " 7   PAY_3      30000 non-null  int64\n",
      " 8   PAY_4      30000 non-null  int64\n",
      " 9   PAY_5      30000 non-null  int64\n",
      " 10  PAY_6      30000 non-null  int64\n",
      " 11  BILL_AMT1  30000 non-null  int64\n",
      " 12  BILL_AMT2  30000 non-null  int64\n",
      " 13  BILL_AMT3  30000 non-null  int64\n",
      " 14  BILL_AMT4  30000 non-null  int64\n",
      " 15  BILL_AMT5  30000 non-null  int64\n",
      " 16  BILL_AMT6  30000 non-null  int64\n",
      " 17  PAY_AMT1   30000 non-null  int64\n",
      " 18  PAY_AMT2   30000 non-null  int64\n",
      " 19  PAY_AMT3   30000 non-null  int64\n",
      " 20  PAY_AMT4   30000 non-null  int64\n",
      " 21  PAY_AMT5   30000 non-null  int64\n",
      " 22  PAY_AMT6   30000 non-null  int64\n",
      " 23  default    30000 non-null  int64\n",
      "dtypes: int64(24)\n",
      "memory usage: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "data = data.rename({'default payment next month': 'default'}, axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "03cd6716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb2030a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18677\n",
       "1     5323\n",
       "Name: default, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what proportion of people default\n",
    "train['default'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254c42de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "default      1.000000\n",
       "PAY_0        0.325840\n",
       "PAY_2        0.266825\n",
       "PAY_3        0.238275\n",
       "PAY_4        0.220944\n",
       "PAY_5        0.205067\n",
       "PAY_6        0.188860\n",
       "EDUCATION    0.030152\n",
       "AGE          0.017088\n",
       "BILL_AMT6   -0.005410\n",
       "BILL_AMT5   -0.007508\n",
       "BILL_AMT4   -0.010971\n",
       "BILL_AMT2   -0.013775\n",
       "BILL_AMT3   -0.014380\n",
       "BILL_AMT1   -0.019299\n",
       "MARRIAGE    -0.023826\n",
       "SEX         -0.043076\n",
       "PAY_AMT5    -0.049705\n",
       "PAY_AMT3    -0.053817\n",
       "PAY_AMT4    -0.054048\n",
       "PAY_AMT6    -0.054989\n",
       "PAY_AMT2    -0.056479\n",
       "PAY_AMT1    -0.067933\n",
       "LIMIT_BAL   -0.154328\n",
       "Name: default, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = train.corr()\n",
    "corr_matrix['default'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f85aa081",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display some of the data distributions\n",
    "display = ['PAY_0', 'EDUCATION', 'AGE', 'BILL_AMT1', 'MARRIAGE', 'SEX', 'PAY_AMT1', 'LIMIT_BAL']\n",
    "train[display].hist(bins=60, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcde94",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "64abc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate X and y\n",
    "X = train.drop('default', axis=1)\n",
    "y = train['default'].copy()\n",
    "X_test = test.drop('default', axis=1)\n",
    "y_test = test['default'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63c12e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize attributes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdscaler = StandardScaler()\n",
    "X_prep = stdscaler.fit_transform(X)\n",
    "X_test_prep = stdscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c078638",
   "metadata": {},
   "source": [
    "## Basic ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "d42699b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36415685918116963"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base estimator. Measure f1 score, combination of precision/recall.\n",
    "from sklearn.metrics import f1_score\n",
    "pred = [1] * len(y)\n",
    "f1_score(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b5097d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7782083333333333"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate base estimator for accuracy\n",
    "pred = [0] * len(y)\n",
    "(pred == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e9e69e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3970066955494289"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm = SGDClassifier()\n",
    "svm.fit(X_prep, y)\n",
    "pred = svm.predict(X_prep)\n",
    "f1_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ebcfeafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28673621764560736, 0.15209838314345192)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation of SVM\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scorer = make_scorer(f1_score)\n",
    "svm_cv = cross_val_score(svm, X_prep, y, cv=10, scoring=f1_scorer)\n",
    "svm_cv.mean(), svm_cv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "591b90ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40969507427677876"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression classifier\n",
    "log_reg = SGDClassifier(loss='log')\n",
    "log_reg.fit(X_prep, y)\n",
    "pred = log_reg.predict(X_prep)\n",
    "f1_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "521f1ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3532325795829606, 0.07023309242949874)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation of logistic regression\n",
    "log_cv = cross_val_score(log_reg, X_prep, y, cv=10, scoring=f1_scorer)\n",
    "log_cv.mean(), log_cv.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "33673be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987783103091815"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_prep, y)\n",
    "pred = forest.predict(X_prep)\n",
    "f1_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6f8a902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4687796472014683, 0.004341792567924913)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation of random forest\n",
    "forest_cv = cross_val_score(forest, X_prep, y, cv=3, scoring=f1_scorer)\n",
    "forest_cv.mean(), forest_cv.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc7201",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "057cb17f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [150, 450], 'max_features': [None],\n",
       "                         'n_estimators': [500, 1000]},\n",
       "             scoring=make_scorer(f1_score))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest performs well, so let's do some fine tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators': [500, 1000], 'max_depth': [150, 450], 'max_features': [None]}\n",
    "forest = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(forest, params, scoring=f1_scorer, cv=2)\n",
    "grid_search.fit(X_prep, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ed95a353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 450, 'max_features': None, 'n_estimators': 1000}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8605e9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4598257502420136"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = grid_search.best_estimator_\n",
    "pred = model.predict(X_test_prep)\n",
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "dd723015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.814"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d91548c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model, open('models/forest_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a9f997",
   "metadata": {},
   "source": [
    "## NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "38e94017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fe89ae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate validation set\n",
    "train, val = train_test_split(train, test_size=0.25, random_state=seed)\n",
    "\n",
    "# Redefine X and y\n",
    "X = train.drop('default', axis=1)\n",
    "y = train['default'].copy()\n",
    "X_val = val.drop('default', axis=1)\n",
    "y_val = val['default'].copy()\n",
    "\n",
    "# Refit scaler\n",
    "stdscaler = StandardScaler()\n",
    "X_prep = stdscaler.fit_transform(X)\n",
    "X_val_prep = stdscaler.transform(X_val)\n",
    "X_test_prep = stdscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "071aea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_v = torch.tensor(X_prep, dtype=torch.float32)\n",
    "y_v = torch.tensor(y.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "X_val_v = torch.tensor(X_val_prep, dtype=torch.float32)\n",
    "y_val_v = torch.tensor(y_val.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "X_test_v = torch.tensor(X_test_prep, dtype=torch.float32)\n",
    "y_test_v = torch.tensor(y_test.to_numpy(), dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c6e710cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some networks\n",
    "class NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(23, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "        \n",
    "class NN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(23, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(23, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "566a798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "batch_size = 60\n",
    "\n",
    "model1 = NN1()\n",
    "model2 = NN2()\n",
    "model3 = NN3()\n",
    "\n",
    "optim1 = optim.Adam(model1.parameters(), weight_decay=0.0007)\n",
    "optim2 = optim.Adam(model2.parameters(), weight_decay=0.003)\n",
    "optim3 = optim.Adam(model3.parameters(), weight_decay=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4ff20bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Reset when testing new model\n",
    "model, model_name, optimizer = model4, \"model4\", optim4\n",
    "global_epoch = 0\n",
    "min_loss = None\n",
    "\n",
    "# Make summary writer\n",
    "writer = SummaryWriter(f'runs/{model_name}')\n",
    "writer.add_graph(model, X_v[0])\n",
    "writer.add_embedding(X_v, metadata=y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b99260a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DefaultDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_loader = DataLoader(DefaultDataset(X_v, y_v), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "d6d1a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(dataloader, model, loss_fn, optimizer, writer):\n",
    "    step = 0\n",
    "    avg_loss = 0.0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        # Make prediction\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print summary data\n",
    "        if (step + 1) % 100 == 0:\n",
    "            avg_loss /= 100\n",
    "            print('training loss: ', avg_loss)\n",
    "            writer.add_scalar('training loss', avg_loss, global_step = global_epoch * len(dataloader) + step)\n",
    "            avg_loss = 0.0\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9e0b162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss of model on X and y\n",
    "def test(X, y, model, loss_fn):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y).mean().item()\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "17a8b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "def val(X, y, model, loss_fn, writer, model_name):\n",
    "    loss = test(X, y, model, loss_fn)\n",
    "    print('validation loss: ', loss)\n",
    "    writer.add_scalar('validation loss', loss, global_step = global_epoch)\n",
    "    if (global_epoch + 1) % 50 == 0:\n",
    "        torch.save(model, f'models/{model_name}/{global_epoch+1}.pth')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f105ec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "training loss:  0.5131336152553558\n",
      "training loss:  0.4605822214484215\n",
      "training loss:  0.4528678126633167\n",
      "validation loss:  0.4416687786579132\n",
      "New best model!\n",
      "Epoch 2\n",
      "------------------------------\n",
      "training loss:  0.4379452282190323\n",
      "training loss:  0.4416827428340912\n",
      "training loss:  0.4524334970116615\n",
      "validation loss:  0.43904218077659607\n",
      "New best model!\n",
      "Epoch 3\n",
      "------------------------------\n",
      "training loss:  0.4360485926270485\n",
      "training loss:  0.436425464451313\n",
      "training loss:  0.4460902193188667\n",
      "validation loss:  0.43454962968826294\n",
      "New best model!\n",
      "Epoch 4\n",
      "------------------------------\n",
      "training loss:  0.42543349355459215\n",
      "training loss:  0.4506446972489357\n",
      "training loss:  0.4408260653913021\n",
      "validation loss:  0.4417099952697754\n",
      "Epoch 5\n",
      "------------------------------\n",
      "training loss:  0.44548611640930175\n",
      "training loss:  0.43347820311784746\n",
      "training loss:  0.4324962919950485\n",
      "validation loss:  0.4331521689891815\n",
      "New best model!\n",
      "Epoch 6\n",
      "------------------------------\n",
      "training loss:  0.4280900067090988\n",
      "training loss:  0.439295699596405\n",
      "training loss:  0.43875561982393263\n",
      "validation loss:  0.4373340308666229\n",
      "Epoch 7\n",
      "------------------------------\n",
      "training loss:  0.43621973156929017\n",
      "training loss:  0.43498062461614606\n",
      "training loss:  0.4369308264553547\n",
      "validation loss:  0.43950599431991577\n",
      "Epoch 8\n",
      "------------------------------\n",
      "training loss:  0.42563005089759826\n",
      "training loss:  0.4356183369457722\n",
      "training loss:  0.4459440389275551\n",
      "validation loss:  0.4353491961956024\n",
      "Epoch 9\n",
      "------------------------------\n",
      "training loss:  0.4353386142849922\n",
      "training loss:  0.43000825226306916\n",
      "training loss:  0.44311704456806184\n",
      "validation loss:  0.4334256052970886\n",
      "Epoch 10\n",
      "------------------------------\n",
      "training loss:  0.43514742523431776\n",
      "training loss:  0.4407923710346222\n",
      "training loss:  0.42604882150888446\n",
      "validation loss:  0.43496182560920715\n",
      "Epoch 11\n",
      "------------------------------\n",
      "training loss:  0.43761411026120184\n",
      "training loss:  0.4442070019245148\n",
      "training loss:  0.4217731375992298\n",
      "validation loss:  0.43303942680358887\n",
      "New best model!\n",
      "Epoch 12\n",
      "------------------------------\n",
      "training loss:  0.43396837592124937\n",
      "training loss:  0.4426241627335548\n",
      "training loss:  0.4244929480552673\n",
      "validation loss:  0.4345712959766388\n",
      "Epoch 13\n",
      "------------------------------\n",
      "training loss:  0.4375318601727486\n",
      "training loss:  0.4364707434177399\n",
      "training loss:  0.42750570982694625\n",
      "validation loss:  0.4348597526550293\n",
      "Epoch 14\n",
      "------------------------------\n",
      "training loss:  0.43594103962183\n",
      "training loss:  0.43177197873592377\n",
      "training loss:  0.4313369917869568\n",
      "validation loss:  0.4365607500076294\n",
      "Epoch 15\n",
      "------------------------------\n",
      "training loss:  0.4284083163738251\n",
      "training loss:  0.43399426847696304\n",
      "training loss:  0.4356322801113129\n",
      "validation loss:  0.4323485493659973\n",
      "New best model!\n",
      "Epoch 16\n",
      "------------------------------\n",
      "training loss:  0.43172440320253375\n",
      "training loss:  0.4286926582455635\n",
      "training loss:  0.43562479943037036\n",
      "validation loss:  0.43305703997612\n",
      "Epoch 17\n",
      "------------------------------\n",
      "training loss:  0.4354149705171585\n",
      "training loss:  0.4235737453401089\n",
      "training loss:  0.4388056269288063\n",
      "validation loss:  0.43210193514823914\n",
      "New best model!\n",
      "Epoch 18\n",
      "------------------------------\n",
      "training loss:  0.4299251276254654\n",
      "training loss:  0.440189496576786\n",
      "training loss:  0.43061149746179583\n",
      "validation loss:  0.4323776364326477\n",
      "Epoch 19\n",
      "------------------------------\n",
      "training loss:  0.446556014418602\n",
      "training loss:  0.41804056242108345\n",
      "training loss:  0.43054827660322187\n",
      "validation loss:  0.43266963958740234\n",
      "Epoch 20\n",
      "------------------------------\n",
      "training loss:  0.42229828774929046\n",
      "training loss:  0.43129059329628944\n",
      "training loss:  0.4417524552345276\n",
      "validation loss:  0.43195244669914246\n",
      "New best model!\n",
      "Epoch 21\n",
      "------------------------------\n",
      "training loss:  0.4217789500951767\n",
      "training loss:  0.44227283254265787\n",
      "training loss:  0.43726866245269774\n",
      "validation loss:  0.4356447756290436\n",
      "Epoch 22\n",
      "------------------------------\n",
      "training loss:  0.4327492454648018\n",
      "training loss:  0.4391606709361076\n",
      "training loss:  0.42451584815979004\n",
      "validation loss:  0.43362000584602356\n",
      "Epoch 23\n",
      "------------------------------\n",
      "training loss:  0.42807651549577713\n",
      "training loss:  0.4431404957175255\n",
      "training loss:  0.4269927316904068\n",
      "validation loss:  0.4320828318595886\n",
      "Epoch 24\n",
      "------------------------------\n",
      "training loss:  0.43444502890110015\n",
      "training loss:  0.429450995028019\n",
      "training loss:  0.4313999380171299\n",
      "validation loss:  0.4356926381587982\n",
      "Epoch 25\n",
      "------------------------------\n",
      "training loss:  0.43436496406793595\n",
      "training loss:  0.43352997571229934\n",
      "training loss:  0.4291735726594925\n",
      "validation loss:  0.4323711395263672\n",
      "Epoch 26\n",
      "------------------------------\n",
      "training loss:  0.43221417829394343\n",
      "training loss:  0.43212810933589935\n",
      "training loss:  0.4315584692358971\n",
      "validation loss:  0.4313165545463562\n",
      "New best model!\n",
      "Epoch 27\n",
      "------------------------------\n",
      "training loss:  0.4285520792007446\n",
      "training loss:  0.4401227322220802\n",
      "training loss:  0.4272558829188347\n",
      "validation loss:  0.43239331245422363\n",
      "Epoch 28\n",
      "------------------------------\n",
      "training loss:  0.43660193860530855\n",
      "training loss:  0.42217457488179205\n",
      "training loss:  0.4370497342944145\n",
      "validation loss:  0.438198983669281\n",
      "Epoch 29\n",
      "------------------------------\n",
      "training loss:  0.4440132799744606\n",
      "training loss:  0.4264592406153679\n",
      "training loss:  0.42254862785339353\n",
      "validation loss:  0.43112829327583313\n",
      "New best model!\n",
      "Epoch 30\n",
      "------------------------------\n",
      "training loss:  0.43134326964616776\n",
      "training loss:  0.4238250866532326\n",
      "training loss:  0.438072592318058\n",
      "validation loss:  0.43202149868011475\n",
      "Epoch 31\n",
      "------------------------------\n",
      "training loss:  0.43072627574205397\n",
      "training loss:  0.43350461885333064\n",
      "training loss:  0.4312679441273212\n",
      "validation loss:  0.43198704719543457\n",
      "Epoch 32\n",
      "------------------------------\n",
      "training loss:  0.42430725902318955\n",
      "training loss:  0.44127184092998506\n",
      "training loss:  0.4267971301078796\n",
      "validation loss:  0.43113330006599426\n",
      "Epoch 33\n",
      "------------------------------\n",
      "training loss:  0.43008803129196166\n",
      "training loss:  0.4410396152734756\n",
      "training loss:  0.4248173767328262\n",
      "validation loss:  0.43512699007987976\n",
      "Epoch 34\n",
      "------------------------------\n",
      "training loss:  0.43491793632507325\n",
      "training loss:  0.42240231022238733\n",
      "training loss:  0.43600465342402456\n",
      "validation loss:  0.4328795075416565\n",
      "Epoch 35\n",
      "------------------------------\n",
      "training loss:  0.42487114027142525\n",
      "training loss:  0.4369547155499458\n",
      "training loss:  0.4313238215446472\n",
      "validation loss:  0.43091753125190735\n",
      "New best model!\n",
      "Epoch 36\n",
      "------------------------------\n",
      "training loss:  0.42721841171383856\n",
      "training loss:  0.4287604171037674\n",
      "training loss:  0.43781880974769594\n",
      "validation loss:  0.43290039896965027\n",
      "Epoch 37\n",
      "------------------------------\n",
      "training loss:  0.4280547919869423\n",
      "training loss:  0.43467469394207003\n",
      "training loss:  0.4297285306453705\n",
      "validation loss:  0.4311245381832123\n",
      "Epoch 38\n",
      "------------------------------\n",
      "training loss:  0.426143501996994\n",
      "training loss:  0.42441969096660614\n",
      "training loss:  0.44307646721601485\n",
      "validation loss:  0.4371401369571686\n",
      "Epoch 39\n",
      "------------------------------\n",
      "training loss:  0.4221714860200882\n",
      "training loss:  0.42596359640359877\n",
      "training loss:  0.44347617447376253\n",
      "validation loss:  0.43107953667640686\n",
      "Epoch 40\n",
      "------------------------------\n",
      "training loss:  0.43472989201545714\n",
      "training loss:  0.42623568773269654\n",
      "training loss:  0.43208395451307297\n",
      "validation loss:  0.4343850910663605\n",
      "Epoch 41\n",
      "------------------------------\n",
      "training loss:  0.42773531839251516\n",
      "training loss:  0.4387356248497963\n",
      "training loss:  0.4244034762680531\n",
      "validation loss:  0.4312876760959625\n",
      "Epoch 42\n",
      "------------------------------\n",
      "training loss:  0.4324643236398697\n",
      "training loss:  0.43196591317653654\n",
      "training loss:  0.42870340913534166\n",
      "validation loss:  0.43128541111946106\n",
      "Epoch 43\n",
      "------------------------------\n",
      "training loss:  0.43319072887301446\n",
      "training loss:  0.4341272032260895\n",
      "training loss:  0.4259077748656273\n",
      "validation loss:  0.43187686800956726\n",
      "Epoch 44\n",
      "------------------------------\n",
      "training loss:  0.42785826086997986\n",
      "training loss:  0.4311133202910423\n",
      "training loss:  0.43332740604877473\n",
      "validation loss:  0.43133169412612915\n",
      "Epoch 45\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.43814634025096894\n",
      "training loss:  0.4258007001876831\n",
      "training loss:  0.42651524424552917\n",
      "validation loss:  0.4351569712162018\n",
      "Epoch 46\n",
      "------------------------------\n",
      "training loss:  0.426536930501461\n",
      "training loss:  0.42879176437854766\n",
      "training loss:  0.4352897107601166\n",
      "validation loss:  0.4353019893169403\n",
      "Epoch 47\n",
      "------------------------------\n",
      "training loss:  0.43179694697260856\n",
      "training loss:  0.43040106147527696\n",
      "training loss:  0.4287233552336693\n",
      "validation loss:  0.43206846714019775\n",
      "Epoch 48\n",
      "------------------------------\n",
      "training loss:  0.42977029353380203\n",
      "training loss:  0.43632405310869216\n",
      "training loss:  0.42176035210490226\n",
      "validation loss:  0.4329625368118286\n",
      "Epoch 49\n",
      "------------------------------\n",
      "training loss:  0.4300330296158791\n",
      "training loss:  0.4321878665685654\n",
      "training loss:  0.42922778487205504\n",
      "validation loss:  0.4309787154197693\n",
      "Epoch 50\n",
      "------------------------------\n",
      "training loss:  0.420199539065361\n",
      "training loss:  0.4381173247098923\n",
      "training loss:  0.42978952527046205\n",
      "validation loss:  0.431399405002594\n",
      "Epoch 51\n",
      "------------------------------\n",
      "training loss:  0.4192884647846222\n",
      "training loss:  0.4289469866454601\n",
      "training loss:  0.44078926295042037\n",
      "validation loss:  0.43392765522003174\n",
      "Epoch 52\n",
      "------------------------------\n",
      "training loss:  0.4355784945189953\n",
      "training loss:  0.42542309492826463\n",
      "training loss:  0.42874813616275786\n",
      "validation loss:  0.4310920834541321\n",
      "Epoch 53\n",
      "------------------------------\n",
      "training loss:  0.4297371059656143\n",
      "training loss:  0.4363228017091751\n",
      "training loss:  0.42394870072603225\n",
      "validation loss:  0.43028759956359863\n",
      "New best model!\n",
      "Epoch 54\n",
      "------------------------------\n",
      "training loss:  0.43070400416851046\n",
      "training loss:  0.4322903245687485\n",
      "training loss:  0.4246018892526627\n",
      "validation loss:  0.4322703182697296\n",
      "Epoch 55\n",
      "------------------------------\n",
      "training loss:  0.4325497668981552\n",
      "training loss:  0.4314529076218605\n",
      "training loss:  0.4257482287287712\n",
      "validation loss:  0.431179940700531\n",
      "Epoch 56\n",
      "------------------------------\n",
      "training loss:  0.4264837847650051\n",
      "training loss:  0.42668806463479997\n",
      "training loss:  0.4360606801509857\n",
      "validation loss:  0.4311453402042389\n",
      "Epoch 57\n",
      "------------------------------\n",
      "training loss:  0.4279913550615311\n",
      "training loss:  0.4286620482802391\n",
      "training loss:  0.4321483752131462\n",
      "validation loss:  0.43202540278434753\n",
      "Epoch 58\n",
      "------------------------------\n",
      "training loss:  0.43345759391784666\n",
      "training loss:  0.42548947155475614\n",
      "training loss:  0.427667875289917\n",
      "validation loss:  0.43576738238334656\n",
      "Epoch 59\n",
      "------------------------------\n",
      "training loss:  0.42418507248163223\n",
      "training loss:  0.4328642240166664\n",
      "training loss:  0.42947946637868883\n",
      "validation loss:  0.43442603945732117\n",
      "Epoch 60\n",
      "------------------------------\n",
      "training loss:  0.4394211122393608\n",
      "training loss:  0.4335212418437004\n",
      "training loss:  0.4160035087168217\n",
      "validation loss:  0.4320215582847595\n",
      "Epoch 61\n",
      "------------------------------\n",
      "training loss:  0.42904423117637636\n",
      "training loss:  0.4284037643671036\n",
      "training loss:  0.4321486222743988\n",
      "validation loss:  0.4315502941608429\n",
      "Epoch 62\n",
      "------------------------------\n",
      "training loss:  0.4212533852458\n",
      "training loss:  0.440484254360199\n",
      "training loss:  0.42775563716888426\n",
      "validation loss:  0.43289971351623535\n",
      "Epoch 63\n",
      "------------------------------\n",
      "training loss:  0.4305189448595047\n",
      "training loss:  0.4227278596162796\n",
      "training loss:  0.4325332251191139\n",
      "validation loss:  0.43458330631256104\n",
      "Epoch 64\n",
      "------------------------------\n",
      "training loss:  0.43220979601144793\n",
      "training loss:  0.4291138195991516\n",
      "training loss:  0.4269977942109108\n",
      "validation loss:  0.4326215088367462\n",
      "Epoch 65\n",
      "------------------------------\n",
      "training loss:  0.42497474074363706\n",
      "training loss:  0.43417981088161467\n",
      "training loss:  0.4259712806344032\n",
      "validation loss:  0.43263277411460876\n",
      "Epoch 66\n",
      "------------------------------\n",
      "training loss:  0.42557486951351164\n",
      "training loss:  0.43498539879918097\n",
      "training loss:  0.4264805144071579\n",
      "validation loss:  0.4334155321121216\n",
      "Epoch 67\n",
      "------------------------------\n",
      "training loss:  0.42859256625175474\n",
      "training loss:  0.4276432108879089\n",
      "training loss:  0.43012540027499196\n",
      "validation loss:  0.4332212209701538\n",
      "Epoch 68\n",
      "------------------------------\n",
      "training loss:  0.4261030071973801\n",
      "training loss:  0.4249225196242332\n",
      "training loss:  0.4387740296125412\n",
      "validation loss:  0.4306459128856659\n",
      "Epoch 69\n",
      "------------------------------\n",
      "training loss:  0.42815265238285066\n",
      "training loss:  0.42455887973308565\n",
      "training loss:  0.4347551403939724\n",
      "validation loss:  0.4330856204032898\n",
      "Epoch 70\n",
      "------------------------------\n",
      "training loss:  0.42536303639411926\n",
      "training loss:  0.44413300931453703\n",
      "training loss:  0.41947048097848894\n",
      "validation loss:  0.4307096302509308\n",
      "Epoch 71\n",
      "------------------------------\n",
      "training loss:  0.4260068118572235\n",
      "training loss:  0.43021155521273613\n",
      "training loss:  0.43116833493113516\n",
      "validation loss:  0.43177905678749084\n",
      "Epoch 72\n",
      "------------------------------\n",
      "training loss:  0.4168416446447372\n",
      "training loss:  0.4275852653384209\n",
      "training loss:  0.4397026631236076\n",
      "validation loss:  0.4304574728012085\n",
      "Epoch 73\n",
      "------------------------------\n",
      "training loss:  0.42888855561614037\n",
      "training loss:  0.4249877655506134\n",
      "training loss:  0.4333065037429333\n",
      "validation loss:  0.4305258095264435\n",
      "Epoch 74\n",
      "------------------------------\n",
      "training loss:  0.43153460770845414\n",
      "training loss:  0.4342476344108582\n",
      "training loss:  0.42127894654870035\n",
      "validation loss:  0.4327673316001892\n",
      "Epoch 75\n",
      "------------------------------\n",
      "training loss:  0.4340469628572464\n",
      "training loss:  0.4282968835532665\n",
      "training loss:  0.4235941365361214\n",
      "validation loss:  0.43563398718833923\n",
      "Epoch 76\n",
      "------------------------------\n",
      "training loss:  0.43199535012245177\n",
      "training loss:  0.43206633925437926\n",
      "training loss:  0.42083060622215274\n",
      "validation loss:  0.4323069751262665\n",
      "Epoch 77\n",
      "------------------------------\n",
      "training loss:  0.4245978966355324\n",
      "training loss:  0.42116629987955095\n",
      "training loss:  0.4410808491706848\n",
      "validation loss:  0.4316783547401428\n",
      "Epoch 78\n",
      "------------------------------\n",
      "training loss:  0.44061374962329863\n",
      "training loss:  0.4219179855287075\n",
      "training loss:  0.42292937487363813\n",
      "validation loss:  0.4330870807170868\n",
      "Epoch 79\n",
      "------------------------------\n",
      "training loss:  0.4147969400882721\n",
      "training loss:  0.43346445441246034\n",
      "training loss:  0.43848603665828706\n",
      "validation loss:  0.43095552921295166\n",
      "Epoch 80\n",
      "------------------------------\n",
      "training loss:  0.4186183866858482\n",
      "training loss:  0.4324066174030304\n",
      "training loss:  0.43588000372052194\n",
      "validation loss:  0.43009790778160095\n",
      "New best model!\n",
      "Epoch 81\n",
      "------------------------------\n",
      "training loss:  0.42626567512750624\n",
      "training loss:  0.4198382368683815\n",
      "training loss:  0.437860279083252\n",
      "validation loss:  0.4308509826660156\n",
      "Epoch 82\n",
      "------------------------------\n",
      "training loss:  0.4280873572826385\n",
      "training loss:  0.44129817485809325\n",
      "training loss:  0.41397245779633524\n",
      "validation loss:  0.4303194582462311\n",
      "Epoch 83\n",
      "------------------------------\n",
      "training loss:  0.4381333637237549\n",
      "training loss:  0.41900769650936126\n",
      "training loss:  0.42585898652672766\n",
      "validation loss:  0.4303989112377167\n",
      "Epoch 84\n",
      "------------------------------\n",
      "training loss:  0.42027811586856845\n",
      "training loss:  0.4453562259674072\n",
      "training loss:  0.41612583518028257\n",
      "validation loss:  0.43338149785995483\n",
      "Epoch 85\n",
      "------------------------------\n",
      "training loss:  0.42539709955453875\n",
      "training loss:  0.43448499381542205\n",
      "training loss:  0.4242212030291557\n",
      "validation loss:  0.43662458658218384\n",
      "Epoch 86\n",
      "------------------------------\n",
      "training loss:  0.4275710305571556\n",
      "training loss:  0.4359441924095154\n",
      "training loss:  0.4237954929471016\n",
      "validation loss:  0.4314723014831543\n",
      "Epoch 87\n",
      "------------------------------\n",
      "training loss:  0.41719981133937833\n",
      "training loss:  0.4347026914358139\n",
      "training loss:  0.4338275745511055\n",
      "validation loss:  0.4322524070739746\n",
      "Epoch 88\n",
      "------------------------------\n",
      "training loss:  0.43872044295072554\n",
      "training loss:  0.43205815702676775\n",
      "training loss:  0.41300751611590386\n",
      "validation loss:  0.42979639768600464\n",
      "New best model!\n",
      "Epoch 89\n",
      "------------------------------\n",
      "training loss:  0.43144155651330945\n",
      "training loss:  0.42877415150403975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.4245219668745995\n",
      "validation loss:  0.43513932824134827\n",
      "Epoch 90\n",
      "------------------------------\n",
      "training loss:  0.4251793004572392\n",
      "training loss:  0.4307211223244667\n",
      "training loss:  0.43191444903612136\n",
      "validation loss:  0.4333178699016571\n",
      "Epoch 91\n",
      "------------------------------\n",
      "training loss:  0.4309960696101189\n",
      "training loss:  0.4203356373310089\n",
      "training loss:  0.43183033406734467\n",
      "validation loss:  0.4319589138031006\n",
      "Epoch 92\n",
      "------------------------------\n",
      "training loss:  0.4265703508257866\n",
      "training loss:  0.4354709222912788\n",
      "training loss:  0.42338361233472827\n",
      "validation loss:  0.4312395751476288\n",
      "Epoch 93\n",
      "------------------------------\n",
      "training loss:  0.4272998321056366\n",
      "training loss:  0.4227337191998959\n",
      "training loss:  0.4339052802324295\n",
      "validation loss:  0.43008989095687866\n",
      "Epoch 94\n",
      "------------------------------\n",
      "training loss:  0.4303314259648323\n",
      "training loss:  0.42572491586208344\n",
      "training loss:  0.4271940341591835\n",
      "validation loss:  0.43061962723731995\n",
      "Epoch 95\n",
      "------------------------------\n",
      "training loss:  0.4313643991947174\n",
      "training loss:  0.4283032229542732\n",
      "training loss:  0.42490401238203046\n",
      "validation loss:  0.4320044219493866\n",
      "Epoch 96\n",
      "------------------------------\n",
      "training loss:  0.42789023607969284\n",
      "training loss:  0.4258828321099281\n",
      "training loss:  0.42924218252301216\n",
      "validation loss:  0.43061336874961853\n",
      "Epoch 97\n",
      "------------------------------\n",
      "training loss:  0.43625463485717775\n",
      "training loss:  0.41640987753868103\n",
      "training loss:  0.43166608735919\n",
      "validation loss:  0.43023890256881714\n",
      "Epoch 98\n",
      "------------------------------\n",
      "training loss:  0.4310802453756332\n",
      "training loss:  0.4198372703790665\n",
      "training loss:  0.43069886058568957\n",
      "validation loss:  0.43026748299598694\n",
      "Epoch 99\n",
      "------------------------------\n",
      "training loss:  0.4235879415273666\n",
      "training loss:  0.42992672517895697\n",
      "training loss:  0.4310507008433342\n",
      "validation loss:  0.43286311626434326\n",
      "Epoch 100\n",
      "------------------------------\n",
      "training loss:  0.42277621522545816\n",
      "training loss:  0.4275422087311745\n",
      "training loss:  0.4319923651218414\n",
      "validation loss:  0.4305949807167053\n",
      "Epoch 101\n",
      "------------------------------\n",
      "training loss:  0.42774845704436304\n",
      "training loss:  0.4140441319346428\n",
      "training loss:  0.4408764430880547\n",
      "validation loss:  0.43423840403556824\n",
      "Epoch 102\n",
      "------------------------------\n",
      "training loss:  0.4211389926075935\n",
      "training loss:  0.4237226256728172\n",
      "training loss:  0.43939441949129104\n",
      "validation loss:  0.4396804869174957\n",
      "Epoch 103\n",
      "------------------------------\n",
      "training loss:  0.42837216824293134\n",
      "training loss:  0.4287525799870491\n",
      "training loss:  0.4289656412601471\n",
      "validation loss:  0.43061116337776184\n",
      "Epoch 104\n",
      "------------------------------\n",
      "training loss:  0.4241748911142349\n",
      "training loss:  0.4267978703975677\n",
      "training loss:  0.43247463792562485\n",
      "validation loss:  0.4303576946258545\n",
      "Epoch 105\n",
      "------------------------------\n",
      "training loss:  0.4186993378400803\n",
      "training loss:  0.4227009218931198\n",
      "training loss:  0.44008147835731504\n",
      "validation loss:  0.4306935667991638\n",
      "Epoch 106\n",
      "------------------------------\n",
      "training loss:  0.43133249282836916\n",
      "training loss:  0.4242946487665176\n",
      "training loss:  0.4272818998992443\n",
      "validation loss:  0.4354192018508911\n",
      "Epoch 107\n",
      "------------------------------\n",
      "training loss:  0.42046045437455176\n",
      "training loss:  0.4290685245394707\n",
      "training loss:  0.4327093943953514\n",
      "validation loss:  0.43093737959861755\n",
      "Epoch 108\n",
      "------------------------------\n",
      "training loss:  0.42230734378099444\n",
      "training loss:  0.4296503928303719\n",
      "training loss:  0.4314653940498829\n",
      "validation loss:  0.4294988512992859\n",
      "New best model!\n",
      "Epoch 109\n",
      "------------------------------\n",
      "training loss:  0.4313725383579731\n",
      "training loss:  0.4245937956869602\n",
      "training loss:  0.4282406958937645\n",
      "validation loss:  0.4303264915943146\n",
      "Epoch 110\n",
      "------------------------------\n",
      "training loss:  0.41608512461185454\n",
      "training loss:  0.43458053410053255\n",
      "training loss:  0.4340639814734459\n",
      "validation loss:  0.429170161485672\n",
      "New best model!\n",
      "Epoch 111\n",
      "------------------------------\n",
      "training loss:  0.4286665067076683\n",
      "training loss:  0.42590093970298765\n",
      "training loss:  0.4296510824561119\n",
      "validation loss:  0.43003302812576294\n",
      "Epoch 112\n",
      "------------------------------\n",
      "training loss:  0.43284926265478135\n",
      "training loss:  0.42168342888355254\n",
      "training loss:  0.4278108295798302\n",
      "validation loss:  0.4308570623397827\n",
      "Epoch 113\n",
      "------------------------------\n",
      "training loss:  0.42885636746883393\n",
      "training loss:  0.4256355279684067\n",
      "training loss:  0.4276491731405258\n",
      "validation loss:  0.4309258759021759\n",
      "Epoch 114\n",
      "------------------------------\n",
      "training loss:  0.4198271459341049\n",
      "training loss:  0.4361340269446373\n",
      "training loss:  0.42679770171642306\n",
      "validation loss:  0.4339667856693268\n",
      "Epoch 115\n",
      "------------------------------\n",
      "training loss:  0.42007026821374893\n",
      "training loss:  0.4376476019620895\n",
      "training loss:  0.4261044874787331\n",
      "validation loss:  0.43196865916252136\n",
      "Epoch 116\n",
      "------------------------------\n",
      "training loss:  0.42274832919239996\n",
      "training loss:  0.4277725011110306\n",
      "training loss:  0.432479213476181\n",
      "validation loss:  0.43137189745903015\n",
      "Epoch 117\n",
      "------------------------------\n",
      "training loss:  0.4266268345713615\n",
      "training loss:  0.4300487196445465\n",
      "training loss:  0.4267048504948616\n",
      "validation loss:  0.4319380223751068\n",
      "Epoch 118\n",
      "------------------------------\n",
      "training loss:  0.42693252116441727\n",
      "training loss:  0.42388581961393357\n",
      "training loss:  0.43046172261238097\n",
      "validation loss:  0.43027397990226746\n",
      "Epoch 119\n",
      "------------------------------\n",
      "training loss:  0.421962656378746\n",
      "training loss:  0.4300039178133011\n",
      "training loss:  0.4306431970000267\n",
      "validation loss:  0.4313233196735382\n",
      "Epoch 120\n",
      "------------------------------\n",
      "training loss:  0.4240022024512291\n",
      "training loss:  0.4234030795097351\n",
      "training loss:  0.4336691176891327\n",
      "validation loss:  0.4306508004665375\n",
      "Epoch 121\n",
      "------------------------------\n",
      "training loss:  0.42697568476200104\n",
      "training loss:  0.4338626018166542\n",
      "training loss:  0.4220338109135628\n",
      "validation loss:  0.43151867389678955\n",
      "Epoch 122\n",
      "------------------------------\n",
      "training loss:  0.42670161187648775\n",
      "training loss:  0.42975601196289065\n",
      "training loss:  0.4264831057190895\n",
      "validation loss:  0.4347195625305176\n",
      "Epoch 123\n",
      "------------------------------\n",
      "training loss:  0.4289194929599762\n",
      "training loss:  0.42224821865558626\n",
      "training loss:  0.4307334116101265\n",
      "validation loss:  0.43092113733291626\n",
      "Epoch 124\n",
      "------------------------------\n",
      "training loss:  0.4300061473250389\n",
      "training loss:  0.43293349355459215\n",
      "training loss:  0.4210169979929924\n",
      "validation loss:  0.431926429271698\n",
      "Epoch 125\n",
      "------------------------------\n",
      "training loss:  0.42813279062509535\n",
      "training loss:  0.43707371652126314\n",
      "training loss:  0.41610709965229037\n",
      "validation loss:  0.4315696656703949\n",
      "Epoch 126\n",
      "------------------------------\n",
      "training loss:  0.42808799594640734\n",
      "training loss:  0.4259835359454155\n",
      "training loss:  0.4269513013958931\n",
      "validation loss:  0.43167853355407715\n",
      "Epoch 127\n",
      "------------------------------\n",
      "training loss:  0.4358470547199249\n",
      "training loss:  0.4262512697279453\n",
      "training loss:  0.4199635538458824\n",
      "validation loss:  0.43111538887023926\n",
      "Epoch 128\n",
      "------------------------------\n",
      "training loss:  0.4248716673254967\n",
      "training loss:  0.43668250501155853\n",
      "training loss:  0.4214565882086754\n",
      "validation loss:  0.4302685856819153\n",
      "Epoch 129\n",
      "------------------------------\n",
      "training loss:  0.43061640590429306\n",
      "training loss:  0.42893442898988726\n",
      "training loss:  0.42237631320953367\n",
      "validation loss:  0.4316158890724182\n",
      "Epoch 130\n",
      "------------------------------\n",
      "training loss:  0.42687861412763595\n",
      "training loss:  0.4201984645426273\n",
      "training loss:  0.43477261930704114\n",
      "validation loss:  0.4339326024055481\n",
      "Epoch 131\n",
      "------------------------------\n",
      "training loss:  0.4231113690137863\n",
      "training loss:  0.42696028500795363\n",
      "training loss:  0.43286176592111586\n",
      "validation loss:  0.43012261390686035\n",
      "Epoch 132\n",
      "------------------------------\n",
      "training loss:  0.42775094777345657\n",
      "training loss:  0.4293132743239403\n",
      "training loss:  0.42501568734645845\n",
      "validation loss:  0.43377962708473206\n",
      "Epoch 133\n",
      "------------------------------\n",
      "training loss:  0.41594252675771715\n",
      "training loss:  0.4277870643138886\n",
      "training loss:  0.4376951092481613\n",
      "validation loss:  0.43072888255119324\n",
      "Epoch 134\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.42690144419670106\n",
      "training loss:  0.42969196185469627\n",
      "training loss:  0.4263673235476017\n",
      "validation loss:  0.4320441484451294\n",
      "Epoch 135\n",
      "------------------------------\n",
      "training loss:  0.420484419465065\n",
      "training loss:  0.43273196667432784\n",
      "training loss:  0.4287589916586876\n",
      "validation loss:  0.43057146668434143\n",
      "Epoch 136\n",
      "------------------------------\n",
      "training loss:  0.41882643699645994\n",
      "training loss:  0.4323742035031319\n",
      "training loss:  0.4301782567799091\n",
      "validation loss:  0.4297342896461487\n",
      "Epoch 137\n",
      "------------------------------\n",
      "training loss:  0.42375497505068777\n",
      "training loss:  0.4301260522007942\n",
      "training loss:  0.42847308456897737\n",
      "validation loss:  0.43115413188934326\n",
      "Epoch 138\n",
      "------------------------------\n",
      "training loss:  0.4333510193228722\n",
      "training loss:  0.41975863635540006\n",
      "training loss:  0.4281419107317925\n",
      "validation loss:  0.43124571442604065\n",
      "Epoch 139\n",
      "------------------------------\n",
      "training loss:  0.4201006814837456\n",
      "training loss:  0.44014970302581785\n",
      "training loss:  0.4216534920036793\n",
      "validation loss:  0.43054232001304626\n",
      "Epoch 140\n",
      "------------------------------\n",
      "training loss:  0.42351496130228045\n",
      "training loss:  0.4395373350381851\n",
      "training loss:  0.4183080494403839\n",
      "validation loss:  0.43155184388160706\n",
      "Epoch 141\n",
      "------------------------------\n",
      "training loss:  0.4214079770445824\n",
      "training loss:  0.4306098708510399\n",
      "training loss:  0.4312495878338814\n",
      "validation loss:  0.43044552206993103\n",
      "Epoch 142\n",
      "------------------------------\n",
      "training loss:  0.4224057576060295\n",
      "training loss:  0.42941911399364474\n",
      "training loss:  0.43021971195936204\n",
      "validation loss:  0.430232435464859\n",
      "Epoch 143\n",
      "------------------------------\n",
      "training loss:  0.4217245078086853\n",
      "training loss:  0.42838368922472\n",
      "training loss:  0.43118965059518816\n",
      "validation loss:  0.4292566776275635\n",
      "Epoch 144\n",
      "------------------------------\n",
      "training loss:  0.42709638208150863\n",
      "training loss:  0.42598425000905993\n",
      "training loss:  0.4278747805953026\n",
      "validation loss:  0.4332696795463562\n",
      "Epoch 145\n",
      "------------------------------\n",
      "training loss:  0.41928122609853746\n",
      "training loss:  0.4191471128165722\n",
      "training loss:  0.4426599928736687\n",
      "validation loss:  0.4300667643547058\n",
      "Epoch 146\n",
      "------------------------------\n",
      "training loss:  0.4303927408158779\n",
      "training loss:  0.42406723022460935\n",
      "training loss:  0.42601064369082453\n",
      "validation loss:  0.4304368793964386\n",
      "Epoch 147\n",
      "------------------------------\n",
      "training loss:  0.42070028245449065\n",
      "training loss:  0.42348652780056\n",
      "training loss:  0.438681920170784\n",
      "validation loss:  0.4316372871398926\n",
      "Epoch 148\n",
      "------------------------------\n",
      "training loss:  0.4255016887187958\n",
      "training loss:  0.4299914059042931\n",
      "training loss:  0.42619627952575684\n",
      "validation loss:  0.43203720450401306\n",
      "Epoch 149\n",
      "------------------------------\n",
      "training loss:  0.41692579194903373\n",
      "training loss:  0.4270581275224686\n",
      "training loss:  0.43625451639294627\n",
      "validation loss:  0.4316389262676239\n",
      "Epoch 150\n",
      "------------------------------\n",
      "training loss:  0.4243514296412468\n",
      "training loss:  0.4244204151630402\n",
      "training loss:  0.43281893789768217\n",
      "validation loss:  0.4298875033855438\n",
      "Epoch 151\n",
      "------------------------------\n",
      "training loss:  0.42442760527133944\n",
      "training loss:  0.4254793255031109\n",
      "training loss:  0.4295820236206055\n",
      "validation loss:  0.43076762557029724\n",
      "Epoch 152\n",
      "------------------------------\n",
      "training loss:  0.43557839184999464\n",
      "training loss:  0.41959628731012344\n",
      "training loss:  0.42628972306847573\n",
      "validation loss:  0.430644154548645\n",
      "Epoch 153\n",
      "------------------------------\n",
      "training loss:  0.42290399968624115\n",
      "training loss:  0.4260020869970322\n",
      "training loss:  0.4330717211961746\n",
      "validation loss:  0.43123161792755127\n",
      "Epoch 154\n",
      "------------------------------\n",
      "training loss:  0.42841632664203644\n",
      "training loss:  0.42557335138320923\n",
      "training loss:  0.42781789898872374\n",
      "validation loss:  0.4302014112472534\n",
      "Epoch 155\n",
      "------------------------------\n",
      "training loss:  0.41784995555877685\n",
      "training loss:  0.41873177886009216\n",
      "training loss:  0.44217415660619735\n",
      "validation loss:  0.4317675828933716\n",
      "Epoch 156\n",
      "------------------------------\n",
      "training loss:  0.4242332883179188\n",
      "training loss:  0.4241511383652687\n",
      "training loss:  0.4322939822077751\n",
      "validation loss:  0.4304313063621521\n",
      "Epoch 157\n",
      "------------------------------\n",
      "training loss:  0.41946860194206237\n",
      "training loss:  0.43287369012832644\n",
      "training loss:  0.42673885703086856\n",
      "validation loss:  0.432647168636322\n",
      "Epoch 158\n",
      "------------------------------\n",
      "training loss:  0.4250306694209576\n",
      "training loss:  0.437345951795578\n",
      "training loss:  0.41942574620246886\n",
      "validation loss:  0.4317912459373474\n",
      "Epoch 159\n",
      "------------------------------\n",
      "training loss:  0.4272082009911537\n",
      "training loss:  0.426955324113369\n",
      "training loss:  0.4297524243593216\n",
      "validation loss:  0.4313422739505768\n",
      "Epoch 160\n",
      "------------------------------\n",
      "training loss:  0.423262320458889\n",
      "training loss:  0.4317679587006569\n",
      "training loss:  0.4254990294575691\n",
      "validation loss:  0.42986372113227844\n",
      "Epoch 161\n",
      "------------------------------\n",
      "training loss:  0.4198495063185692\n",
      "training loss:  0.4329689919948578\n",
      "training loss:  0.42951776564121247\n",
      "validation loss:  0.4305938184261322\n",
      "Epoch 162\n",
      "------------------------------\n",
      "training loss:  0.4185709366202354\n",
      "training loss:  0.43261819273233415\n",
      "training loss:  0.42999108612537384\n",
      "validation loss:  0.4299650192260742\n",
      "Epoch 163\n",
      "------------------------------\n",
      "training loss:  0.421221399307251\n",
      "training loss:  0.4218343269824982\n",
      "training loss:  0.4371194364130497\n",
      "validation loss:  0.43059343099594116\n",
      "Epoch 164\n",
      "------------------------------\n",
      "training loss:  0.43200474351644513\n",
      "training loss:  0.4355513146519661\n",
      "training loss:  0.4126607218384743\n",
      "validation loss:  0.43066543340682983\n",
      "Epoch 165\n",
      "------------------------------\n",
      "training loss:  0.42661721974611283\n",
      "training loss:  0.4258417382836342\n",
      "training loss:  0.42998301416635515\n",
      "validation loss:  0.4301733374595642\n",
      "Epoch 166\n",
      "------------------------------\n",
      "training loss:  0.43422127395868304\n",
      "training loss:  0.42243080109357833\n",
      "training loss:  0.42130766838788986\n",
      "validation loss:  0.4315243065357208\n",
      "Epoch 167\n",
      "------------------------------\n",
      "training loss:  0.43391927182674406\n",
      "training loss:  0.4237336391210556\n",
      "training loss:  0.42259002938866613\n",
      "validation loss:  0.43033337593078613\n",
      "Epoch 168\n",
      "------------------------------\n",
      "training loss:  0.42245265692472456\n",
      "training loss:  0.4257818776369095\n",
      "training loss:  0.43145627558231353\n",
      "validation loss:  0.4328973889350891\n",
      "Epoch 169\n",
      "------------------------------\n",
      "training loss:  0.42877618968486786\n",
      "training loss:  0.4293102790415287\n",
      "training loss:  0.4230039492249489\n",
      "validation loss:  0.43014299869537354\n",
      "Epoch 170\n",
      "------------------------------\n",
      "training loss:  0.42155381351709365\n",
      "training loss:  0.42600748434662816\n",
      "training loss:  0.43215678244829175\n",
      "validation loss:  0.43779703974723816\n",
      "Epoch 171\n",
      "------------------------------\n",
      "training loss:  0.4182676374912262\n",
      "training loss:  0.4326705092191696\n",
      "training loss:  0.4300616002082825\n",
      "validation loss:  0.4302772283554077\n",
      "Epoch 172\n",
      "------------------------------\n",
      "training loss:  0.4304559987783432\n",
      "training loss:  0.42864058166742325\n",
      "training loss:  0.4188263018429279\n",
      "validation loss:  0.4313490390777588\n",
      "Epoch 173\n",
      "------------------------------\n",
      "training loss:  0.43096586644649504\n",
      "training loss:  0.4247429823875427\n",
      "training loss:  0.42381489649415016\n",
      "validation loss:  0.4333829879760742\n",
      "Epoch 174\n",
      "------------------------------\n",
      "training loss:  0.42449078530073164\n",
      "training loss:  0.4225605806708336\n",
      "training loss:  0.43471746116876603\n",
      "validation loss:  0.43117383122444153\n",
      "Epoch 175\n",
      "------------------------------\n",
      "training loss:  0.42526829689741136\n",
      "training loss:  0.4300354114174843\n",
      "training loss:  0.4234729152917862\n",
      "validation loss:  0.42926985025405884\n",
      "Epoch 176\n",
      "------------------------------\n",
      "training loss:  0.4352046532928944\n",
      "training loss:  0.42942083179950713\n",
      "training loss:  0.41475351616740225\n",
      "validation loss:  0.4358130693435669\n",
      "Epoch 177\n",
      "------------------------------\n",
      "training loss:  0.43451447814702987\n",
      "training loss:  0.42626638323068616\n",
      "training loss:  0.4187694537639618\n",
      "validation loss:  0.4317370057106018\n",
      "Epoch 178\n",
      "------------------------------\n",
      "training loss:  0.417962401509285\n",
      "training loss:  0.42520967453718184\n",
      "training loss:  0.43583772957324984\n",
      "validation loss:  0.4333951473236084\n",
      "Epoch 179\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.42166364699602127\n",
      "training loss:  0.4213005983829498\n",
      "training loss:  0.4362365886569023\n",
      "validation loss:  0.4309079647064209\n",
      "Epoch 180\n",
      "------------------------------\n",
      "training loss:  0.4299923856556416\n",
      "training loss:  0.42280193597078325\n",
      "training loss:  0.42742603421211245\n",
      "validation loss:  0.43451860547065735\n",
      "Epoch 181\n",
      "------------------------------\n",
      "training loss:  0.4371155846118927\n",
      "training loss:  0.4176762682199478\n",
      "training loss:  0.4270454508066177\n",
      "validation loss:  0.43140870332717896\n",
      "Epoch 182\n",
      "------------------------------\n",
      "training loss:  0.42122809201478956\n",
      "training loss:  0.4292545706033707\n",
      "training loss:  0.4297568340599537\n",
      "validation loss:  0.43032020330429077\n",
      "Epoch 183\n",
      "------------------------------\n",
      "training loss:  0.4201209065318108\n",
      "training loss:  0.43212259918451307\n",
      "training loss:  0.42836117535829543\n",
      "validation loss:  0.43212199211120605\n",
      "Epoch 184\n",
      "------------------------------\n",
      "training loss:  0.43099177271127703\n",
      "training loss:  0.43412465423345564\n",
      "training loss:  0.41478505089879036\n",
      "validation loss:  0.4300300180912018\n",
      "Epoch 185\n",
      "------------------------------\n",
      "training loss:  0.42336256206035616\n",
      "training loss:  0.42563688188791277\n",
      "training loss:  0.43044886767864227\n",
      "validation loss:  0.42998844385147095\n",
      "Epoch 186\n",
      "------------------------------\n",
      "training loss:  0.4271679374575615\n",
      "training loss:  0.43365574985742567\n",
      "training loss:  0.4194608876109123\n",
      "validation loss:  0.43035784363746643\n",
      "Epoch 187\n",
      "------------------------------\n",
      "training loss:  0.4140861101448536\n",
      "training loss:  0.4314755094051361\n",
      "training loss:  0.43545867562294005\n",
      "validation loss:  0.4319276213645935\n",
      "Epoch 188\n",
      "------------------------------\n",
      "training loss:  0.4239540056884289\n",
      "training loss:  0.4268044117093086\n",
      "training loss:  0.430169013440609\n",
      "validation loss:  0.43017062544822693\n",
      "Epoch 189\n",
      "------------------------------\n",
      "training loss:  0.42395967230200765\n",
      "training loss:  0.42666688352823257\n",
      "training loss:  0.4292610812187195\n",
      "validation loss:  0.4293920397758484\n",
      "Epoch 190\n",
      "------------------------------\n",
      "training loss:  0.43128139615058897\n",
      "training loss:  0.4273802725970745\n",
      "training loss:  0.42148382917046545\n",
      "validation loss:  0.4317546486854553\n",
      "Epoch 191\n",
      "------------------------------\n",
      "training loss:  0.4300839275121689\n",
      "training loss:  0.42649485886096955\n",
      "training loss:  0.4237778270244598\n",
      "validation loss:  0.4313168227672577\n",
      "Epoch 192\n",
      "------------------------------\n",
      "training loss:  0.43142896264791486\n",
      "training loss:  0.4264065881073475\n",
      "training loss:  0.4230435410141945\n",
      "validation loss:  0.4294699430465698\n",
      "Epoch 193\n",
      "------------------------------\n",
      "training loss:  0.4318072894215584\n",
      "training loss:  0.4197274702787399\n",
      "training loss:  0.42649324238300323\n",
      "validation loss:  0.4314775764942169\n",
      "Epoch 194\n",
      "------------------------------\n",
      "training loss:  0.43571804732084274\n",
      "training loss:  0.4340446349978447\n",
      "training loss:  0.41222247123718264\n",
      "validation loss:  0.43378034234046936\n",
      "Epoch 195\n",
      "------------------------------\n",
      "training loss:  0.42023070946335794\n",
      "training loss:  0.43247957020998\n",
      "training loss:  0.4275596052408218\n",
      "validation loss:  0.43156424164772034\n",
      "Epoch 196\n",
      "------------------------------\n",
      "training loss:  0.4284487760066986\n",
      "training loss:  0.4219836583733559\n",
      "training loss:  0.43011297911405566\n",
      "validation loss:  0.43342408537864685\n",
      "Epoch 197\n",
      "------------------------------\n",
      "training loss:  0.4153339073061943\n",
      "training loss:  0.43251614093780516\n",
      "training loss:  0.4326544973254204\n",
      "validation loss:  0.43167373538017273\n",
      "Epoch 198\n",
      "------------------------------\n",
      "training loss:  0.42226167023181915\n",
      "training loss:  0.4278399124741554\n",
      "training loss:  0.4295397272706032\n",
      "validation loss:  0.4314255714416504\n",
      "Epoch 199\n",
      "------------------------------\n",
      "training loss:  0.424076711833477\n",
      "training loss:  0.4207712382078171\n",
      "training loss:  0.43518313512206075\n",
      "validation loss:  0.4307376444339752\n",
      "Epoch 200\n",
      "------------------------------\n",
      "training loss:  0.43342928647994994\n",
      "training loss:  0.42985741183161735\n",
      "training loss:  0.4168262958526611\n",
      "validation loss:  0.43140608072280884\n",
      "Epoch 201\n",
      "------------------------------\n",
      "training loss:  0.4253401280939579\n",
      "training loss:  0.42881195843219755\n",
      "training loss:  0.42796263977885246\n",
      "validation loss:  0.4325246512889862\n",
      "Epoch 202\n",
      "------------------------------\n",
      "training loss:  0.4321772652864456\n",
      "training loss:  0.4181120216846466\n",
      "training loss:  0.43087171971797944\n",
      "validation loss:  0.4304563105106354\n",
      "Epoch 203\n",
      "------------------------------\n",
      "training loss:  0.4267427650094032\n",
      "training loss:  0.431490313410759\n",
      "training loss:  0.42005799859762194\n",
      "validation loss:  0.43019768595695496\n",
      "Epoch 204\n",
      "------------------------------\n",
      "training loss:  0.4166233068704605\n",
      "training loss:  0.4266239964962006\n",
      "training loss:  0.4353864899277687\n",
      "validation loss:  0.4311484098434448\n",
      "Epoch 205\n",
      "------------------------------\n",
      "training loss:  0.4318599212169647\n",
      "training loss:  0.42685219168663024\n",
      "training loss:  0.4189732828736305\n",
      "validation loss:  0.4295738935470581\n",
      "Epoch 206\n",
      "------------------------------\n",
      "training loss:  0.4233013817667961\n",
      "training loss:  0.4200882741808891\n",
      "training loss:  0.43500865906476976\n",
      "validation loss:  0.43154340982437134\n",
      "Epoch 207\n",
      "------------------------------\n",
      "training loss:  0.42515311628580094\n",
      "training loss:  0.42122491806745527\n",
      "training loss:  0.4344631573557854\n",
      "validation loss:  0.4316618740558624\n",
      "Epoch 208\n",
      "------------------------------\n",
      "training loss:  0.4133946445584297\n",
      "training loss:  0.4405681312084198\n",
      "training loss:  0.4250556284189224\n",
      "validation loss:  0.4322815239429474\n",
      "Epoch 209\n",
      "------------------------------\n",
      "training loss:  0.43316713958978653\n",
      "training loss:  0.41417006492614744\n",
      "training loss:  0.4323143726587296\n",
      "validation loss:  0.4309896230697632\n",
      "Epoch 210\n",
      "------------------------------\n",
      "training loss:  0.420745478272438\n",
      "training loss:  0.4267207944393158\n",
      "training loss:  0.43186398208141324\n",
      "validation loss:  0.4288276433944702\n",
      "New best model!\n",
      "Epoch 211\n",
      "------------------------------\n",
      "training loss:  0.42807947635650634\n",
      "training loss:  0.41845582097768785\n",
      "training loss:  0.43202169239521027\n",
      "validation loss:  0.4305036962032318\n",
      "Epoch 212\n",
      "------------------------------\n",
      "training loss:  0.4317686498165131\n",
      "training loss:  0.427107265740633\n",
      "training loss:  0.4205435156822205\n",
      "validation loss:  0.43243420124053955\n",
      "Epoch 213\n",
      "------------------------------\n",
      "training loss:  0.4231803049147129\n",
      "training loss:  0.4353691479563713\n",
      "training loss:  0.420410258769989\n",
      "validation loss:  0.4310598075389862\n",
      "Epoch 214\n",
      "------------------------------\n",
      "training loss:  0.43099480599164963\n",
      "training loss:  0.4192506730556488\n",
      "training loss:  0.43025362968444825\n",
      "validation loss:  0.4304354786872864\n",
      "Epoch 215\n",
      "------------------------------\n",
      "training loss:  0.4298057296872139\n",
      "training loss:  0.42644982278347016\n",
      "training loss:  0.4237082174420357\n",
      "validation loss:  0.43087515234947205\n",
      "Epoch 216\n",
      "------------------------------\n",
      "training loss:  0.42873341143131255\n",
      "training loss:  0.43087777972221375\n",
      "training loss:  0.42150869101285937\n",
      "validation loss:  0.43035417795181274\n",
      "Epoch 217\n",
      "------------------------------\n",
      "training loss:  0.42633385479450225\n",
      "training loss:  0.43828309506177904\n",
      "training loss:  0.4158335879445076\n",
      "validation loss:  0.4330693483352661\n",
      "Epoch 218\n",
      "------------------------------\n",
      "training loss:  0.4286795660853386\n",
      "training loss:  0.42363683104515076\n",
      "training loss:  0.4296162495017052\n",
      "validation loss:  0.42923054099082947\n",
      "Epoch 219\n",
      "------------------------------\n",
      "training loss:  0.4194281643629074\n",
      "training loss:  0.435618696808815\n",
      "training loss:  0.4240457499027252\n",
      "validation loss:  0.43338558077812195\n",
      "Epoch 220\n",
      "------------------------------\n",
      "training loss:  0.42121540889143944\n",
      "training loss:  0.42901512399315833\n",
      "training loss:  0.4313743044435978\n",
      "validation loss:  0.43066683411598206\n",
      "Epoch 221\n",
      "------------------------------\n",
      "training loss:  0.4258971920609474\n",
      "training loss:  0.4316780054569244\n",
      "training loss:  0.4221525427699089\n",
      "validation loss:  0.43127554655075073\n",
      "Epoch 222\n",
      "------------------------------\n",
      "training loss:  0.42005347833037376\n",
      "training loss:  0.43376559227705\n",
      "training loss:  0.42594833225011824\n",
      "validation loss:  0.42920446395874023\n",
      "Epoch 223\n",
      "------------------------------\n",
      "training loss:  0.42596246898174284\n",
      "training loss:  0.4227910113334656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.43159664869308473\n",
      "validation loss:  0.4297836422920227\n",
      "Epoch 224\n",
      "------------------------------\n",
      "training loss:  0.43739778101444243\n",
      "training loss:  0.4155618461966515\n",
      "training loss:  0.4272486957907677\n",
      "validation loss:  0.4305136799812317\n",
      "Epoch 225\n",
      "------------------------------\n",
      "training loss:  0.4328043678402901\n",
      "training loss:  0.424676206111908\n",
      "training loss:  0.42083301305770876\n",
      "validation loss:  0.4292641282081604\n",
      "Epoch 226\n",
      "------------------------------\n",
      "training loss:  0.42167558163404467\n",
      "training loss:  0.4297046874463558\n",
      "training loss:  0.43049764290452003\n",
      "validation loss:  0.42930448055267334\n",
      "Epoch 227\n",
      "------------------------------\n",
      "training loss:  0.4165587478876114\n",
      "training loss:  0.42504504054784775\n",
      "training loss:  0.4364960440993309\n",
      "validation loss:  0.43121659755706787\n",
      "Epoch 228\n",
      "------------------------------\n",
      "training loss:  0.42328328639268875\n",
      "training loss:  0.42496526151895525\n",
      "training loss:  0.43049523010849955\n",
      "validation loss:  0.4302251935005188\n",
      "Epoch 229\n",
      "------------------------------\n",
      "training loss:  0.40890374809503555\n",
      "training loss:  0.432329777777195\n",
      "training loss:  0.4372566574811935\n",
      "validation loss:  0.43001508712768555\n",
      "Epoch 230\n",
      "------------------------------\n",
      "training loss:  0.4225109471380711\n",
      "training loss:  0.4256794917583466\n",
      "training loss:  0.43394959092140195\n",
      "validation loss:  0.4304734170436859\n",
      "Epoch 231\n",
      "------------------------------\n",
      "training loss:  0.4238150864839554\n",
      "training loss:  0.4263248971104622\n",
      "training loss:  0.4300347712635994\n",
      "validation loss:  0.4315122067928314\n",
      "Epoch 232\n",
      "------------------------------\n",
      "training loss:  0.42239153787493705\n",
      "training loss:  0.42932759106159213\n",
      "training loss:  0.42731242448091505\n",
      "validation loss:  0.4297211766242981\n",
      "Epoch 233\n",
      "------------------------------\n",
      "training loss:  0.42938228622078894\n",
      "training loss:  0.4299110350012779\n",
      "training loss:  0.41902399986982347\n",
      "validation loss:  0.4316888451576233\n",
      "Epoch 234\n",
      "------------------------------\n",
      "training loss:  0.4250504258275032\n",
      "training loss:  0.4292597392201424\n",
      "training loss:  0.4262093359231949\n",
      "validation loss:  0.4303719103336334\n",
      "Epoch 235\n",
      "------------------------------\n",
      "training loss:  0.42616279631853105\n",
      "training loss:  0.42698722928762434\n",
      "training loss:  0.42762383103370666\n",
      "validation loss:  0.4308088421821594\n",
      "Epoch 236\n",
      "------------------------------\n",
      "training loss:  0.42484545081853864\n",
      "training loss:  0.42356853038072584\n",
      "training loss:  0.4322653949260712\n",
      "validation loss:  0.43252453207969666\n",
      "Epoch 237\n",
      "------------------------------\n",
      "training loss:  0.42529236555099487\n",
      "training loss:  0.42832985013723374\n",
      "training loss:  0.42552014321088794\n",
      "validation loss:  0.43105387687683105\n",
      "Epoch 238\n",
      "------------------------------\n",
      "training loss:  0.422833601385355\n",
      "training loss:  0.4261139905452728\n",
      "training loss:  0.43006498098373414\n",
      "validation loss:  0.43366116285324097\n",
      "Epoch 239\n",
      "------------------------------\n",
      "training loss:  0.4375068235397339\n",
      "training loss:  0.41718719810247423\n",
      "training loss:  0.42389907032251356\n",
      "validation loss:  0.4312591254711151\n",
      "Epoch 240\n",
      "------------------------------\n",
      "training loss:  0.43334307432174685\n",
      "training loss:  0.40942071840167044\n",
      "training loss:  0.43744765311479566\n",
      "validation loss:  0.4313565194606781\n",
      "Epoch 241\n",
      "------------------------------\n",
      "training loss:  0.42365222826600074\n",
      "training loss:  0.43760455399751663\n",
      "training loss:  0.4183516515791416\n",
      "validation loss:  0.4305667281150818\n",
      "Epoch 242\n",
      "------------------------------\n",
      "training loss:  0.4229609435796738\n",
      "training loss:  0.44273051649332046\n",
      "training loss:  0.4124845343828201\n",
      "validation loss:  0.4314430356025696\n",
      "Epoch 243\n",
      "------------------------------\n",
      "training loss:  0.41871347710490225\n",
      "training loss:  0.4358456763625145\n",
      "training loss:  0.4250377231836319\n",
      "validation loss:  0.43030956387519836\n",
      "Epoch 244\n",
      "------------------------------\n",
      "training loss:  0.4254406759142876\n",
      "training loss:  0.4288895946741104\n",
      "training loss:  0.4259386482834816\n",
      "validation loss:  0.4313395619392395\n",
      "Epoch 245\n",
      "------------------------------\n",
      "training loss:  0.43013314098119737\n",
      "training loss:  0.4244619983434677\n",
      "training loss:  0.42387063056230545\n",
      "validation loss:  0.4293595850467682\n",
      "Epoch 246\n",
      "------------------------------\n",
      "training loss:  0.43160098493099214\n",
      "training loss:  0.41930238902568817\n",
      "training loss:  0.4271896359324455\n",
      "validation loss:  0.43000394105911255\n",
      "Epoch 247\n",
      "------------------------------\n",
      "training loss:  0.42930037081241607\n",
      "training loss:  0.4197705367207527\n",
      "training loss:  0.42951597839593886\n",
      "validation loss:  0.4297545254230499\n",
      "Epoch 248\n",
      "------------------------------\n",
      "training loss:  0.4059609781205654\n",
      "training loss:  0.4422332778573036\n",
      "training loss:  0.42927452102303504\n",
      "validation loss:  0.4310009777545929\n",
      "Epoch 249\n",
      "------------------------------\n",
      "training loss:  0.42335621267557144\n",
      "training loss:  0.43123232170939446\n",
      "training loss:  0.4245573827624321\n",
      "validation loss:  0.4302276074886322\n",
      "Epoch 250\n",
      "------------------------------\n",
      "training loss:  0.4166999039053917\n",
      "training loss:  0.43697019696235656\n",
      "training loss:  0.4248572388291359\n",
      "validation loss:  0.4302152991294861\n",
      "Epoch 251\n",
      "------------------------------\n",
      "training loss:  0.43036371558904646\n",
      "training loss:  0.42279145449399946\n",
      "training loss:  0.4235228492319584\n",
      "validation loss:  0.4310724139213562\n",
      "Epoch 252\n",
      "------------------------------\n",
      "training loss:  0.4282215864956379\n",
      "training loss:  0.42512762039899826\n",
      "training loss:  0.42535173773765567\n",
      "validation loss:  0.42968395352363586\n",
      "Epoch 253\n",
      "------------------------------\n",
      "training loss:  0.4232382033765316\n",
      "training loss:  0.42714050859212876\n",
      "training loss:  0.42938232570886614\n",
      "validation loss:  0.43118372559547424\n",
      "Epoch 254\n",
      "------------------------------\n",
      "training loss:  0.4256217813491821\n",
      "training loss:  0.4285066643357277\n",
      "training loss:  0.4234128504991531\n",
      "validation loss:  0.4317921996116638\n",
      "Epoch 255\n",
      "------------------------------\n",
      "training loss:  0.42810664892196654\n",
      "training loss:  0.42169518560171126\n",
      "training loss:  0.42843252152204514\n",
      "validation loss:  0.43133196234703064\n",
      "Epoch 256\n",
      "------------------------------\n",
      "training loss:  0.42508392065763473\n",
      "training loss:  0.4267727670073509\n",
      "training loss:  0.42616268783807754\n",
      "validation loss:  0.43267109990119934\n",
      "Epoch 257\n",
      "------------------------------\n",
      "training loss:  0.4283612149953842\n",
      "training loss:  0.4221363094449043\n",
      "training loss:  0.4292459559440613\n",
      "validation loss:  0.4316958487033844\n",
      "Epoch 258\n",
      "------------------------------\n",
      "training loss:  0.43432867646217344\n",
      "training loss:  0.43019112884998323\n",
      "training loss:  0.41552814453840253\n",
      "validation loss:  0.4299166798591614\n",
      "Epoch 259\n",
      "------------------------------\n",
      "training loss:  0.4207353907823563\n",
      "training loss:  0.4339152485132217\n",
      "training loss:  0.42488772571086886\n",
      "validation loss:  0.4320564270019531\n",
      "Epoch 260\n",
      "------------------------------\n",
      "training loss:  0.41754848331212996\n",
      "training loss:  0.4289060901105404\n",
      "training loss:  0.43353387355804446\n",
      "validation loss:  0.4308770000934601\n",
      "Epoch 261\n",
      "------------------------------\n",
      "training loss:  0.4206495669484138\n",
      "training loss:  0.42458498328924177\n",
      "training loss:  0.4340805871784687\n",
      "validation loss:  0.4302231967449188\n",
      "Epoch 262\n",
      "------------------------------\n",
      "training loss:  0.4221714034676552\n",
      "training loss:  0.4322798252105713\n",
      "training loss:  0.42278587266802786\n",
      "validation loss:  0.43077799677848816\n",
      "Epoch 263\n",
      "------------------------------\n",
      "training loss:  0.426417421400547\n",
      "training loss:  0.4141333889961243\n",
      "training loss:  0.4370640951395035\n",
      "validation loss:  0.43080517649650574\n",
      "Epoch 264\n",
      "------------------------------\n",
      "training loss:  0.424301081597805\n",
      "training loss:  0.43685153633356094\n",
      "training loss:  0.41797680646181107\n",
      "validation loss:  0.42975732684135437\n",
      "Epoch 265\n",
      "------------------------------\n",
      "training loss:  0.4169969566166401\n",
      "training loss:  0.44538085281848905\n",
      "training loss:  0.4153680907189846\n",
      "validation loss:  0.43180420994758606\n",
      "Epoch 266\n",
      "------------------------------\n",
      "training loss:  0.41958938106894494\n",
      "training loss:  0.43132814168930056\n",
      "training loss:  0.4265448495745659\n",
      "validation loss:  0.431375652551651\n",
      "Epoch 267\n",
      "------------------------------\n",
      "training loss:  0.4269428899884224\n",
      "training loss:  0.42716392055153846\n",
      "training loss:  0.4272300288081169\n",
      "validation loss:  0.42888689041137695\n",
      "Epoch 268\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.4273370584845543\n",
      "training loss:  0.4211583052575588\n",
      "training loss:  0.42934659838676453\n",
      "validation loss:  0.4298587143421173\n",
      "Epoch 269\n",
      "------------------------------\n",
      "training loss:  0.42364440083503724\n",
      "training loss:  0.42368908882141115\n",
      "training loss:  0.43134372144937516\n",
      "validation loss:  0.42940255999565125\n",
      "Epoch 270\n",
      "------------------------------\n",
      "training loss:  0.4289392900466919\n",
      "training loss:  0.429285903275013\n",
      "training loss:  0.42093987703323366\n",
      "validation loss:  0.43027177453041077\n",
      "Epoch 271\n",
      "------------------------------\n",
      "training loss:  0.41580617144703863\n",
      "training loss:  0.4326805591583252\n",
      "training loss:  0.4282104939222336\n",
      "validation loss:  0.4308449625968933\n",
      "Epoch 272\n",
      "------------------------------\n",
      "training loss:  0.43440761625766755\n",
      "training loss:  0.4291394960880279\n",
      "training loss:  0.41569562017917633\n",
      "validation loss:  0.43062686920166016\n",
      "Epoch 273\n",
      "------------------------------\n",
      "training loss:  0.4242257410287857\n",
      "training loss:  0.4327743297815323\n",
      "training loss:  0.4223626999557018\n",
      "validation loss:  0.42913275957107544\n",
      "Epoch 274\n",
      "------------------------------\n",
      "training loss:  0.4219594967365265\n",
      "training loss:  0.4292851236462593\n",
      "training loss:  0.4268722438812256\n",
      "validation loss:  0.4303878843784332\n",
      "Epoch 275\n",
      "------------------------------\n",
      "training loss:  0.42943062841892243\n",
      "training loss:  0.43901866018772123\n",
      "training loss:  0.40919948875904083\n",
      "validation loss:  0.43069198727607727\n",
      "Epoch 276\n",
      "------------------------------\n",
      "training loss:  0.4273242348432541\n",
      "training loss:  0.4287503251433373\n",
      "training loss:  0.42251398146152497\n",
      "validation loss:  0.43251752853393555\n",
      "Epoch 277\n",
      "------------------------------\n",
      "training loss:  0.4311703959107399\n",
      "training loss:  0.4305926784873009\n",
      "training loss:  0.41648256093263625\n",
      "validation loss:  0.4307941794395447\n",
      "Epoch 278\n",
      "------------------------------\n",
      "training loss:  0.409687230437994\n",
      "training loss:  0.4250412371754646\n",
      "training loss:  0.44344511240720746\n",
      "validation loss:  0.4310030937194824\n",
      "Epoch 279\n",
      "------------------------------\n",
      "training loss:  0.4295492550730705\n",
      "training loss:  0.42845780596137045\n",
      "training loss:  0.4219177708029747\n",
      "validation loss:  0.4303576648235321\n",
      "Epoch 280\n",
      "------------------------------\n",
      "training loss:  0.42214784502983094\n",
      "training loss:  0.42915623635053635\n",
      "training loss:  0.4270244412124157\n",
      "validation loss:  0.4299514889717102\n",
      "Epoch 281\n",
      "------------------------------\n",
      "training loss:  0.42577673733234406\n",
      "training loss:  0.4275852781534195\n",
      "training loss:  0.42574874222278597\n",
      "validation loss:  0.4303830564022064\n",
      "Epoch 282\n",
      "------------------------------\n",
      "training loss:  0.4211014802753925\n",
      "training loss:  0.42892502158880236\n",
      "training loss:  0.4291262719035149\n",
      "validation loss:  0.432891309261322\n",
      "Epoch 283\n",
      "------------------------------\n",
      "training loss:  0.4312628507614136\n",
      "training loss:  0.4222118166089058\n",
      "training loss:  0.4240745437145233\n",
      "validation loss:  0.43239012360572815\n",
      "Epoch 284\n",
      "------------------------------\n",
      "training loss:  0.41889301300048826\n",
      "training loss:  0.42081035673618317\n",
      "training loss:  0.4391873970627785\n",
      "validation loss:  0.42979979515075684\n",
      "Epoch 285\n",
      "------------------------------\n",
      "training loss:  0.41620464488863945\n",
      "training loss:  0.4309209230542183\n",
      "training loss:  0.43145730093121526\n",
      "validation loss:  0.4297843873500824\n",
      "Epoch 286\n",
      "------------------------------\n",
      "training loss:  0.44002141177654264\n",
      "training loss:  0.42371260285377504\n",
      "training loss:  0.4141470590233803\n",
      "validation loss:  0.43023425340652466\n",
      "Epoch 287\n",
      "------------------------------\n",
      "training loss:  0.4233985370397568\n",
      "training loss:  0.4284222012758255\n",
      "training loss:  0.42626100182533266\n",
      "validation loss:  0.4322740435600281\n",
      "Epoch 288\n",
      "------------------------------\n",
      "training loss:  0.42726134628057477\n",
      "training loss:  0.42317317247390746\n",
      "training loss:  0.4285575237870216\n",
      "validation loss:  0.42991557717323303\n",
      "Epoch 289\n",
      "------------------------------\n",
      "training loss:  0.4293232648074627\n",
      "training loss:  0.4194634658098221\n",
      "training loss:  0.42935356169939043\n",
      "validation loss:  0.4304756224155426\n",
      "Epoch 290\n",
      "------------------------------\n",
      "training loss:  0.41798765599727633\n",
      "training loss:  0.42585771352052687\n",
      "training loss:  0.43596938014030456\n",
      "validation loss:  0.4304962456226349\n",
      "Epoch 291\n",
      "------------------------------\n",
      "training loss:  0.43490089058876036\n",
      "training loss:  0.433285673558712\n",
      "training loss:  0.41010696545243264\n",
      "validation loss:  0.43003857135772705\n",
      "Epoch 292\n",
      "------------------------------\n",
      "training loss:  0.4161324128508568\n",
      "training loss:  0.4299066036939621\n",
      "training loss:  0.4331644806265831\n",
      "validation loss:  0.4350023865699768\n",
      "Epoch 293\n",
      "------------------------------\n",
      "training loss:  0.43176190882921217\n",
      "training loss:  0.4244983844459057\n",
      "training loss:  0.42277366936206817\n",
      "validation loss:  0.430270791053772\n",
      "Epoch 294\n",
      "------------------------------\n",
      "training loss:  0.41810431957244876\n",
      "training loss:  0.43462561547756196\n",
      "training loss:  0.42524309694767\n",
      "validation loss:  0.43037354946136475\n",
      "Epoch 295\n",
      "------------------------------\n",
      "training loss:  0.43421188786625864\n",
      "training loss:  0.42760944545269014\n",
      "training loss:  0.4175912161171436\n",
      "validation loss:  0.43088778853416443\n",
      "Epoch 296\n",
      "------------------------------\n",
      "training loss:  0.42658579170703886\n",
      "training loss:  0.4136488488316536\n",
      "training loss:  0.43819440096616746\n",
      "validation loss:  0.4314514994621277\n",
      "Epoch 297\n",
      "------------------------------\n",
      "training loss:  0.4358416426181793\n",
      "training loss:  0.4167776766419411\n",
      "training loss:  0.4262939918041229\n",
      "validation loss:  0.4325304627418518\n",
      "Epoch 298\n",
      "------------------------------\n",
      "training loss:  0.41951492697000503\n",
      "training loss:  0.43675012320280077\n",
      "training loss:  0.4224613891541958\n",
      "validation loss:  0.43116217851638794\n",
      "Epoch 299\n",
      "------------------------------\n",
      "training loss:  0.4374702176451683\n",
      "training loss:  0.42297873288393023\n",
      "training loss:  0.41969977140426634\n",
      "validation loss:  0.4306933879852295\n",
      "Epoch 300\n",
      "------------------------------\n",
      "training loss:  0.4309222207963467\n",
      "training loss:  0.4243526741862297\n",
      "training loss:  0.4258798015117645\n",
      "validation loss:  0.4289931356906891\n",
      "Epoch 301\n",
      "------------------------------\n",
      "training loss:  0.42505628898739817\n",
      "training loss:  0.416749287545681\n",
      "training loss:  0.4366129222512245\n",
      "validation loss:  0.4304216802120209\n",
      "Epoch 302\n",
      "------------------------------\n",
      "training loss:  0.4290203416347504\n",
      "training loss:  0.42875943809747696\n",
      "training loss:  0.42000869393348694\n",
      "validation loss:  0.4330087900161743\n",
      "Epoch 303\n",
      "------------------------------\n",
      "training loss:  0.42569692343473436\n",
      "training loss:  0.42537750452756884\n",
      "training loss:  0.42823542281985283\n",
      "validation loss:  0.43368104100227356\n",
      "Epoch 304\n",
      "------------------------------\n",
      "training loss:  0.4231514582037926\n",
      "training loss:  0.4319439134001732\n",
      "training loss:  0.4244511526823044\n",
      "validation loss:  0.4289528429508209\n",
      "Epoch 305\n",
      "------------------------------\n",
      "training loss:  0.4182025264203548\n",
      "training loss:  0.42896593242883685\n",
      "training loss:  0.43228471964597703\n",
      "validation loss:  0.4302058517932892\n",
      "Epoch 306\n",
      "------------------------------\n",
      "training loss:  0.42655770778656005\n",
      "training loss:  0.4312321752309799\n",
      "training loss:  0.42125513702630996\n",
      "validation loss:  0.4306451082229614\n",
      "Epoch 307\n",
      "------------------------------\n",
      "training loss:  0.42051877453923225\n",
      "training loss:  0.43257721692323686\n",
      "training loss:  0.4236566296219826\n",
      "validation loss:  0.43057191371917725\n",
      "Epoch 308\n",
      "------------------------------\n",
      "training loss:  0.4283311015367508\n",
      "training loss:  0.4266573566198349\n",
      "training loss:  0.4249522931873798\n",
      "validation loss:  0.4311661720275879\n",
      "Epoch 309\n",
      "------------------------------\n",
      "training loss:  0.41870025515556336\n",
      "training loss:  0.4217944452166557\n",
      "training loss:  0.43803305208683013\n",
      "validation loss:  0.4318383038043976\n",
      "Epoch 310\n",
      "------------------------------\n",
      "training loss:  0.4221887141466141\n",
      "training loss:  0.4297652688622475\n",
      "training loss:  0.4270887629687786\n",
      "validation loss:  0.43007227778434753\n",
      "Epoch 311\n",
      "------------------------------\n",
      "training loss:  0.41741465151309964\n",
      "training loss:  0.4278192153573036\n",
      "training loss:  0.4337563899159431\n",
      "validation loss:  0.4345860481262207\n",
      "Epoch 312\n",
      "------------------------------\n",
      "training loss:  0.42135035529732706\n",
      "training loss:  0.4348818090558052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.42373732835054395\n",
      "validation loss:  0.42954540252685547\n",
      "Epoch 313\n",
      "------------------------------\n",
      "training loss:  0.42308811128139495\n",
      "training loss:  0.4335410448908806\n",
      "training loss:  0.4207727760076523\n",
      "validation loss:  0.430012583732605\n",
      "Epoch 314\n",
      "------------------------------\n",
      "training loss:  0.4351870948076248\n",
      "training loss:  0.4303917053341866\n",
      "training loss:  0.41115917414426806\n",
      "validation loss:  0.4291455149650574\n",
      "Epoch 315\n",
      "------------------------------\n",
      "training loss:  0.42316029608249667\n",
      "training loss:  0.42903651535511017\n",
      "training loss:  0.4263655251264572\n",
      "validation loss:  0.4305979013442993\n",
      "Epoch 316\n",
      "------------------------------\n",
      "training loss:  0.4218134416639805\n",
      "training loss:  0.42996875554323194\n",
      "training loss:  0.4283678317070007\n",
      "validation loss:  0.4309098422527313\n",
      "Epoch 317\n",
      "------------------------------\n",
      "training loss:  0.4361905257403851\n",
      "training loss:  0.4229159200191498\n",
      "training loss:  0.41939629092812536\n",
      "validation loss:  0.4309144616127014\n",
      "Epoch 318\n",
      "------------------------------\n",
      "training loss:  0.42714569687843323\n",
      "training loss:  0.4132326464354992\n",
      "training loss:  0.43893968284130097\n",
      "validation loss:  0.43000829219818115\n",
      "Epoch 319\n",
      "------------------------------\n",
      "training loss:  0.42369513601064684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-259-ec78b9a81421>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i+1}\\n------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-20dabc4f9a42>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, writer)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Print summary data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.9/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for i in range(epochs):\n",
    "    print(f\"Epoch {i+1}\\n------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer, writer)\n",
    "    loss = val(X_val_v, y_val_v, model, loss_fn, writer, model_name)\n",
    "    if min_loss is None or loss < min_loss:\n",
    "        min_loss = loss\n",
    "        print('New best model!')\n",
    "        torch.save(model, f'models/{model_name}/best.pth')\n",
    "    global_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c506c888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4297366440296173 0.43173083662986755\n"
     ]
    }
   ],
   "source": [
    "# Test loss\n",
    "model3_best = torch.load('models/model3/best.pth')\n",
    "model3_800 = torch.load('models/model3/800.pth')\n",
    "loss1 = test(X_test_v, y_test_v, model3_best, loss_fn)\n",
    "loss2 = test(X_test_v, y_test_v, model3_800, loss_fn)\n",
    "print(loss1, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "d48d0821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47726161369193154 0.486898523106241\n"
     ]
    }
   ],
   "source": [
    "# F1 scores\n",
    "sigmoid = nn.Sigmoid()\n",
    "pred1 = np.round(sigmoid(model3_best(X_test_v)).detach().numpy())\n",
    "pred2 = np.round(sigmoid(model3_800(X_test_v)).detach().numpy())\n",
    "score1 = f1_score(pred1, y_test)\n",
    "score2 = f1_score(pred2, y_test)\n",
    "print(score1, score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "e6056f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8218333333333333"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = pred1.squeeze(1)\n",
    "(pred1 == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "626ea924",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model3_best\n",
    "torch.save(best_model, 'models/best_NN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
