{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cb27306",
   "metadata": {},
   "source": [
    "# Wine Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e5556",
   "metadata": {},
   "source": [
    "I work with the Wine Quality data set. The regression task is to estimate the quality of a wine given its attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6223b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('data/winequality-white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5707fc",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0f7023f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8572be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898 entries, 0 to 4897\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         4898 non-null   float64\n",
      " 1   volatile acidity      4898 non-null   float64\n",
      " 2   citric acid           4898 non-null   float64\n",
      " 3   residual sugar        4898 non-null   float64\n",
      " 4   chlorides             4898 non-null   float64\n",
      " 5   free sulfur dioxide   4898 non-null   float64\n",
      " 6   total sulfur dioxide  4898 non-null   float64\n",
      " 7   density               4898 non-null   float64\n",
      " 8   pH                    4898 non-null   float64\n",
      " 9   sulphates             4898 non-null   float64\n",
      " 10  alcohol               4898 non-null   float64\n",
      " 11  quality               4898 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 459.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1028c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.hist(bins=50, figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08a4d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality                 1.000000\n",
       "alcohol                 0.435575\n",
       "pH                      0.099427\n",
       "sulphates               0.053678\n",
       "free sulfur dioxide     0.008158\n",
       "citric acid            -0.009209\n",
       "residual sugar         -0.097577\n",
       "fixed acidity          -0.113663\n",
       "total sulfur dioxide   -0.174737\n",
       "volatile acidity       -0.194723\n",
       "chlorides              -0.209934\n",
       "density                -0.307123\n",
       "Name: quality, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix['quality'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624341e1",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ca757b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test sets and X/y pairs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "X_train = train.drop('quality', axis=1)\n",
    "y_train = train['quality'].copy()\n",
    "X_test = test.drop('quality', axis=1)\n",
    "y_test = test['quality'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a096107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdscaler = StandardScaler()\n",
    "X_train_prep = stdscaler.fit_transform(X_train)\n",
    "X_test_prep = stdscaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ddfb04",
   "metadata": {},
   "source": [
    "## Basic ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a714b364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8867994142404612"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start with base-level prediction\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "predictions = [y_train.mean()] * len(y_train)\n",
    "base_mse = mean_squared_error(predictions, y_train)\n",
    "base_mse = np.sqrt(base_mse)\n",
    "base_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d1e6db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7502172818316427"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Fit model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_prep, y_train)\n",
    "# Measure training loss\n",
    "predictions = lin_reg.predict(X_train_prep)\n",
    "lin_mse = mean_squared_error(predictions, y_train)\n",
    "lin_mse = np.sqrt(lin_mse)\n",
    "lin_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9e564b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7525277645111913, 0.04313342916930864)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure k-fold cross-validation loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross_val_score expects utility function, not loss function\n",
    "lin_scores = cross_val_score(lin_reg, X_train_prep, y_train, \n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_scores = np.sqrt(-lin_scores)\n",
    "lin_scores.mean(), lin_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed352a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train_prep, y_train)\n",
    "# Measure training loss\n",
    "predictions = tree_reg.predict(X_train_prep)\n",
    "tree_mse = mean_squared_error(predictions, y_train)\n",
    "tree_mse = np.sqrt(tree_mse)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496e623d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.881229842595989, 0.03729342336290044)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure cross-validation loss\n",
    "tree_scores = cross_val_score(tree_reg, X_train_prep, y_train, \n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_scores = np.sqrt(-tree_scores)\n",
    "tree_scores.mean(), tree_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cfcfebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2306654709077995"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X_train_prep, y_train)\n",
    "# Measure training loss\n",
    "predictions = forest_reg.predict(X_train_prep)\n",
    "forest_mse = mean_squared_error(predictions, y_train)\n",
    "forest_mse = np.sqrt(forest_mse)\n",
    "forest_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6434820f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6236919706699335, 0.035345768629928914)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure cross-validation loss\n",
    "forest_scores = cross_val_score(forest_reg, X_train_prep, y_train, \n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_scores = np.sqrt(-forest_scores)\n",
    "forest_scores.mean(), forest_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d126f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6277916391386317"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train_prep, y_train)\n",
    "# Training loss\n",
    "predictions = svr.predict(X_train_prep)\n",
    "svr_mse = mean_squared_error(predictions, y_train)\n",
    "svr_mse = np.sqrt(svr_mse)\n",
    "svr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e76f2464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.693354085198933, 0.03555820940991809)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation loss\n",
    "svr_scores = cross_val_score(svr, X_train_prep, y_train,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "svr_scores = np.sqrt(-svr_scores)\n",
    "svr_scores.mean(), svr_scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eaf175",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d708ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestRegressor(),\n",
       "             param_grid={'max_depth': [30], 'max_features': [3],\n",
       "                         'n_estimators': [600]},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search on random forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ran in various incarnations based on intermediate results\n",
    "tree_grid = {'n_estimators': [300, 600, 900], 'max_depth': [20, 30, 45], 'max_features': [1, 3, 9]}\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, tree_grid, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18eaff87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 30, 'max_features': 3, 'n_estimators': 600}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1cc89be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22417283897879908"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training error\n",
    "forest_reg = grid_search.best_estimator_\n",
    "predictions = forest_reg.predict(X_train_prep)\n",
    "forest_mse = mean_squared_error(predictions, y_train)\n",
    "forest_mse = np.sqrt(forest_mse)\n",
    "forest_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2613931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6189297791841215"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation error\n",
    "forest_cv = grid_search.cv_results_['mean_test_score'].max()\n",
    "forest_cv = np.sqrt(-forest_cv)\n",
    "forest_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d84c87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVR(),\n",
       "             param_grid={'C': [2.5, 5], 'epsilon': [0.25, 0.5],\n",
       "                         'kernel': ['rbf']},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid search on SVR\n",
    "svr_grid = {'kernel': ['rbf'], 'C': [2.5, 5], 'epsilon': [0.25, 0.5]}\n",
    "svr = SVR()\n",
    "grid_search = GridSearchCV(svr, svr_grid, scoring=\"neg_mean_squared_error\")\n",
    "grid_search.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f69caf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2.5, 'epsilon': 0.25, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca2cfbce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5889396721481854"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training error\n",
    "svr = grid_search.best_estimator_\n",
    "predictions = svr.predict(X_train_prep)\n",
    "svr_mse = mean_squared_error(predictions, y_train)\n",
    "svr_mse = np.sqrt(svr_mse)\n",
    "svr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10bdff9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6867259020679415"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-validation error\n",
    "svr_cv = grid_search.cv_results_['mean_test_score'].max()\n",
    "svr_cv = np.sqrt(-svr_cv)\n",
    "svr_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "efc27ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best models\n",
    "import pickle\n",
    "\n",
    "forest_tuple = (forest_reg, forest_cv)\n",
    "pickle.dump(forest_tuple, open(\"models/forest_model.pkl\", \"wb\"))\n",
    "\n",
    "svr_tuple = (svr, svr_cv)\n",
    "pickle.dump(svr_tuple, open(\"models/svr_model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f32644",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1ef77e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5763289975160201"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test error\n",
    "final_model = forest_reg\n",
    "predictions = final_model.predict(X_test_prep)\n",
    "mse = mean_squared_error(predictions, y_test)\n",
    "mse = np.sqrt(mse)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa867fa",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b43bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67858284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors and create validation set\n",
    "X_train_v = torch.tensor(X_train_prep, dtype=torch.float32)\n",
    "y_train_v = torch.tensor(y_train.to_numpy(), dtype=torch.float32)\n",
    "X_train_v, X_val_v, y_train_v, y_val_v = train_test_split(X_train_v, y_train_v, test_size=0.1)\n",
    "X_test_v = torch.tensor(X_test_prep, dtype=torch.float32)\n",
    "y_test_v = torch.tensor(y_test.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cb41e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define shallow and deep NN modules\n",
    "class NN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(11, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "class NN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(11, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ab357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow model\n",
    "model1 = NN1()\n",
    "loss_fn = nn.MSELoss()\n",
    "optim1 = optim.Adam(model1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7892f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper model\n",
    "model2 = NN2()\n",
    "optim2 = optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f6d3306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper model with L2 regularization\n",
    "model3 = NN2()\n",
    "optim3 = optim.Adam(model3.parameters(), weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f7caa66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: Embedding dir exists, did you set global_step for add_embedding()?\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# NOTE: Reset when testing new model\n",
    "model, name, optimizer = model3, \"model3\", optim3\n",
    "global_epoch = 0\n",
    "min_loss = None\n",
    "\n",
    "# Make summary writer\n",
    "writer = SummaryWriter(f'runs/{name}')\n",
    "writer.add_graph(model, X_train_v[0])\n",
    "writer.add_embedding(X_train_v, metadata=y_train_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fda02bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(X, y, model, loss_fn, optimizer, writer):\n",
    "    size = len(X)\n",
    "    avg_loss = 0.0\n",
    "    for i, x in enumerate(X):\n",
    "        # Make prediciton\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y[i])\n",
    "        avg_loss += loss.item()\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print summary data\n",
    "        if (i + 1) % 100 == 0:\n",
    "            avg_loss /= 100\n",
    "            print(f'training loss: {avg_loss}')\n",
    "            writer.add_scalar('training loss', avg_loss, global_step = global_epoch * len(X) + i)\n",
    "            writer.flush()\n",
    "            avg_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b449d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation loop\n",
    "def val(X, y, model, loss_fn, writer=None, name=None):\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(X)\n",
    "    val_loss = 0.0\n",
    "    # Compute loss on set\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(X):\n",
    "            pred = model(x)\n",
    "            val_loss += loss_fn(pred, y[i]).item()\n",
    "    val_loss /= size\n",
    "    \n",
    "    # Print summary data\n",
    "    if writer is not None:\n",
    "        print(f'validation_loss: {val_loss}')\n",
    "        writer.add_scalar('validation loss', val_loss, global_step = global_epoch)\n",
    "        writer.flush()\n",
    "    if name is not None and global_epoch % 50 == 0:\n",
    "        torch.save(model, f'models/NN_{name}_{global_epoch}.pth')\n",
    "    \n",
    "    model.train()\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e63dbe52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "------------------------------\n",
      "training loss: 0.401981549912216\n",
      "training loss: 0.32889697881437313\n",
      "training loss: 0.3208599850651808\n",
      "training loss: 0.3637877637564088\n",
      "training loss: 0.3571837540949946\n",
      "training loss: 0.24785457792711896\n",
      "training loss: 0.4212871479272144\n",
      "training loss: 0.34654833721830075\n",
      "training loss: 0.27949733243880476\n",
      "training loss: 0.31750818889413496\n",
      "training loss: 0.31562711870836213\n",
      "training loss: 0.31697031567498923\n",
      "training loss: 0.2847757696831832\n",
      "training loss: 0.4286951813774795\n",
      "training loss: 0.4309986798901809\n",
      "training loss: 0.4809764934548389\n",
      "training loss: 0.4574608810513769\n",
      "training loss: 0.3260750366869456\n",
      "training loss: 0.3237614437741149\n",
      "training loss: 0.40020354684616904\n",
      "training loss: 0.28100975819172164\n",
      "training loss: 0.36216145271388994\n",
      "training loss: 0.28682082724513747\n",
      "training loss: 0.3544771045750167\n",
      "training loss: 0.32439260840372297\n",
      "training loss: 0.4504751240497717\n",
      "training loss: 0.44982359486408313\n",
      "training loss: 0.4354703643688117\n",
      "training loss: 0.3401518509504967\n",
      "training loss: 0.46488193665398286\n",
      "training loss: 0.28529638246860484\n",
      "training loss: 0.2858775369578143\n",
      "training loss: 0.36513955239433016\n",
      "training loss: 0.3025502083457104\n",
      "training loss: 0.35925197519536595\n",
      "validation_loss: 0.48209589182883694\n",
      "\n",
      "Epoch 2\n",
      "------------------------------\n",
      "training loss: 0.3648246154384469\n",
      "training loss: 0.35037646444136955\n",
      "training loss: 0.33803303409619045\n",
      "training loss: 0.26841188171107205\n",
      "training loss: 0.4096867610060144\n",
      "training loss: 0.4000050347353681\n",
      "training loss: 0.40175784374503565\n",
      "training loss: 0.3633547348284719\n",
      "training loss: 0.3135108652290728\n",
      "training loss: 0.2634836450760213\n",
      "training loss: 0.23911918059657183\n",
      "training loss: 0.3269471564368359\n",
      "training loss: 0.4140584595308246\n",
      "training loss: 0.4014411339294747\n",
      "training loss: 0.3192874370209938\n",
      "training loss: 0.4855157934338786\n",
      "training loss: 0.3514170646699495\n",
      "training loss: 0.31856189462083423\n",
      "training loss: 0.24459032636375924\n",
      "training loss: 0.30253231646549694\n",
      "training loss: 0.35438021589128765\n",
      "training loss: 0.39168614532712126\n",
      "training loss: 0.3626318451309089\n",
      "training loss: 0.43609829825123597\n",
      "training loss: 0.3928324359966791\n",
      "training loss: 0.42833454807561794\n",
      "training loss: 0.27733356117911173\n",
      "training loss: 0.4107673016157241\n",
      "training loss: 0.29266768017289907\n",
      "training loss: 0.3440572577072453\n",
      "training loss: 0.34625271922202955\n",
      "training loss: 0.4356480932960494\n",
      "training loss: 0.3652660465647932\n",
      "training loss: 0.47670882281456217\n",
      "training loss: 0.3649753568754568\n",
      "validation_loss: 0.45269222034190365\n",
      "\n",
      "Epoch 3\n",
      "------------------------------\n",
      "training loss: 0.23185842751612654\n",
      "training loss: 0.3882038233152184\n",
      "training loss: 0.3224480106058036\n",
      "training loss: 0.37756263980329097\n",
      "training loss: 0.3743490387231941\n",
      "training loss: 0.33972176870272963\n",
      "training loss: 0.35696080105502914\n",
      "training loss: 0.33242123850992356\n",
      "training loss: 0.3594993217359297\n",
      "training loss: 0.2700720940106771\n",
      "training loss: 0.33412812464217495\n",
      "training loss: 0.3964197981961479\n",
      "training loss: 0.5990171103682133\n",
      "training loss: 0.3187495383809437\n",
      "training loss: 0.3263713784596621\n",
      "training loss: 0.21440279869355436\n",
      "training loss: 0.3344468351245814\n",
      "training loss: 0.5057092651101993\n",
      "training loss: 0.30743408111681675\n",
      "training loss: 0.3500807997526863\n",
      "training loss: 0.3771518015815127\n",
      "training loss: 0.33667952122661515\n",
      "training loss: 0.29719634607434275\n",
      "training loss: 0.5076117008066285\n",
      "training loss: 0.3319421500097451\n",
      "training loss: 0.2801112775877118\n",
      "training loss: 0.30114184329744603\n",
      "training loss: 0.3550986373592423\n",
      "training loss: 0.3189387955584971\n",
      "training loss: 0.3759139401327411\n",
      "training loss: 0.43064731987251437\n",
      "training loss: 0.4732006130125956\n",
      "training loss: 0.3619119683029203\n",
      "training loss: 0.265552564705431\n",
      "training loss: 0.32140254990255923\n",
      "validation_loss: 0.4404355842355898\n",
      "\n",
      "Epoch 4\n",
      "------------------------------\n",
      "training loss: 0.2403320370685833\n",
      "training loss: 0.36386467433592773\n",
      "training loss: 0.3832313882398012\n",
      "training loss: 0.36619083482986753\n",
      "training loss: 0.32631114505260483\n",
      "training loss: 0.3287844518337988\n",
      "training loss: 0.28671425896231084\n",
      "training loss: 0.32208352733611717\n",
      "training loss: 0.34719918830506913\n",
      "training loss: 0.40428007317590525\n",
      "training loss: 0.4463762632694852\n",
      "training loss: 0.292464253695216\n",
      "training loss: 0.2922083174931322\n",
      "training loss: 0.46034679689793845\n",
      "training loss: 0.3685866097142207\n",
      "training loss: 0.41473547304049135\n",
      "training loss: 0.31155383552148125\n",
      "training loss: 0.31516974316649793\n",
      "training loss: 0.36082144641515695\n",
      "training loss: 0.3470888595561178\n",
      "training loss: 0.46195712183402066\n",
      "training loss: 0.3394589086407086\n",
      "training loss: 0.3052972440990561\n",
      "training loss: 0.28239672842815705\n",
      "training loss: 0.3868849653050711\n",
      "training loss: 0.45427216391573894\n",
      "training loss: 0.36402437326525844\n",
      "training loss: 0.40157841358519364\n",
      "training loss: 0.3824894815071093\n",
      "training loss: 0.26168887126948903\n",
      "training loss: 0.42011656994494845\n",
      "training loss: 0.35845413298107814\n",
      "training loss: 0.4823073321170523\n",
      "training loss: 0.33846226005262\n",
      "training loss: 0.31439181037600067\n",
      "validation_loss: 0.48427723952383067\n",
      "\n",
      "Epoch 5\n",
      "------------------------------\n",
      "training loss: 0.3561627283294001\n",
      "training loss: 0.21647563376682227\n",
      "training loss: 0.28223428517128923\n",
      "training loss: 0.2902399540125248\n",
      "training loss: 0.3641959282675816\n",
      "training loss: 0.3970237026287941\n",
      "training loss: 0.3535937515224214\n",
      "training loss: 0.36475442108445577\n",
      "training loss: 0.315698854378752\n",
      "training loss: 0.36961423393380755\n",
      "training loss: 0.33807018820760276\n",
      "training loss: 0.40474456403258957\n",
      "training loss: 0.3699139433333767\n",
      "training loss: 0.44510230857588795\n",
      "training loss: 0.4083591131461799\n",
      "training loss: 0.30585689410607303\n",
      "training loss: 0.30220605550857726\n",
      "training loss: 0.3945503646119323\n",
      "training loss: 0.44548586864789286\n",
      "training loss: 0.390613743900758\n",
      "training loss: 0.28839448396276796\n",
      "training loss: 0.4446985214750043\n",
      "training loss: 0.46712034440396566\n",
      "training loss: 0.4001027128437363\n",
      "training loss: 0.23612613730925658\n",
      "training loss: 0.351885572352403\n",
      "training loss: 0.2723119995346497\n",
      "training loss: 0.3772727264201967\n",
      "training loss: 0.3532319071235543\n",
      "training loss: 0.28772293517093206\n",
      "training loss: 0.28616448709555925\n",
      "training loss: 0.3109288572601872\n",
      "training loss: 0.3858374968777298\n",
      "training loss: 0.3682362110717804\n",
      "training loss: 0.4063424566735284\n",
      "validation_loss: 0.477708773488806\n",
      "\n",
      "Epoch 6\n",
      "------------------------------\n",
      "training loss: 0.4225738076532252\n",
      "training loss: 0.30877049891446406\n",
      "training loss: 0.41410733157841606\n",
      "training loss: 0.3047890290929718\n",
      "training loss: 0.34488323838556423\n",
      "training loss: 0.2893320056948687\n",
      "training loss: 0.41534715199253697\n",
      "training loss: 0.37848680357201375\n",
      "training loss: 0.3127075404525385\n",
      "training loss: 0.32933234792231814\n",
      "training loss: 0.37043620550612105\n",
      "training loss: 0.4038864256508532\n",
      "training loss: 0.33707172596698\n",
      "training loss: 0.35134231355070367\n",
      "training loss: 0.33180531331952806\n",
      "training loss: 0.3978037921037412\n",
      "training loss: 0.3882174887158999\n",
      "training loss: 0.45467521102953695\n",
      "training loss: 0.3156849678571871\n",
      "training loss: 0.3286952011813264\n",
      "training loss: 0.3234383853524923\n",
      "training loss: 0.4050101585003722\n",
      "training loss: 0.3451714509842714\n",
      "training loss: 0.34329782582753976\n",
      "training loss: 0.4295550026155615\n",
      "training loss: 0.29885708354791857\n",
      "training loss: 0.3162369907754419\n",
      "training loss: 0.3546948520043952\n",
      "training loss: 0.460813019315683\n",
      "training loss: 0.49856332278402987\n",
      "training loss: 0.34841800523456185\n",
      "training loss: 0.22773310465159738\n",
      "training loss: 0.3332320039720071\n",
      "training loss: 0.394232088651188\n",
      "training loss: 0.2501327946965557\n",
      "validation_loss: 0.46721762836547703\n",
      "\n",
      "Epoch 7\n",
      "------------------------------\n",
      "training loss: 0.3667817856391775\n",
      "training loss: 0.28441538439761643\n",
      "training loss: 0.34723101953528385\n",
      "training loss: 0.40496081605797374\n",
      "training loss: 0.3313296280410941\n",
      "training loss: 0.28943388233482437\n",
      "training loss: 0.3916841323394692\n",
      "training loss: 0.4065864292300466\n",
      "training loss: 0.3103677473673451\n",
      "training loss: 0.3902724896842847\n",
      "training loss: 0.30584533540531994\n",
      "training loss: 0.32029157784214474\n",
      "training loss: 0.3442806758430379\n",
      "training loss: 0.38077269440487727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2877949400199577\n",
      "training loss: 0.44109853686386485\n",
      "training loss: 0.29689625465192876\n",
      "training loss: 0.2501363315285948\n",
      "training loss: 0.29101260631408876\n",
      "training loss: 0.4217535567651157\n",
      "training loss: 0.3587837568757823\n",
      "training loss: 0.32715852338516016\n",
      "training loss: 0.4799084645998664\n",
      "training loss: 0.33549081902617217\n",
      "training loss: 0.4246476794040609\n",
      "training loss: 0.38366235988637526\n",
      "training loss: 0.3179873853225763\n",
      "training loss: 0.4903747152768483\n",
      "training loss: 0.2483423599703019\n",
      "training loss: 0.42193501951071083\n",
      "training loss: 0.33320978423344966\n",
      "training loss: 0.4234279247190807\n",
      "training loss: 0.49183025252918017\n",
      "training loss: 0.3704838127439143\n",
      "training loss: 0.34406307617202403\n",
      "validation_loss: 0.4900717351270032\n",
      "\n",
      "Epoch 8\n",
      "------------------------------\n",
      "training loss: 0.31213829140273447\n",
      "training loss: 0.35724992065331207\n",
      "training loss: 0.3999616264679935\n",
      "training loss: 0.3852661991572677\n",
      "training loss: 0.3235863106915531\n",
      "training loss: 0.31971169861834825\n",
      "training loss: 0.332094858236851\n",
      "training loss: 0.36726769985241847\n",
      "training loss: 0.3575855710630276\n",
      "training loss: 0.4549378996008477\n",
      "training loss: 0.2780820900411345\n",
      "training loss: 0.3339384628379412\n",
      "training loss: 0.2851243526271901\n",
      "training loss: 0.3196994030790665\n",
      "training loss: 0.3649345766673741\n",
      "training loss: 0.36350984093471195\n",
      "training loss: 0.30395179708953945\n",
      "training loss: 0.36086584272808975\n",
      "training loss: 0.36601541441643576\n",
      "training loss: 0.29013622826023494\n",
      "training loss: 0.33262483423706724\n",
      "training loss: 0.46107321409523366\n",
      "training loss: 0.31558015241237625\n",
      "training loss: 0.2538932371959618\n",
      "training loss: 0.3316247184453823\n",
      "training loss: 0.2719739936030237\n",
      "training loss: 0.33055008217387694\n",
      "training loss: 0.42697318448103033\n",
      "training loss: 0.4688400485122475\n",
      "training loss: 0.3161271177787421\n",
      "training loss: 0.46474182956037113\n",
      "training loss: 0.5090440433226467\n",
      "training loss: 0.34968759459392457\n",
      "training loss: 0.36072158500493967\n",
      "training loss: 0.4521647599116932\n",
      "validation_loss: 0.46895395249195726\n",
      "\n",
      "Epoch 9\n",
      "------------------------------\n",
      "training loss: 0.42181750973760246\n",
      "training loss: 0.3221570121717468\n",
      "training loss: 0.2904823214918815\n",
      "training loss: 0.33509630810411184\n",
      "training loss: 0.28773193348115456\n",
      "training loss: 0.25565638067238067\n",
      "training loss: 0.299944118239946\n",
      "training loss: 0.3914050430411794\n",
      "training loss: 0.33408608477882806\n",
      "training loss: 0.31610617251521034\n",
      "training loss: 0.49318915893207305\n",
      "training loss: 0.3749411877956663\n",
      "training loss: 0.30663583032523095\n",
      "training loss: 0.39449659880367105\n",
      "training loss: 0.31693049007299123\n",
      "training loss: 0.3628890968940686\n",
      "training loss: 0.2873660174020188\n",
      "training loss: 0.40561302613125005\n",
      "training loss: 0.32043613273184746\n",
      "training loss: 0.3106484302452736\n",
      "training loss: 0.24004904084980808\n",
      "training loss: 0.41165584581613074\n",
      "training loss: 0.3437118278453363\n",
      "training loss: 0.31764767494689294\n",
      "training loss: 0.4292144937801641\n",
      "training loss: 0.33421356455514795\n",
      "training loss: 0.39576162565557754\n",
      "training loss: 0.4812970068668892\n",
      "training loss: 0.3486467148737574\n",
      "training loss: 0.35964980994787765\n",
      "training loss: 0.4029459365495859\n",
      "training loss: 0.3704789899837488\n",
      "training loss: 0.32963788894697243\n",
      "training loss: 0.3341798114989433\n",
      "training loss: 0.39356443480224695\n",
      "validation_loss: 0.49047368380074985\n",
      "\n",
      "Epoch 10\n",
      "------------------------------\n",
      "training loss: 0.32114096069635706\n",
      "training loss: 0.2361003905369239\n",
      "training loss: 0.3257192065200434\n",
      "training loss: 0.3353097009928956\n",
      "training loss: 0.35541556827811294\n",
      "training loss: 0.4058625197904439\n",
      "training loss: 0.3890464609668197\n",
      "training loss: 0.2664947609000069\n",
      "training loss: 0.44266625010495775\n",
      "training loss: 0.3581950525729553\n",
      "training loss: 0.29695336614429835\n",
      "training loss: 0.37900840844049527\n",
      "training loss: 0.44065118905149575\n",
      "training loss: 0.3597319016804249\n",
      "training loss: 0.2706626948538542\n",
      "training loss: 0.2650030782340764\n",
      "training loss: 0.2633016265693095\n",
      "training loss: 0.3282331936074479\n",
      "training loss: 0.36980946798837977\n",
      "training loss: 0.4237614244269207\n",
      "training loss: 0.36019530593548554\n",
      "training loss: 0.42168100388098767\n",
      "training loss: 0.3826596458025233\n",
      "training loss: 0.30674864109701955\n",
      "training loss: 0.28608944369625533\n",
      "training loss: 0.33259467283329286\n",
      "training loss: 0.348215752323822\n",
      "training loss: 0.47490396278488334\n",
      "training loss: 0.4727462190722144\n",
      "training loss: 0.40618487088279154\n",
      "training loss: 0.30514685631278554\n",
      "training loss: 0.43068877039666403\n",
      "training loss: 0.3258073451728706\n",
      "training loss: 0.3847790481348056\n",
      "training loss: 0.35206833692865075\n",
      "validation_loss: 0.4877961068871358\n",
      "\n",
      "Epoch 11\n",
      "------------------------------\n",
      "training loss: 0.34541310274300485\n",
      "training loss: 0.34286251131526113\n",
      "training loss: 0.3139426059049583\n",
      "training loss: 0.3560399396688808\n",
      "training loss: 0.423625893672006\n",
      "training loss: 0.28683133783357334\n",
      "training loss: 0.3403106202988784\n",
      "training loss: 0.31368976005855986\n",
      "training loss: 0.2892688478056516\n",
      "training loss: 0.4762264204085659\n",
      "training loss: 0.26742544627824827\n",
      "training loss: 0.3165367056975083\n",
      "training loss: 0.3653876337035399\n",
      "training loss: 0.32087227508134675\n",
      "training loss: 0.30309885286151256\n",
      "training loss: 0.3558819838017371\n",
      "training loss: 0.3428017231659396\n",
      "training loss: 0.37526335936200667\n",
      "training loss: 0.35237620634079575\n",
      "training loss: 0.27843505721655676\n",
      "training loss: 0.3300072779769835\n",
      "training loss: 0.3273036847408366\n",
      "training loss: 0.358826110887976\n",
      "training loss: 0.36938891748607827\n",
      "training loss: 0.4143509857257595\n",
      "training loss: 0.3734070954465119\n",
      "training loss: 0.32959847556376415\n",
      "training loss: 0.34781260234653016\n",
      "training loss: 0.36043579789387875\n",
      "training loss: 0.23426838837975084\n",
      "training loss: 0.4980106294574216\n",
      "training loss: 0.37678429037085154\n",
      "training loss: 0.5141330100610503\n",
      "training loss: 0.35229206435978994\n",
      "training loss: 0.3704210728171893\n",
      "validation_loss: 0.4625087730174436\n",
      "\n",
      "Epoch 12\n",
      "------------------------------\n",
      "training loss: 0.37938529290153383\n",
      "training loss: 0.47629343663164264\n",
      "training loss: 0.3848612653567943\n",
      "training loss: 0.36792001905356303\n",
      "training loss: 0.3108491022500493\n",
      "training loss: 0.3653526497325674\n",
      "training loss: 0.23980201401416706\n",
      "training loss: 0.3824729012174066\n",
      "training loss: 0.3535549763176914\n",
      "training loss: 0.286477740031296\n",
      "training loss: 0.38714761261478997\n",
      "training loss: 0.3448925148655053\n",
      "training loss: 0.30274950722523497\n",
      "training loss: 0.40344146695193556\n",
      "training loss: 0.3516244793520309\n",
      "training loss: 0.27210037224445843\n",
      "training loss: 0.34400194271573126\n",
      "training loss: 0.43278654361703955\n",
      "training loss: 0.2692669785112958\n",
      "training loss: 0.3806541273847688\n",
      "training loss: 0.28897030915633876\n",
      "training loss: 0.43392669048625976\n",
      "training loss: 0.3701508053824364\n",
      "training loss: 0.30577337425609585\n",
      "training loss: 0.4116569023156444\n",
      "training loss: 0.4513899820973165\n",
      "training loss: 0.43083353597576207\n",
      "training loss: 0.29756273834206015\n",
      "training loss: 0.4433136429881415\n",
      "training loss: 0.2847514014564513\n",
      "training loss: 0.4339655846800088\n",
      "training loss: 0.3572952786398673\n",
      "training loss: 0.274791015907831\n",
      "training loss: 0.3463865178346168\n",
      "training loss: 0.28820128581065546\n",
      "validation_loss: 0.4741634631340141\n",
      "\n",
      "Epoch 13\n",
      "------------------------------\n",
      "training loss: 0.3070194308689679\n",
      "training loss: 0.2823166894250608\n",
      "training loss: 0.48518574288274974\n",
      "training loss: 0.327491248223123\n",
      "training loss: 0.33530146108874986\n",
      "training loss: 0.2844160954793915\n",
      "training loss: 0.3019026525797381\n",
      "training loss: 0.2872605040695635\n",
      "training loss: 0.571303374239942\n",
      "training loss: 0.31791031961697624\n",
      "training loss: 0.29103428350237665\n",
      "training loss: 0.43958152102681197\n",
      "training loss: 0.3547449990272435\n",
      "training loss: 0.3569954246380439\n",
      "training loss: 0.360202395846527\n",
      "training loss: 0.4347227943827602\n",
      "training loss: 0.47820192278191825\n",
      "training loss: 0.4184405097778654\n",
      "training loss: 0.32269461706484437\n",
      "training loss: 0.32550142244873315\n",
      "training loss: 0.3359936372523953\n",
      "training loss: 0.26347473302587787\n",
      "training loss: 0.2859656032341536\n",
      "training loss: 0.43033556900322084\n",
      "training loss: 0.3593833126541017\n",
      "training loss: 0.37538616721351903\n",
      "training loss: 0.35623977384035244\n",
      "training loss: 0.3678828292880644\n",
      "training loss: 0.34747601360199953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3446476301307848\n",
      "training loss: 0.29848489373843223\n",
      "training loss: 0.3848371884568769\n",
      "training loss: 0.3642780870931165\n",
      "training loss: 0.39021364512220313\n",
      "training loss: 0.31974930889766257\n",
      "validation_loss: 0.461613980359588\n",
      "\n",
      "Epoch 14\n",
      "------------------------------\n",
      "training loss: 0.29614488277053397\n",
      "training loss: 0.2919779372461198\n",
      "training loss: 0.4031136847842572\n",
      "training loss: 0.4051319673024409\n",
      "training loss: 0.35496486695698193\n",
      "training loss: 0.27130069784529953\n",
      "training loss: 0.3996614495178801\n",
      "training loss: 0.45155929115136134\n",
      "training loss: 0.5105044214134159\n",
      "training loss: 0.3790034543769525\n",
      "training loss: 0.30113527528541456\n",
      "training loss: 0.31750867066788485\n",
      "training loss: 0.4024144548291952\n",
      "training loss: 0.35528990127775617\n",
      "training loss: 0.3781441686684343\n",
      "training loss: 0.32034224292030555\n",
      "training loss: 0.3123887523601388\n",
      "training loss: 0.4019084715691861\n",
      "training loss: 0.268070134400914\n",
      "training loss: 0.3955611348907405\n",
      "training loss: 0.4157247053596075\n",
      "training loss: 0.2948026302781909\n",
      "training loss: 0.33927589435403205\n",
      "training loss: 0.3159450055601701\n",
      "training loss: 0.34789040537225446\n",
      "training loss: 0.3171384004309948\n",
      "training loss: 0.43253283894329797\n",
      "training loss: 0.34125757328267353\n",
      "training loss: 0.37777306989315546\n",
      "training loss: 0.3430066598212943\n",
      "training loss: 0.45376171413059635\n",
      "training loss: 0.32776574953037196\n",
      "training loss: 0.3624495835450944\n",
      "training loss: 0.3585186226281803\n",
      "training loss: 0.24073402281640482\n",
      "validation_loss: 0.4701829677914693\n",
      "\n",
      "Epoch 15\n",
      "------------------------------\n",
      "training loss: 0.4195126159002416\n",
      "training loss: 0.2568197193052583\n",
      "training loss: 0.2365206039286568\n",
      "training loss: 0.3465732210872375\n",
      "training loss: 0.23456929827123532\n",
      "training loss: 0.2941676855130936\n",
      "training loss: 0.4096165746989573\n",
      "training loss: 0.3025031690113246\n",
      "training loss: 0.4028228854192275\n",
      "training loss: 0.4159422671523862\n",
      "training loss: 0.31507902830810053\n",
      "training loss: 0.30013128400636563\n",
      "training loss: 0.3207087967871576\n",
      "training loss: 0.32345465620459435\n",
      "training loss: 0.33740002718786855\n",
      "training loss: 0.2762645896355207\n",
      "training loss: 0.45914056894295074\n",
      "training loss: 0.30981260000236943\n",
      "training loss: 0.3099169085567155\n",
      "training loss: 0.3672759715655775\n",
      "training loss: 0.3998279604519848\n",
      "training loss: 0.4216229144611134\n",
      "training loss: 0.5412725641412931\n",
      "training loss: 0.493556290040724\n",
      "training loss: 0.32805427857282665\n",
      "training loss: 0.33985204769731353\n",
      "training loss: 0.41163503400370244\n",
      "training loss: 0.35301740547583904\n",
      "training loss: 0.4495469975038031\n",
      "training loss: 0.3807529496826305\n",
      "training loss: 0.4228763339742727\n",
      "training loss: 0.30576315043832664\n",
      "training loss: 0.3414004396094697\n",
      "training loss: 0.3157971567318236\n",
      "training loss: 0.37755589643133136\n",
      "validation_loss: 0.4508695384150073\n",
      "\n",
      "Epoch 16\n",
      "------------------------------\n",
      "training loss: 0.45197925317366755\n",
      "training loss: 0.3386780250524407\n",
      "training loss: 0.26605402136789963\n",
      "training loss: 0.2899500649431138\n",
      "training loss: 0.41655702870339156\n",
      "training loss: 0.46353304321077304\n",
      "training loss: 0.42693354360279046\n",
      "training loss: 0.41148433390026184\n",
      "training loss: 0.3935211656833781\n",
      "training loss: 0.432666234158678\n",
      "training loss: 0.2662139233661583\n",
      "training loss: 0.3170357707934454\n",
      "training loss: 0.3367806478505372\n",
      "training loss: 0.3067973923671525\n",
      "training loss: 0.24661753521726495\n",
      "training loss: 0.2507971290886144\n",
      "training loss: 0.30385110720675584\n",
      "training loss: 0.314101434696513\n",
      "training loss: 0.32796831172711793\n",
      "training loss: 0.40707374944875485\n",
      "training loss: 0.32916259699961303\n",
      "training loss: 0.2955978102269819\n",
      "training loss: 0.35552776859592994\n",
      "training loss: 0.39403287591558184\n",
      "training loss: 0.4391941692940236\n",
      "training loss: 0.31533633018162616\n",
      "training loss: 0.22905331634878168\n",
      "training loss: 0.4476597146566928\n",
      "training loss: 0.30122327826439005\n",
      "training loss: 0.36847674145625203\n",
      "training loss: 0.2690159662530732\n",
      "training loss: 0.4459605085593648\n",
      "training loss: 0.45349557812763125\n",
      "training loss: 0.4425174664665246\n",
      "training loss: 0.32142684582246145\n",
      "validation_loss: 0.46130858511369865\n",
      "\n",
      "Epoch 17\n",
      "------------------------------\n",
      "training loss: 0.39095891793052945\n",
      "training loss: 0.34533748225450833\n",
      "training loss: 0.3805204072419292\n",
      "training loss: 0.25644380468525924\n",
      "training loss: 0.25594125875821194\n",
      "training loss: 0.34035093517546555\n",
      "training loss: 0.4385923522526355\n",
      "training loss: 0.4694104525896546\n",
      "training loss: 0.3749183583375998\n",
      "training loss: 0.3998413690277539\n",
      "training loss: 0.2987067389863228\n",
      "training loss: 0.3766704147816199\n",
      "training loss: 0.35340031128922417\n",
      "training loss: 0.3306142846235252\n",
      "training loss: 0.3741645797927049\n",
      "training loss: 0.4313840694599367\n",
      "training loss: 0.2647738280425256\n",
      "training loss: 0.29515405030480907\n",
      "training loss: 0.37629581483779473\n",
      "training loss: 0.36684386569262645\n",
      "training loss: 0.2965234642154337\n",
      "training loss: 0.31418273803066765\n",
      "training loss: 0.31987835195934167\n",
      "training loss: 0.37165923172757176\n",
      "training loss: 0.40651347144429567\n",
      "training loss: 0.3213378164621827\n",
      "training loss: 0.34842851770709105\n",
      "training loss: 0.30275352893485435\n",
      "training loss: 0.42781981536543756\n",
      "training loss: 0.3015365735403611\n",
      "training loss: 0.511299722349695\n",
      "training loss: 0.4199070827064952\n",
      "training loss: 0.4737863234811084\n",
      "training loss: 0.3131055126213323\n",
      "training loss: 0.3073678010996082\n",
      "validation_loss: 0.4623129246684533\n",
      "\n",
      "Epoch 18\n",
      "------------------------------\n",
      "training loss: 0.3382462327709072\n",
      "training loss: 0.26976028730015966\n",
      "training loss: 0.3677328219683841\n",
      "training loss: 0.37405296714568975\n",
      "training loss: 0.2618232352190535\n",
      "training loss: 0.3992950652334912\n",
      "training loss: 0.3491359807248227\n",
      "training loss: 0.398768721272063\n",
      "training loss: 0.3038645555849325\n",
      "training loss: 0.4156859925662866\n",
      "training loss: 0.2925322299315394\n",
      "training loss: 0.32939643692512616\n",
      "training loss: 0.30985153688428\n",
      "training loss: 0.4076581025714768\n",
      "training loss: 0.3760840854680646\n",
      "training loss: 0.3038376858313495\n",
      "training loss: 0.3879488083799993\n",
      "training loss: 0.3760444946296138\n",
      "training loss: 0.5052066169920727\n",
      "training loss: 0.27732757286023113\n",
      "training loss: 0.4752185621555327\n",
      "training loss: 0.263148392692101\n",
      "training loss: 0.385513731125684\n",
      "training loss: 0.4313292765675578\n",
      "training loss: 0.290452877280477\n",
      "training loss: 0.3759456119721199\n",
      "training loss: 0.3445907998141047\n",
      "training loss: 0.3412090622982942\n",
      "training loss: 0.4278790976118853\n",
      "training loss: 0.3814541280811682\n",
      "training loss: 0.2533819726803631\n",
      "training loss: 0.45927733270509635\n",
      "training loss: 0.30956518187792426\n",
      "training loss: 0.4901862800919116\n",
      "training loss: 0.3059875761184958\n",
      "validation_loss: 0.45665844036792974\n",
      "\n",
      "Epoch 19\n",
      "------------------------------\n",
      "training loss: 0.2962549003654567\n",
      "training loss: 0.32643419823369185\n",
      "training loss: 0.40507598867770866\n",
      "training loss: 0.3297989479012904\n",
      "training loss: 0.38484016103110663\n",
      "training loss: 0.2977564745836935\n",
      "training loss: 0.2717221637665807\n",
      "training loss: 0.30777410939615946\n",
      "training loss: 0.2817314179298046\n",
      "training loss: 0.4524800496986427\n",
      "training loss: 0.4236756541504292\n",
      "training loss: 0.4295087920950027\n",
      "training loss: 0.31450838239438783\n",
      "training loss: 0.3772796652121178\n",
      "training loss: 0.33050014917582304\n",
      "training loss: 0.41771917459867836\n",
      "training loss: 0.32671124779569255\n",
      "training loss: 0.3939481121441713\n",
      "training loss: 0.44666355825727805\n",
      "training loss: 0.3161498044102336\n",
      "training loss: 0.4100152147854533\n",
      "training loss: 0.3031451328914409\n",
      "training loss: 0.3335139177867768\n",
      "training loss: 0.26308872707161074\n",
      "training loss: 0.31477926077796153\n",
      "training loss: 0.3302193049311063\n",
      "training loss: 0.2841866822470729\n",
      "training loss: 0.4049901599429199\n",
      "training loss: 0.3256602779266541\n",
      "training loss: 0.4412877120032499\n",
      "training loss: 0.4053261638089316\n",
      "training loss: 0.37521668394867447\n",
      "training loss: 0.4372432598178693\n",
      "training loss: 0.3005415365605586\n",
      "training loss: 0.2904663698283548\n",
      "validation_loss: 0.45674964109541033\n",
      "\n",
      "Epoch 20\n",
      "------------------------------\n",
      "training loss: 0.3782615537069796\n",
      "training loss: 0.4021365279538463\n",
      "training loss: 0.44347901993853156\n",
      "training loss: 0.4061439048633133\n",
      "training loss: 0.41159557847184486\n",
      "training loss: 0.3180882650721833\n",
      "training loss: 0.36580789882165843\n",
      "training loss: 0.2650370913195729\n",
      "training loss: 0.4123664990971156\n",
      "training loss: 0.383023142159293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3653166387087913\n",
      "training loss: 0.35190331732753294\n",
      "training loss: 0.3538439843317815\n",
      "training loss: 0.3686511366299237\n",
      "training loss: 0.31989059110506785\n",
      "training loss: 0.2813458202412039\n",
      "training loss: 0.27813334770872644\n",
      "training loss: 0.31642683070138444\n",
      "training loss: 0.41297744062059794\n",
      "training loss: 0.33572500134825534\n",
      "training loss: 0.36604956656941795\n",
      "training loss: 0.4570468156144489\n",
      "training loss: 0.25566953457020647\n",
      "training loss: 0.3811689388954983\n",
      "training loss: 0.30886140629138026\n",
      "training loss: 0.4060403915360803\n",
      "training loss: 0.3401118620302532\n",
      "training loss: 0.48871529533844293\n",
      "training loss: 0.33056628152759915\n",
      "training loss: 0.26613703568240454\n",
      "training loss: 0.3973741194083436\n",
      "training loss: 0.27441176763692054\n",
      "training loss: 0.3107455599516561\n",
      "training loss: 0.34938785486709095\n",
      "training loss: 0.38540423648941213\n",
      "validation_loss: 0.46423122873153605\n",
      "\n",
      "Epoch 21\n",
      "------------------------------\n",
      "training loss: 0.317483552822232\n",
      "training loss: 0.22559059712937596\n",
      "training loss: 0.26991127352652255\n",
      "training loss: 0.43594272830261616\n",
      "training loss: 0.36136126253695694\n",
      "training loss: 0.26831513740617086\n",
      "training loss: 0.3375595754858023\n",
      "training loss: 0.3048778144448079\n",
      "training loss: 0.39349420715783706\n",
      "training loss: 0.31962296460784273\n",
      "training loss: 0.3878608355106553\n",
      "training loss: 0.22668210726177904\n",
      "training loss: 0.4261294875450767\n",
      "training loss: 0.36009466579525906\n",
      "training loss: 0.3307646327306065\n",
      "training loss: 0.400384457565724\n",
      "training loss: 0.5059299796307459\n",
      "training loss: 0.39935573923095946\n",
      "training loss: 0.40803285010893886\n",
      "training loss: 0.27335027869645273\n",
      "training loss: 0.40787807567903656\n",
      "training loss: 0.3284913492367195\n",
      "training loss: 0.43507336885496445\n",
      "training loss: 0.4301824096195196\n",
      "training loss: 0.2831932400816004\n",
      "training loss: 0.3856578479050404\n",
      "training loss: 0.42929861377867384\n",
      "training loss: 0.32590065781463634\n",
      "training loss: 0.2513796458671095\n",
      "training loss: 0.3836599018082779\n",
      "training loss: 0.30559846791717293\n",
      "training loss: 0.3478598652544315\n",
      "training loss: 0.3913663542914583\n",
      "training loss: 0.4216111042952616\n",
      "training loss: 0.4070318368674816\n",
      "validation_loss: 0.46212678606061547\n",
      "\n",
      "Epoch 22\n",
      "------------------------------\n",
      "training loss: 0.43264885147144017\n",
      "training loss: 0.47512139767991357\n",
      "training loss: 0.2547280714736553\n",
      "training loss: 0.37358341816685653\n",
      "training loss: 0.370790644865483\n",
      "training loss: 0.38568876957091563\n",
      "training loss: 0.2929404502866237\n",
      "training loss: 0.41475914715887485\n",
      "training loss: 0.39705110298084034\n",
      "training loss: 0.310061171370819\n",
      "training loss: 0.3004908088969387\n",
      "training loss: 0.363610907465918\n",
      "training loss: 0.29669709633875757\n",
      "training loss: 0.4776920863918349\n",
      "training loss: 0.4158112001698464\n",
      "training loss: 0.29298900020698054\n",
      "training loss: 0.3848293569302041\n",
      "training loss: 0.3988558567111613\n",
      "training loss: 0.4757026303037128\n",
      "training loss: 0.27587322204141856\n",
      "training loss: 0.38084482968341943\n",
      "training loss: 0.3641341547941647\n",
      "training loss: 0.24028397340863195\n",
      "training loss: 0.2588464973698865\n",
      "training loss: 0.3857448310383188\n",
      "training loss: 0.38593105726834437\n",
      "training loss: 0.29780277177575043\n",
      "training loss: 0.32225029502506003\n",
      "training loss: 0.37108801479640535\n",
      "training loss: 0.3631913726089988\n",
      "training loss: 0.4290602322475843\n",
      "training loss: 0.3319645815086551\n",
      "training loss: 0.3895892575781909\n",
      "training loss: 0.3452353088393647\n",
      "training loss: 0.366143119186645\n",
      "validation_loss: 0.4430533126668827\n",
      "\n",
      "Epoch 23\n",
      "------------------------------\n",
      "training loss: 0.4012329505942762\n",
      "training loss: 0.4324847324781149\n",
      "training loss: 0.3736464290962249\n",
      "training loss: 0.3182549853409\n",
      "training loss: 0.33674929616201554\n",
      "training loss: 0.39202813782354495\n",
      "training loss: 0.35535773100047663\n",
      "training loss: 0.2710754290713885\n",
      "training loss: 0.22066559737941133\n",
      "training loss: 0.3518679609223\n",
      "training loss: 0.40406470784299925\n",
      "training loss: 0.31981304516230014\n",
      "training loss: 0.28498859442394175\n",
      "training loss: 0.3296873635182078\n",
      "training loss: 0.3113228007497037\n",
      "training loss: 0.3039453060892265\n",
      "training loss: 0.30759053355937793\n",
      "training loss: 0.30922094232022573\n",
      "training loss: 0.3171232400403096\n",
      "training loss: 0.30450620054383765\n",
      "training loss: 0.39124285461367436\n",
      "training loss: 0.4453284007789625\n",
      "training loss: 0.32770684655691185\n",
      "training loss: 0.3629693202391718\n",
      "training loss: 0.42264778391579283\n",
      "training loss: 0.36702852453876406\n",
      "training loss: 0.43439321464829844\n",
      "training loss: 0.4308822515407883\n",
      "training loss: 0.32566077463969123\n",
      "training loss: 0.34686015603860143\n",
      "training loss: 0.39087638439982586\n",
      "training loss: 0.32864641707390546\n",
      "training loss: 0.42103914724528296\n",
      "training loss: 0.3173797758038404\n",
      "training loss: 0.38033858087146655\n",
      "validation_loss: 0.4555381139514478\n",
      "\n",
      "Epoch 24\n",
      "------------------------------\n",
      "training loss: 0.33853442891402663\n",
      "training loss: 0.29560909222957205\n",
      "training loss: 0.4278702364998753\n",
      "training loss: 0.2520633889680357\n",
      "training loss: 0.3609463383544062\n",
      "training loss: 0.494280873334485\n",
      "training loss: 0.3294912326075882\n",
      "training loss: 0.28004658491722695\n",
      "training loss: 0.39083791884186214\n",
      "training loss: 0.3596611434192164\n",
      "training loss: 0.3533680580555756\n",
      "training loss: 0.2679314766437892\n",
      "training loss: 0.35754503364907575\n",
      "training loss: 0.3669641098662396\n",
      "training loss: 0.26024217413185396\n",
      "training loss: 0.35245342601512675\n",
      "training loss: 0.3019777937710842\n",
      "training loss: 0.323246216726302\n",
      "training loss: 0.4802171157887824\n",
      "training loss: 0.31455530561111117\n",
      "training loss: 0.3662729122392216\n",
      "training loss: 0.3974354392308396\n",
      "training loss: 0.43484039336355634\n",
      "training loss: 0.2916725913991104\n",
      "training loss: 0.35392455693705416\n",
      "training loss: 0.47832117858633866\n",
      "training loss: 0.3652155662628866\n",
      "training loss: 0.48241993510733894\n",
      "training loss: 0.2624890961006167\n",
      "training loss: 0.37455766749713804\n",
      "training loss: 0.3950496142951306\n",
      "training loss: 0.4199475640396122\n",
      "training loss: 0.43206925546284763\n",
      "training loss: 0.30415873711703173\n",
      "training loss: 0.25302182027016895\n",
      "validation_loss: 0.4772094835208915\n",
      "\n",
      "Epoch 25\n",
      "------------------------------\n",
      "training loss: 0.34501163335247836\n",
      "training loss: 0.367689392274915\n",
      "training loss: 0.3801191237619787\n",
      "training loss: 0.40093854367580206\n",
      "training loss: 0.24896254617371597\n",
      "training loss: 0.36870117337557756\n",
      "training loss: 0.34463829820786485\n",
      "training loss: 0.28861605410187624\n",
      "training loss: 0.3295142462115564\n",
      "training loss: 0.44277710642018975\n",
      "training loss: 0.3216733277992171\n",
      "training loss: 0.3555351948738098\n",
      "training loss: 0.29448035120178245\n",
      "training loss: 0.31437511794883904\n",
      "training loss: 0.3054995999735911\n",
      "training loss: 0.35853980122017676\n",
      "training loss: 0.32954622206278145\n",
      "training loss: 0.3357921957828512\n",
      "training loss: 0.3760486503478023\n",
      "training loss: 0.44891981030654277\n",
      "training loss: 0.414022234895092\n",
      "training loss: 0.3381873904084023\n",
      "training loss: 0.2227830301478116\n",
      "training loss: 0.3475286376051372\n",
      "training loss: 0.3178528896017178\n",
      "training loss: 0.36636731737409717\n",
      "training loss: 0.2600726349089473\n",
      "training loss: 0.46908981971407226\n",
      "training loss: 0.4117416715418608\n",
      "training loss: 0.4412107178859878\n",
      "training loss: 0.2791873557284407\n",
      "training loss: 0.3901959894808169\n",
      "training loss: 0.4809455556116154\n",
      "training loss: 0.35594077796296913\n",
      "training loss: 0.4642761534638703\n",
      "validation_loss: 0.4504175870797089\n",
      "\n",
      "Epoch 26\n",
      "------------------------------\n",
      "training loss: 0.3823940374353515\n",
      "training loss: 0.3464804674828338\n",
      "training loss: 0.40706998713038045\n",
      "training loss: 0.28288931553659497\n",
      "training loss: 0.42572182134332254\n",
      "training loss: 0.2327385904352741\n",
      "training loss: 0.31180619772290813\n",
      "training loss: 0.2926124782184434\n",
      "training loss: 0.3440212183917265\n",
      "training loss: 0.35688373935598067\n",
      "training loss: 0.49540364597924963\n",
      "training loss: 0.35730680998880415\n",
      "training loss: 0.38457046880110285\n",
      "training loss: 0.3445321766937559\n",
      "training loss: 0.3639244466748846\n",
      "training loss: 0.3667320992459281\n",
      "training loss: 0.4124814473709921\n",
      "training loss: 0.3425622963821297\n",
      "training loss: 0.36011341997011187\n",
      "training loss: 0.3351632477280509\n",
      "training loss: 0.4821661237688386\n",
      "training loss: 0.3366222829942126\n",
      "training loss: 0.284321800990474\n",
      "training loss: 0.34444535141565213\n",
      "training loss: 0.34542972413270034\n",
      "training loss: 0.37236015029775443\n",
      "training loss: 0.3577866073989571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.363727911520109\n",
      "training loss: 0.29840180050116033\n",
      "training loss: 0.2676374551013578\n",
      "training loss: 0.41106194055298145\n",
      "training loss: 0.2805683468627467\n",
      "training loss: 0.4197936079410283\n",
      "training loss: 0.3111356746616093\n",
      "training loss: 0.3439850732681316\n",
      "validation_loss: 0.47097048012388926\n",
      "\n",
      "Epoch 27\n",
      "------------------------------\n",
      "training loss: 0.3304924563952227\n",
      "training loss: 0.365235770138097\n",
      "training loss: 0.3437279357053285\n",
      "training loss: 0.3041734415967949\n",
      "training loss: 0.3955783297698508\n",
      "training loss: 0.34436521000050563\n",
      "training loss: 0.32397171773729494\n",
      "training loss: 0.37678935810291475\n",
      "training loss: 0.4620275250632494\n",
      "training loss: 0.37566188532669914\n",
      "training loss: 0.3585241053323625\n",
      "training loss: 0.46278327276944764\n",
      "training loss: 0.3789143941673774\n",
      "training loss: 0.2811161988987442\n",
      "training loss: 0.24780458320187107\n",
      "training loss: 0.3454397490457632\n",
      "training loss: 0.4060034449266095\n",
      "training loss: 0.34011190426187565\n",
      "training loss: 0.3677619420037081\n",
      "training loss: 0.5004158557698247\n",
      "training loss: 0.31227593808922394\n",
      "training loss: 0.3145067989214385\n",
      "training loss: 0.4944269598968185\n",
      "training loss: 0.32783102021145166\n",
      "training loss: 0.4038840478670886\n",
      "training loss: 0.3536377932654432\n",
      "training loss: 0.2984656991971133\n",
      "training loss: 0.29615719083303704\n",
      "training loss: 0.26760306600819606\n",
      "training loss: 0.3628254093253054\n",
      "training loss: 0.34957680306944894\n",
      "training loss: 0.30299390270425647\n",
      "training loss: 0.2750905718236754\n",
      "training loss: 0.4234151100978488\n",
      "training loss: 0.4880272974455147\n",
      "validation_loss: 0.4718641321911974\n",
      "\n",
      "Epoch 28\n",
      "------------------------------\n",
      "training loss: 0.3048271644487977\n",
      "training loss: 0.31208128575418415\n",
      "training loss: 0.28161732121167915\n",
      "training loss: 0.41147922107295015\n",
      "training loss: 0.4275744498091808\n",
      "training loss: 0.2831311112170806\n",
      "training loss: 0.3806781236330062\n",
      "training loss: 0.2888243740055077\n",
      "training loss: 0.26503480429630144\n",
      "training loss: 0.36878804323932857\n",
      "training loss: 0.310353730886427\n",
      "training loss: 0.290950091425766\n",
      "training loss: 0.375122019302944\n",
      "training loss: 0.3359708077292817\n",
      "training loss: 0.43992653914250696\n",
      "training loss: 0.3029747530048553\n",
      "training loss: 0.37760784236947076\n",
      "training loss: 0.33946902832307385\n",
      "training loss: 0.32477473489907427\n",
      "training loss: 0.38152962019546977\n",
      "training loss: 0.4728457250059364\n",
      "training loss: 0.5067347166139462\n",
      "training loss: 0.4131647279393292\n",
      "training loss: 0.3551191014771393\n",
      "training loss: 0.2784197164948091\n",
      "training loss: 0.2827418454080907\n",
      "training loss: 0.3163323079055408\n",
      "training loss: 0.35776832352570637\n",
      "training loss: 0.3369808932552405\n",
      "training loss: 0.4953315090713386\n",
      "training loss: 0.3046098379974137\n",
      "training loss: 0.35923443089752255\n",
      "training loss: 0.33080284882656996\n",
      "training loss: 0.48140527809966444\n",
      "training loss: 0.30610247728785905\n",
      "validation_loss: 0.4816494986908412\n",
      "\n",
      "Epoch 29\n",
      "------------------------------\n",
      "training loss: 0.3315643643133717\n",
      "training loss: 0.38597447876752994\n",
      "training loss: 0.32735372346361147\n",
      "training loss: 0.3759964098486671\n",
      "training loss: 0.2670001661966671\n",
      "training loss: 0.319873593623488\n",
      "training loss: 0.380855528579923\n",
      "training loss: 0.26596468221760006\n",
      "training loss: 0.3759489391761963\n",
      "training loss: 0.3583946834833114\n",
      "training loss: 0.4211888790086232\n",
      "training loss: 0.2614124989061384\n",
      "training loss: 0.2717219848005334\n",
      "training loss: 0.2970776298008059\n",
      "training loss: 0.29220792881940727\n",
      "training loss: 0.30246640852754353\n",
      "training loss: 0.3618579262756248\n",
      "training loss: 0.5265559339660103\n",
      "training loss: 0.44787991076664185\n",
      "training loss: 0.4435858510608523\n",
      "training loss: 0.49449867244904455\n",
      "training loss: 0.3779077500142921\n",
      "training loss: 0.3757933462433721\n",
      "training loss: 0.35150153227878034\n",
      "training loss: 0.36333592738556036\n",
      "training loss: 0.3680325001947494\n",
      "training loss: 0.3424534474426764\n",
      "training loss: 0.3262894021247121\n",
      "training loss: 0.2591868163541949\n",
      "training loss: 0.41303369149332864\n",
      "training loss: 0.30319141527499593\n",
      "training loss: 0.32678843429225707\n",
      "training loss: 0.2817581915892333\n",
      "training loss: 0.37976472213238593\n",
      "training loss: 0.36746083625606846\n",
      "validation_loss: 0.46547258157905835\n",
      "\n",
      "Epoch 30\n",
      "------------------------------\n",
      "training loss: 0.3307420757241198\n",
      "training loss: 0.4066562187971431\n",
      "training loss: 0.4322336902820825\n",
      "training loss: 0.38344979303928994\n",
      "training loss: 0.44945374206516137\n",
      "training loss: 0.33402660857671435\n",
      "training loss: 0.30496537586347583\n",
      "training loss: 0.33091197263165667\n",
      "training loss: 0.28608478583832947\n",
      "training loss: 0.42615440472510274\n",
      "training loss: 0.261183326517903\n",
      "training loss: 0.4030594697370657\n",
      "training loss: 0.3857603072587517\n",
      "training loss: 0.29551898512989283\n",
      "training loss: 0.34552731955307536\n",
      "training loss: 0.45299301065751935\n",
      "training loss: 0.27799563478765776\n",
      "training loss: 0.3071638464382249\n",
      "training loss: 0.3903060408728197\n",
      "training loss: 0.2536308862749502\n",
      "training loss: 0.38012984535897887\n",
      "training loss: 0.39270605106521544\n",
      "training loss: 0.3693650152462942\n",
      "training loss: 0.3509840437107778\n",
      "training loss: 0.4035345079121544\n",
      "training loss: 0.45006317018667913\n",
      "training loss: 0.40393512563663536\n",
      "training loss: 0.3195083219332537\n",
      "training loss: 0.3763801051306018\n",
      "training loss: 0.24717024186936215\n",
      "training loss: 0.30896101858339536\n",
      "training loss: 0.38927644039620646\n",
      "training loss: 0.3823473876285425\n",
      "training loss: 0.3950294899975415\n",
      "training loss: 0.32939862930863456\n",
      "validation_loss: 0.438162813837591\n",
      "\n",
      "Epoch 31\n",
      "------------------------------\n",
      "training loss: 0.3157184881351441\n",
      "training loss: 0.2769842201713459\n",
      "training loss: 0.32077003992002573\n",
      "training loss: 0.3683204777647916\n",
      "training loss: 0.34726754369417906\n",
      "training loss: 0.5629906239319825\n",
      "training loss: 0.29057229673373514\n",
      "training loss: 0.3235151742290418\n",
      "training loss: 0.4040714656258933\n",
      "training loss: 0.2646946385303454\n",
      "training loss: 0.3170098397914262\n",
      "training loss: 0.39082236918620766\n",
      "training loss: 0.442661566737188\n",
      "training loss: 0.29309863535730984\n",
      "training loss: 0.35832385361005437\n",
      "training loss: 0.37745990716248345\n",
      "training loss: 0.40699053921278394\n",
      "training loss: 0.41821179429371114\n",
      "training loss: 0.2755544056254166\n",
      "training loss: 0.2567357574502375\n",
      "training loss: 0.3818704879231973\n",
      "training loss: 0.2813007586942331\n",
      "training loss: 0.3046553791855058\n",
      "training loss: 0.3060266279055304\n",
      "training loss: 0.2974996090636705\n",
      "training loss: 0.4764437449500201\n",
      "training loss: 0.3831370985170361\n",
      "training loss: 0.4322797085256025\n",
      "training loss: 0.42590260071097874\n",
      "training loss: 0.3970041384443175\n",
      "training loss: 0.3090500522491857\n",
      "training loss: 0.39672593956178387\n",
      "training loss: 0.3373488417526187\n",
      "training loss: 0.35388084302954664\n",
      "training loss: 0.3851407556158665\n",
      "validation_loss: 0.47626226266604543\n",
      "\n",
      "Epoch 32\n",
      "------------------------------\n",
      "training loss: 0.3506769610101037\n",
      "training loss: 0.2593789221041516\n",
      "training loss: 0.3250908615384833\n",
      "training loss: 0.42418785038709755\n",
      "training loss: 0.2186104936779884\n",
      "training loss: 0.3711140066954704\n",
      "training loss: 0.31615754609171065\n",
      "training loss: 0.30830607285693984\n",
      "training loss: 0.2961254019692569\n",
      "training loss: 0.2846581094772591\n",
      "training loss: 0.43714205735872386\n",
      "training loss: 0.43608723464320065\n",
      "training loss: 0.29596620159798476\n",
      "training loss: 0.43139873900055137\n",
      "training loss: 0.2963905895127391\n",
      "training loss: 0.3466717298104254\n",
      "training loss: 0.324173934340879\n",
      "training loss: 0.3612345096080935\n",
      "training loss: 0.3482458294231037\n",
      "training loss: 0.4125394024225625\n",
      "training loss: 0.3565738895481627\n",
      "training loss: 0.3729736963059463\n",
      "training loss: 0.36056258955233716\n",
      "training loss: 0.3023800227438187\n",
      "training loss: 0.4904252347505189\n",
      "training loss: 0.29011328062581015\n",
      "training loss: 0.41084366572838915\n",
      "training loss: 0.4386276439464655\n",
      "training loss: 0.3867278980299307\n",
      "training loss: 0.44418410855581897\n",
      "training loss: 0.29231051712034967\n",
      "training loss: 0.41174775825039434\n",
      "training loss: 0.35894122651319776\n",
      "training loss: 0.3260285735951038\n",
      "training loss: 0.42921412578125684\n",
      "validation_loss: 0.4580036771094508\n",
      "\n",
      "Epoch 33\n",
      "------------------------------\n",
      "training loss: 0.2975758986239089\n",
      "training loss: 0.3489079833401865\n",
      "training loss: 0.31009521526590106\n",
      "training loss: 0.26801576225996543\n",
      "training loss: 0.37289473840690335\n",
      "training loss: 0.4416103142593056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.36460487540825853\n",
      "training loss: 0.3168496271636104\n",
      "training loss: 0.33850721018611696\n",
      "training loss: 0.36474585505538015\n",
      "training loss: 0.41015945830673445\n",
      "training loss: 0.2935289868824475\n",
      "training loss: 0.4104122121418186\n",
      "training loss: 0.3002034887794525\n",
      "training loss: 0.3554863435819607\n",
      "training loss: 0.3524292125939428\n",
      "training loss: 0.3323757349198058\n",
      "training loss: 0.30463959035929294\n",
      "training loss: 0.261411665290143\n",
      "training loss: 0.33594715014041865\n",
      "training loss: 0.34437177902327676\n",
      "training loss: 0.5432488257087789\n",
      "training loss: 0.3909754724574691\n",
      "training loss: 0.45269774985972616\n",
      "training loss: 0.2820655507270021\n",
      "training loss: 0.3043140238491196\n",
      "training loss: 0.37166586410621677\n",
      "training loss: 0.4660180772243257\n",
      "training loss: 0.3083334876822482\n",
      "training loss: 0.33038235219013584\n",
      "training loss: 0.3650347100885483\n",
      "training loss: 0.5294776177076801\n",
      "training loss: 0.43541354850695824\n",
      "training loss: 0.32249193503172136\n",
      "training loss: 0.3630382609572553\n",
      "validation_loss: 0.46938733560119655\n",
      "\n",
      "Epoch 34\n",
      "------------------------------\n",
      "training loss: 0.40825327966798797\n",
      "training loss: 0.3354655501624802\n",
      "training loss: 0.24813107771187787\n",
      "training loss: 0.26426237314230094\n",
      "training loss: 0.4470987941286512\n",
      "training loss: 0.24707203282479895\n",
      "training loss: 0.40853333404695147\n",
      "training loss: 0.2756658706495\n",
      "training loss: 0.36803930281199426\n",
      "training loss: 0.24155667900340633\n",
      "training loss: 0.3151304461362088\n",
      "training loss: 0.3948700191289049\n",
      "training loss: 0.4168632982138024\n",
      "training loss: 0.4032539195640129\n",
      "training loss: 0.40037368806835727\n",
      "training loss: 0.3063734746443606\n",
      "training loss: 0.42358617193971443\n",
      "training loss: 0.2951776086934842\n",
      "training loss: 0.31249469222006154\n",
      "training loss: 0.3822209986072994\n",
      "training loss: 0.4676886050295434\n",
      "training loss: 0.4389809717734897\n",
      "training loss: 0.21402817669862997\n",
      "training loss: 0.33352679159925175\n",
      "training loss: 0.433766347376295\n",
      "training loss: 0.30949943545860154\n",
      "training loss: 0.4019393697634246\n",
      "training loss: 0.3140281849590247\n",
      "training loss: 0.4912134007808527\n",
      "training loss: 0.3029229634761759\n",
      "training loss: 0.3906184250337537\n",
      "training loss: 0.38050017271350955\n",
      "training loss: 0.3607051793203573\n",
      "training loss: 0.37445659132761877\n",
      "training loss: 0.3144909091201498\n",
      "validation_loss: 0.45909535668698015\n",
      "\n",
      "Epoch 35\n",
      "------------------------------\n",
      "training loss: 0.5353704906179427\n",
      "training loss: 0.3804209123028886\n",
      "training loss: 0.33806928203562164\n",
      "training loss: 0.33336088177406964\n",
      "training loss: 0.3239229754279586\n",
      "training loss: 0.3360959677048413\n",
      "training loss: 0.41270153737583315\n",
      "training loss: 0.3111260900070579\n",
      "training loss: 0.39096830590995524\n",
      "training loss: 0.35577924935729244\n",
      "training loss: 0.2942786281079316\n",
      "training loss: 0.27777433112612926\n",
      "training loss: 0.37992860276984175\n",
      "training loss: 0.28185392143088395\n",
      "training loss: 0.37679682783928004\n",
      "training loss: 0.2906506232907941\n",
      "training loss: 0.32809440031356646\n",
      "training loss: 0.40602417676265756\n",
      "training loss: 0.3145246022924948\n",
      "training loss: 0.301826993889963\n",
      "training loss: 0.3737776441995902\n",
      "training loss: 0.23393803158696982\n",
      "training loss: 0.3526515905471024\n",
      "training loss: 0.35228257535607554\n",
      "training loss: 0.33263266214089526\n",
      "training loss: 0.42492570415523234\n",
      "training loss: 0.33168879547880353\n",
      "training loss: 0.4106823531613918\n",
      "training loss: 0.24165401841062703\n",
      "training loss: 0.3762830619081888\n",
      "training loss: 0.31627510395876923\n",
      "training loss: 0.4793966068404552\n",
      "training loss: 0.44282535573587667\n",
      "training loss: 0.4703084818148727\n",
      "training loss: 0.340053152852488\n",
      "validation_loss: 0.4581129931889661\n",
      "\n",
      "Epoch 36\n",
      "------------------------------\n",
      "training loss: 0.41028548885020427\n",
      "training loss: 0.40353171315896363\n",
      "training loss: 0.26028485024307885\n",
      "training loss: 0.3455741115656929\n",
      "training loss: 0.2487890566383794\n",
      "training loss: 0.3138875413405276\n",
      "training loss: 0.3835474375600461\n",
      "training loss: 0.34421510938584104\n",
      "training loss: 0.293412936634013\n",
      "training loss: 0.6591801493696403\n",
      "training loss: 0.42113344387358664\n",
      "training loss: 0.32930282285373325\n",
      "training loss: 0.42671559068712667\n",
      "training loss: 0.37155042125938964\n",
      "training loss: 0.39698680227018485\n",
      "training loss: 0.28636467968274704\n",
      "training loss: 0.30894251984820587\n",
      "training loss: 0.26222896112217314\n",
      "training loss: 0.3896226472999115\n",
      "training loss: 0.25238478223996935\n",
      "training loss: 0.33233903707703577\n",
      "training loss: 0.36472342717719813\n",
      "training loss: 0.35909399542393655\n",
      "training loss: 0.30030010577913346\n",
      "training loss: 0.41214917788514865\n",
      "training loss: 0.43823399324614004\n",
      "training loss: 0.4172351783636259\n",
      "training loss: 0.2602373517718661\n",
      "training loss: 0.41586698339728173\n",
      "training loss: 0.34568116210008154\n",
      "training loss: 0.44810786122500756\n",
      "training loss: 0.2741554595262278\n",
      "training loss: 0.2608071453935918\n",
      "training loss: 0.33350685688750903\n",
      "training loss: 0.2556046821367636\n",
      "validation_loss: 0.45904934497538336\n",
      "\n",
      "Epoch 37\n",
      "------------------------------\n",
      "training loss: 0.29255784689237774\n",
      "training loss: 0.21643342329552978\n",
      "training loss: 0.34784434106200934\n",
      "training loss: 0.414206494336986\n",
      "training loss: 0.3133283267346997\n",
      "training loss: 0.2804817862832351\n",
      "training loss: 0.3894693011965524\n",
      "training loss: 0.34048026708369433\n",
      "training loss: 0.39132009857672756\n",
      "training loss: 0.3811399521275962\n",
      "training loss: 0.2985750085754262\n",
      "training loss: 0.4509065547166392\n",
      "training loss: 0.32588136570360804\n",
      "training loss: 0.4321383283067553\n",
      "training loss: 0.28924555207879166\n",
      "training loss: 0.3470300172643692\n",
      "training loss: 0.3370711668922013\n",
      "training loss: 0.3730943145311176\n",
      "training loss: 0.38484145904600153\n",
      "training loss: 0.30913217648430874\n",
      "training loss: 0.34768838041025446\n",
      "training loss: 0.4053706192802929\n",
      "training loss: 0.36091400638615595\n",
      "training loss: 0.3993108909251896\n",
      "training loss: 0.3830390388273372\n",
      "training loss: 0.3897973577812172\n",
      "training loss: 0.5125768306407941\n",
      "training loss: 0.27677782899394515\n",
      "training loss: 0.28895564531558193\n",
      "training loss: 0.28833837768784176\n",
      "training loss: 0.33718097497039706\n",
      "training loss: 0.40695694417360073\n",
      "training loss: 0.41675928228462\n",
      "training loss: 0.4697948843694212\n",
      "training loss: 0.35374314207147106\n",
      "validation_loss: 0.4584175919584205\n",
      "\n",
      "Epoch 38\n",
      "------------------------------\n",
      "training loss: 0.3258665112152653\n",
      "training loss: 0.3199854862943175\n",
      "training loss: 0.4259243911435624\n",
      "training loss: 0.32804774172139334\n",
      "training loss: 0.24106733854430787\n",
      "training loss: 0.3031159080253565\n",
      "training loss: 0.293616224276484\n",
      "training loss: 0.5028415520204725\n",
      "training loss: 0.3251193108496955\n",
      "training loss: 0.26787089597957675\n",
      "training loss: 0.342200800829869\n",
      "training loss: 0.4053927498453413\n",
      "training loss: 0.3096200486924499\n",
      "training loss: 0.37200536631804426\n",
      "training loss: 0.3791086021985393\n",
      "training loss: 0.42135371685748396\n",
      "training loss: 0.29708374722679765\n",
      "training loss: 0.3019598256680183\n",
      "training loss: 0.41496438767964716\n",
      "training loss: 0.27989714042421837\n",
      "training loss: 0.45975620163681014\n",
      "training loss: 0.2931049340336176\n",
      "training loss: 0.2723957806441672\n",
      "training loss: 0.43021811505608637\n",
      "training loss: 0.36781995436409487\n",
      "training loss: 0.3656931866305604\n",
      "training loss: 0.3142547018709683\n",
      "training loss: 0.4294561904092552\n",
      "training loss: 0.2769481246083524\n",
      "training loss: 0.4099700936660156\n",
      "training loss: 0.37687610373905045\n",
      "training loss: 0.43619737760105637\n",
      "training loss: 0.3345925210052883\n",
      "training loss: 0.4787830101821601\n",
      "training loss: 0.3345946761749656\n",
      "validation_loss: 0.4854145940986613\n",
      "\n",
      "Epoch 39\n",
      "------------------------------\n",
      "training loss: 0.43787366642134656\n",
      "training loss: 0.37259433368562894\n",
      "training loss: 0.33535515639316144\n",
      "training loss: 0.31607220568606864\n",
      "training loss: 0.38505942127776505\n",
      "training loss: 0.32520511217084275\n",
      "training loss: 0.33087150618608574\n",
      "training loss: 0.44925114763825147\n",
      "training loss: 0.27882074948662194\n",
      "training loss: 0.30592883093286216\n",
      "training loss: 0.41886896902520676\n",
      "training loss: 0.27209148191617716\n",
      "training loss: 0.3780334666451381\n",
      "training loss: 0.4934960971822147\n",
      "training loss: 0.31457981316743827\n",
      "training loss: 0.34486313588917256\n",
      "training loss: 0.42056521292819526\n",
      "training loss: 0.36961090152429277\n",
      "training loss: 0.319324470335705\n",
      "training loss: 0.2775169031188125\n",
      "training loss: 0.3785345890669123\n",
      "training loss: 0.40129760383757457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3161709268767663\n",
      "training loss: 0.3895148269200581\n",
      "training loss: 0.3904433052056993\n",
      "training loss: 0.39179649609516676\n",
      "training loss: 0.27638903807484894\n",
      "training loss: 0.4453920513664434\n",
      "training loss: 0.3925218289217446\n",
      "training loss: 0.2052938037116837\n",
      "training loss: 0.30610858915495553\n",
      "training loss: 0.3815194562198303\n",
      "training loss: 0.37429485708202265\n",
      "training loss: 0.3619449912093933\n",
      "training loss: 0.3228803752022395\n",
      "validation_loss: 0.4730789979162971\n",
      "\n",
      "Epoch 40\n",
      "------------------------------\n",
      "training loss: 0.43276156090922313\n",
      "training loss: 0.2342067300875897\n",
      "training loss: 0.33276890663226366\n",
      "training loss: 0.2928947244576557\n",
      "training loss: 0.2986522985836655\n",
      "training loss: 0.37759606210486707\n",
      "training loss: 0.5743187544346438\n",
      "training loss: 0.35046677426129463\n",
      "training loss: 0.4493846294778268\n",
      "training loss: 0.3737882403563708\n",
      "training loss: 0.2840378081936251\n",
      "training loss: 0.44965993724417785\n",
      "training loss: 0.2813974299646361\n",
      "training loss: 0.43108802173461297\n",
      "training loss: 0.37884342402387117\n",
      "training loss: 0.35331115004639285\n",
      "training loss: 0.3172566987634309\n",
      "training loss: 0.2879080450136462\n",
      "training loss: 0.302288049487106\n",
      "training loss: 0.3742705666064285\n",
      "training loss: 0.26232478474595156\n",
      "training loss: 0.38566984904478885\n",
      "training loss: 0.4292599250110652\n",
      "training loss: 0.411301659754281\n",
      "training loss: 0.3921729611293995\n",
      "training loss: 0.2454484317565948\n",
      "training loss: 0.3763519338408514\n",
      "training loss: 0.27190492912857733\n",
      "training loss: 0.2767420604592371\n",
      "training loss: 0.41350829086077284\n",
      "training loss: 0.40476124493561655\n",
      "training loss: 0.3147389953085076\n",
      "training loss: 0.3535456353284098\n",
      "training loss: 0.35574749358604324\n",
      "training loss: 0.3216004391122624\n",
      "validation_loss: 0.46300064095730104\n",
      "\n",
      "Epoch 41\n",
      "------------------------------\n",
      "training loss: 0.4376585667669133\n",
      "training loss: 0.3164484023350087\n",
      "training loss: 0.3013512371804973\n",
      "training loss: 0.49475150626261893\n",
      "training loss: 0.43732572656561614\n",
      "training loss: 0.3619409447876387\n",
      "training loss: 0.3583521023804497\n",
      "training loss: 0.31902558699910744\n",
      "training loss: 0.3241298832157554\n",
      "training loss: 0.2710738916464288\n",
      "training loss: 0.28093368436461785\n",
      "training loss: 0.2858854140350013\n",
      "training loss: 0.3406978087307653\n",
      "training loss: 0.31777510009867\n",
      "training loss: 0.39437074866511695\n",
      "training loss: 0.35255758772138507\n",
      "training loss: 0.4383440008993057\n",
      "training loss: 0.3284284647346635\n",
      "training loss: 0.3710555614173245\n",
      "training loss: 0.31968816150111706\n",
      "training loss: 0.4387335797228297\n",
      "training loss: 0.2956118243671153\n",
      "training loss: 0.3995805784672848\n",
      "training loss: 0.2726655974128516\n",
      "training loss: 0.3200658748822025\n",
      "training loss: 0.5041687296951568\n",
      "training loss: 0.3931212696022339\n",
      "training loss: 0.43619843596941793\n",
      "training loss: 0.3981793083121829\n",
      "training loss: 0.26009963066249836\n",
      "training loss: 0.3028034562058747\n",
      "training loss: 0.3147727258117811\n",
      "training loss: 0.36187321966492164\n",
      "training loss: 0.34410802078276903\n",
      "training loss: 0.41799810630676804\n",
      "validation_loss: 0.4713326711858822\n",
      "\n",
      "Epoch 42\n",
      "------------------------------\n",
      "training loss: 0.3573920042430109\n",
      "training loss: 0.2831501760588526\n",
      "training loss: 0.28019965603845776\n",
      "training loss: 0.3473478181066457\n",
      "training loss: 0.3208294555118846\n",
      "training loss: 0.2614215680882285\n",
      "training loss: 0.31456688012203815\n",
      "training loss: 0.211169801416072\n",
      "training loss: 0.33712134447437164\n",
      "training loss: 0.2769160084210307\n",
      "training loss: 0.3426128592571422\n",
      "training loss: 0.4056365225459831\n",
      "training loss: 0.3666119730252831\n",
      "training loss: 0.3740534384682542\n",
      "training loss: 0.3127181294301408\n",
      "training loss: 0.2445162157629966\n",
      "training loss: 0.2970290655927238\n",
      "training loss: 0.4077923477786976\n",
      "training loss: 0.37225005931250055\n",
      "training loss: 0.4950480309387785\n",
      "training loss: 0.3485526892315829\n",
      "training loss: 0.5558486648940744\n",
      "training loss: 0.3397956508165953\n",
      "training loss: 0.45509166615892355\n",
      "training loss: 0.27030386953499147\n",
      "training loss: 0.33372437351426926\n",
      "training loss: 0.4546554820212259\n",
      "training loss: 0.3390787730654483\n",
      "training loss: 0.41624035473694676\n",
      "training loss: 0.38815649840769995\n",
      "training loss: 0.36369862985797224\n",
      "training loss: 0.4981010268500927\n",
      "training loss: 0.3794332706434943\n",
      "training loss: 0.34819802998772503\n",
      "training loss: 0.3256327110118218\n",
      "validation_loss: 0.47815543537940497\n",
      "\n",
      "Epoch 43\n",
      "------------------------------\n",
      "training loss: 0.4181245941777979\n",
      "training loss: 0.25463466112705874\n",
      "training loss: 0.33869346923558624\n",
      "training loss: 0.37579628174114077\n",
      "training loss: 0.2929804219196376\n",
      "training loss: 0.32378795269757576\n",
      "training loss: 0.3425628334184876\n",
      "training loss: 0.366957070250628\n",
      "training loss: 0.39516979019535936\n",
      "training loss: 0.43683544942303343\n",
      "training loss: 0.307829549901362\n",
      "training loss: 0.3476230995984224\n",
      "training loss: 0.35857491626709814\n",
      "training loss: 0.35290353754520765\n",
      "training loss: 0.4227983130158918\n",
      "training loss: 0.3315848778345025\n",
      "training loss: 0.31762230919787726\n",
      "training loss: 0.4056079950576896\n",
      "training loss: 0.2670817105804963\n",
      "training loss: 0.29476035739062356\n",
      "training loss: 0.46180544686721986\n",
      "training loss: 0.348311997808114\n",
      "training loss: 0.3141965703101255\n",
      "training loss: 0.46232433923002875\n",
      "training loss: 0.27380960888345723\n",
      "training loss: 0.38010512058477613\n",
      "training loss: 0.3178761792847945\n",
      "training loss: 0.29443384063444683\n",
      "training loss: 0.4573918851585495\n",
      "training loss: 0.31171340060524244\n",
      "training loss: 0.34091439088275366\n",
      "training loss: 0.30858063430915533\n",
      "training loss: 0.35908353031696605\n",
      "training loss: 0.3360810221045176\n",
      "training loss: 0.34423742414161096\n",
      "validation_loss: 0.4857155320335444\n",
      "\n",
      "Epoch 44\n",
      "------------------------------\n",
      "training loss: 0.27564871956114984\n",
      "training loss: 0.28891996130074404\n",
      "training loss: 0.3160131767787607\n",
      "training loss: 0.4049200437725904\n",
      "training loss: 0.36351983603025473\n",
      "training loss: 0.3815968385826272\n",
      "training loss: 0.44623873768316114\n",
      "training loss: 0.3561875842797235\n",
      "training loss: 0.3266923747432884\n",
      "training loss: 0.3377442247486215\n",
      "training loss: 0.3505610562098445\n",
      "training loss: 0.2612247706062135\n",
      "training loss: 0.4705041336963859\n",
      "training loss: 0.34852274417848095\n",
      "training loss: 0.442369737448862\n",
      "training loss: 0.3212387089868025\n",
      "training loss: 0.3557083206769312\n",
      "training loss: 0.38813365545807754\n",
      "training loss: 0.3347667300158355\n",
      "training loss: 0.3355065469735564\n",
      "training loss: 0.369286717569521\n",
      "training loss: 0.400264568619541\n",
      "training loss: 0.38737339610728666\n",
      "training loss: 0.29599111216452\n",
      "training loss: 0.430310496064194\n",
      "training loss: 0.2394939283050735\n",
      "training loss: 0.24612645715315012\n",
      "training loss: 0.3610806354871602\n",
      "training loss: 0.44382993556093425\n",
      "training loss: 0.4118972544432472\n",
      "training loss: 0.2572078601564863\n",
      "training loss: 0.32418261160812106\n",
      "training loss: 0.2952112626269172\n",
      "training loss: 0.38320027058874984\n",
      "training loss: 0.4040449292724952\n",
      "validation_loss: 0.49905961828669987\n",
      "\n",
      "Epoch 45\n",
      "------------------------------\n",
      "training loss: 0.36491637976971103\n",
      "training loss: 0.30827956806323525\n",
      "training loss: 0.27736443667275124\n",
      "training loss: 0.3169534304203262\n",
      "training loss: 0.3894375436505743\n",
      "training loss: 0.33722564305047853\n",
      "training loss: 0.311314295174036\n",
      "training loss: 0.32797041221973816\n",
      "training loss: 0.2609356184507942\n",
      "training loss: 0.2794354978137653\n",
      "training loss: 0.2716016837314237\n",
      "training loss: 0.3927992179390276\n",
      "training loss: 0.312718457045421\n",
      "training loss: 0.30614470817148687\n",
      "training loss: 0.2931008606272826\n",
      "training loss: 0.3604465652075305\n",
      "training loss: 0.3657762490649475\n",
      "training loss: 0.4024996414531779\n",
      "training loss: 0.33612421393947445\n",
      "training loss: 0.37660528564971174\n",
      "training loss: 0.3851783622490257\n",
      "training loss: 0.3897014786926411\n",
      "training loss: 0.2999878463765344\n",
      "training loss: 0.3769994327719178\n",
      "training loss: 0.33779175495161323\n",
      "training loss: 0.4455336913319479\n",
      "training loss: 0.3679310544594773\n",
      "training loss: 0.37579037200892346\n",
      "training loss: 0.3203446600980533\n",
      "training loss: 0.35891305858807754\n",
      "training loss: 0.45551439716780806\n",
      "training loss: 0.39178163172234237\n",
      "training loss: 0.384182557871718\n",
      "training loss: 0.47430532723956276\n",
      "training loss: 0.4644162690924168\n",
      "validation_loss: 0.5148383635630465\n",
      "\n",
      "Epoch 46\n",
      "------------------------------\n",
      "training loss: 0.2534828673457378\n",
      "training loss: 0.33377713147201576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.20694076537276715\n",
      "training loss: 0.38642270007577506\n",
      "training loss: 0.36024579802637163\n",
      "training loss: 0.4481566843307519\n",
      "training loss: 0.24471564493796905\n",
      "training loss: 0.3577471520769268\n",
      "training loss: 0.38738041894564046\n",
      "training loss: 0.4621539479391595\n",
      "training loss: 0.3688696844534206\n",
      "training loss: 0.43941932989171617\n",
      "training loss: 0.3345190023963005\n",
      "training loss: 0.26781646930023273\n",
      "training loss: 0.4018911852307065\n",
      "training loss: 0.40656603167924915\n",
      "training loss: 0.2661396511953717\n",
      "training loss: 0.3088998721457028\n",
      "training loss: 0.3766801465014942\n",
      "training loss: 0.4191250116007086\n",
      "training loss: 0.2722266072801813\n",
      "training loss: 0.3810449852509191\n",
      "training loss: 0.35038330610368573\n",
      "training loss: 0.2905209037032273\n",
      "training loss: 0.31699000422724566\n",
      "training loss: 0.27917757862932374\n",
      "training loss: 0.4238859571596186\n",
      "training loss: 0.3154022900988366\n",
      "training loss: 0.3455595425929641\n",
      "training loss: 0.4015699556304844\n",
      "training loss: 0.4273659146379214\n",
      "training loss: 0.4651198562080026\n",
      "training loss: 0.39346258658708394\n",
      "training loss: 0.31359398916582903\n",
      "training loss: 0.3074239272905106\n",
      "validation_loss: 0.4876601095774954\n",
      "\n",
      "Epoch 47\n",
      "------------------------------\n",
      "training loss: 0.3960005416923377\n",
      "training loss: 0.27222367651120294\n",
      "training loss: 0.29875962351801716\n",
      "training loss: 0.3626440853047825\n",
      "training loss: 0.29739364605033186\n",
      "training loss: 0.3105069411933073\n",
      "training loss: 0.3555616517039016\n",
      "training loss: 0.351077507107193\n",
      "training loss: 0.42705477189301744\n",
      "training loss: 0.31431331430096177\n",
      "training loss: 0.35949457296687115\n",
      "training loss: 0.35594614310815814\n",
      "training loss: 0.3767970511704334\n",
      "training loss: 0.40851335712346554\n",
      "training loss: 0.398116367356306\n",
      "training loss: 0.33876288604224103\n",
      "training loss: 0.33649023245088755\n",
      "training loss: 0.45054748564027247\n",
      "training loss: 0.25157669962001816\n",
      "training loss: 0.341826924749912\n",
      "training loss: 0.3333324228800484\n",
      "training loss: 0.29209184839879754\n",
      "training loss: 0.49186026658455373\n",
      "training loss: 0.3711687478829845\n",
      "training loss: 0.28106949033503953\n",
      "training loss: 0.3169095282326475\n",
      "training loss: 0.41754726763465444\n",
      "training loss: 0.40969191741151006\n",
      "training loss: 0.27108264166007073\n",
      "training loss: 0.44232456489655303\n",
      "training loss: 0.4422002096727374\n",
      "training loss: 0.2606921526566748\n",
      "training loss: 0.31613681864841053\n",
      "training loss: 0.3705955748703127\n",
      "training loss: 0.34719609021543874\n",
      "validation_loss: 0.47847482573726147\n",
      "\n",
      "Epoch 48\n",
      "------------------------------\n",
      "training loss: 0.24894815334308076\n",
      "training loss: 0.3842450798268328\n",
      "training loss: 0.34681544934472186\n",
      "training loss: 0.3591984800065984\n",
      "training loss: 0.3210972064705493\n",
      "training loss: 0.4102479843952642\n",
      "training loss: 0.3541525006524171\n",
      "training loss: 0.3385813012656399\n",
      "training loss: 0.3728959242465885\n",
      "training loss: 0.36598011041874995\n",
      "training loss: 0.22587386619681638\n",
      "training loss: 0.23897116186034736\n",
      "training loss: 0.38553915366139335\n",
      "training loss: 0.4159493394638048\n",
      "training loss: 0.32346700276071716\n",
      "training loss: 0.3183407866665948\n",
      "training loss: 0.3413015571279175\n",
      "training loss: 0.324888324047497\n",
      "training loss: 0.3856195485464741\n",
      "training loss: 0.36974867553510193\n",
      "training loss: 0.3540532791518672\n",
      "training loss: 0.32054619816894275\n",
      "training loss: 0.5731818397571624\n",
      "training loss: 0.30966136927014304\n",
      "training loss: 0.34772086832297644\n",
      "training loss: 0.3724399992254985\n",
      "training loss: 0.30055944027095394\n",
      "training loss: 0.3024732791983479\n",
      "training loss: 0.373690077153733\n",
      "training loss: 0.3537087239586981\n",
      "training loss: 0.2807368537444654\n",
      "training loss: 0.35952265166190045\n",
      "training loss: 0.39442725806393353\n",
      "training loss: 0.4045577696984401\n",
      "training loss: 0.41059158806358936\n",
      "validation_loss: 0.48399432800952547\n",
      "\n",
      "Epoch 49\n",
      "------------------------------\n",
      "training loss: 0.32565539420378625\n",
      "training loss: 0.3901393049531998\n",
      "training loss: 0.4139465187135011\n",
      "training loss: 0.30047250942956455\n",
      "training loss: 0.29080301022171623\n",
      "training loss: 0.40936878251501185\n",
      "training loss: 0.36361444492111333\n",
      "training loss: 0.41755412550894105\n",
      "training loss: 0.28126849276284926\n",
      "training loss: 0.36970137956377586\n",
      "training loss: 0.32224491530665544\n",
      "training loss: 0.4197151087509337\n",
      "training loss: 0.3195098038739161\n",
      "training loss: 0.3630313882053724\n",
      "training loss: 0.3436341264857947\n",
      "training loss: 0.3358185994035739\n",
      "training loss: 0.22565256944533757\n",
      "training loss: 0.35387687154114245\n",
      "training loss: 0.3207967256086704\n",
      "training loss: 0.2892675233486807\n",
      "training loss: 0.4684843836144864\n",
      "training loss: 0.37355745467328233\n",
      "training loss: 0.4174971093619479\n",
      "training loss: 0.4597060773006524\n",
      "training loss: 0.350524728630553\n",
      "training loss: 0.39511961987889666\n",
      "training loss: 0.3921682584380324\n",
      "training loss: 0.288948395823536\n",
      "training loss: 0.43480978560977745\n",
      "training loss: 0.3649657182239753\n",
      "training loss: 0.2872837448888822\n",
      "training loss: 0.27071634646687925\n",
      "training loss: 0.3860322400880978\n",
      "training loss: 0.3658837534861232\n",
      "training loss: 0.3703909028216731\n",
      "validation_loss: 0.47535256145558413\n",
      "\n",
      "Epoch 50\n",
      "------------------------------\n",
      "training loss: 0.3519733741332311\n",
      "training loss: 0.3549949502834352\n",
      "training loss: 0.3456890581025073\n",
      "training loss: 0.33788450095817096\n",
      "training loss: 0.34601827092090387\n",
      "training loss: 0.3522226424886617\n",
      "training loss: 0.2799472376670656\n",
      "training loss: 0.40845451700733976\n",
      "training loss: 0.380295945685657\n",
      "training loss: 0.3240715965681011\n",
      "training loss: 0.2440877383860061\n",
      "training loss: 0.366860834097788\n",
      "training loss: 0.316573234917887\n",
      "training loss: 0.2971577065611291\n",
      "training loss: 0.4023945462321171\n",
      "training loss: 0.4255353734624805\n",
      "training loss: 0.3872596576641445\n",
      "training loss: 0.35575885783942796\n",
      "training loss: 0.259720684444801\n",
      "training loss: 0.3631587606329413\n",
      "training loss: 0.3937113153231621\n",
      "training loss: 0.32283247338288673\n",
      "training loss: 0.33602608364701153\n",
      "training loss: 0.32460132385022006\n",
      "training loss: 0.3269816257486309\n",
      "training loss: 0.3101444731242736\n",
      "training loss: 0.5883919178100768\n",
      "training loss: 0.4278779302613111\n",
      "training loss: 0.397786588468025\n",
      "training loss: 0.28380726212650187\n",
      "training loss: 0.4252259361797769\n",
      "training loss: 0.25837030598745514\n",
      "training loss: 0.32799841925225337\n",
      "training loss: 0.3500557413778733\n",
      "training loss: 0.4601597187758148\n",
      "validation_loss: 0.45787850059339374\n",
      "\n",
      "Epoch 51\n",
      "------------------------------\n",
      "training loss: 0.3652813809044551\n",
      "training loss: 0.29908621541705543\n",
      "training loss: 0.3411355144072513\n",
      "training loss: 0.27701811104187074\n",
      "training loss: 0.4103749377444228\n",
      "training loss: 0.42651645883423955\n",
      "training loss: 0.37557617109827335\n",
      "training loss: 0.3344179225379139\n",
      "training loss: 0.32174082538371296\n",
      "training loss: 0.25968805970449466\n",
      "training loss: 0.3363430848342705\n",
      "training loss: 0.3730023396539036\n",
      "training loss: 0.282936912997975\n",
      "training loss: 0.35299255137113505\n",
      "training loss: 0.32026494731155253\n",
      "training loss: 0.42330179028424025\n",
      "training loss: 0.5375810789881144\n",
      "training loss: 0.3423836946983488\n",
      "training loss: 0.37527345604394213\n",
      "training loss: 0.5454589258902707\n",
      "training loss: 0.36553374729626964\n",
      "training loss: 0.4029843207362137\n",
      "training loss: 0.42155020406383303\n",
      "training loss: 0.24177891014826172\n",
      "training loss: 0.2729992304680582\n",
      "training loss: 0.36569087284901797\n",
      "training loss: 0.28130606227095994\n",
      "training loss: 0.33442484189039534\n",
      "training loss: 0.2873708176352375\n",
      "training loss: 0.438392917944584\n",
      "training loss: 0.2819731599462466\n",
      "training loss: 0.312664157503059\n",
      "training loss: 0.3494391478738544\n",
      "training loss: 0.3759803171583917\n",
      "training loss: 0.3977096974563756\n",
      "validation_loss: 0.43766918442898545\n",
      "\n",
      "Epoch 52\n",
      "------------------------------\n",
      "training loss: 0.23394914893691748\n",
      "training loss: 0.23313200980846888\n",
      "training loss: 0.4047931075177621\n",
      "training loss: 0.37265996188405554\n",
      "training loss: 0.30844529376540775\n",
      "training loss: 0.33189562231098535\n",
      "training loss: 0.3187966685422725\n",
      "training loss: 0.33406189227796856\n",
      "training loss: 0.36388678410598\n",
      "training loss: 0.34357875397796306\n",
      "training loss: 0.2988433543348219\n",
      "training loss: 0.2809897625652957\n",
      "training loss: 0.41393965669467436\n",
      "training loss: 0.3211892919280217\n",
      "training loss: 0.32524621703287265\n",
      "training loss: 0.3724995855275574\n",
      "training loss: 0.28371208256012326\n",
      "training loss: 0.3783807216173227\n",
      "training loss: 0.4284521825943375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.34170370312338494\n",
      "training loss: 0.39820757723400674\n",
      "training loss: 0.37675622014859983\n",
      "training loss: 0.2249593820120208\n",
      "training loss: 0.3764646108058514\n",
      "training loss: 0.3917527156752476\n",
      "training loss: 0.293582297417679\n",
      "training loss: 0.3337422248047369\n",
      "training loss: 0.43833702311618256\n",
      "training loss: 0.4515244097489631\n",
      "training loss: 0.49017836325587266\n",
      "training loss: 0.40586748623056335\n",
      "training loss: 0.37408475265569907\n",
      "training loss: 0.2852618713825564\n",
      "training loss: 0.3339418622199446\n",
      "training loss: 0.41416350422652615\n",
      "validation_loss: 0.4715964033454069\n",
      "\n",
      "Epoch 53\n",
      "------------------------------\n",
      "training loss: 0.40320860953375814\n",
      "training loss: 0.37101238329116315\n",
      "training loss: 0.34283509338463775\n",
      "training loss: 0.3597080974170444\n",
      "training loss: 0.4283479327815644\n",
      "training loss: 0.2838572832707541\n",
      "training loss: 0.28565444166484666\n",
      "training loss: 0.33165472235023347\n",
      "training loss: 0.3122913613678975\n",
      "training loss: 0.3853840918938863\n",
      "training loss: 0.3039277306736312\n",
      "training loss: 0.29063448582943235\n",
      "training loss: 0.32823531946470214\n",
      "training loss: 0.35928801583489983\n",
      "training loss: 0.3958689646255516\n",
      "training loss: 0.30516917790479053\n",
      "training loss: 0.38237124976392806\n",
      "training loss: 0.29860766096259794\n",
      "training loss: 0.3066660796767974\n",
      "training loss: 0.28433299119083133\n",
      "training loss: 0.431219874282333\n",
      "training loss: 0.27906436900491827\n",
      "training loss: 0.35040676286459527\n",
      "training loss: 0.4749665684066349\n",
      "training loss: 0.3397675284446814\n",
      "training loss: 0.32934411513924716\n",
      "training loss: 0.3226079116889741\n",
      "training loss: 0.4152230847517785\n",
      "training loss: 0.3772361964022639\n",
      "training loss: 0.31470580018070904\n",
      "training loss: 0.4711397441722511\n",
      "training loss: 0.25295880601630416\n",
      "training loss: 0.39246817904197084\n",
      "training loss: 0.489009437248933\n",
      "training loss: 0.4207989063544687\n",
      "validation_loss: 0.4490648869844484\n",
      "\n",
      "Epoch 54\n",
      "------------------------------\n",
      "training loss: 0.22635816771136888\n",
      "training loss: 0.2700278229855394\n",
      "training loss: 0.30875990108559564\n",
      "training loss: 0.35371621580503415\n",
      "training loss: 0.3076014740576284\n",
      "training loss: 0.33776971194427463\n",
      "training loss: 0.4369680058171798\n",
      "training loss: 0.3984970952529693\n",
      "training loss: 0.3674614134748117\n",
      "training loss: 0.2882984389916237\n",
      "training loss: 0.33471188169456584\n",
      "training loss: 0.35751511211306936\n",
      "training loss: 0.38022733328867614\n",
      "training loss: 0.30621287065288927\n",
      "training loss: 0.24195014281591284\n",
      "training loss: 0.5058981612597563\n",
      "training loss: 0.3909257943814737\n",
      "training loss: 0.31860748137594785\n",
      "training loss: 0.37201319527244775\n",
      "training loss: 0.38868199410979287\n",
      "training loss: 0.21655259784922237\n",
      "training loss: 0.3489873960425416\n",
      "training loss: 0.3861660450228737\n",
      "training loss: 0.3389808429919867\n",
      "training loss: 0.36657538188577976\n",
      "training loss: 0.3686926810372097\n",
      "training loss: 0.4163145215145778\n",
      "training loss: 0.35207372474288606\n",
      "training loss: 0.39440788810647975\n",
      "training loss: 0.4547070364629508\n",
      "training loss: 0.34675329608595346\n",
      "training loss: 0.45705200104275717\n",
      "training loss: 0.27772805411968876\n",
      "training loss: 0.3362955711926043\n",
      "training loss: 0.4053730265135914\n",
      "validation_loss: 0.5038032180540615\n",
      "\n",
      "Epoch 55\n",
      "------------------------------\n",
      "training loss: 0.27494046508823883\n",
      "training loss: 0.37819581741583536\n",
      "training loss: 0.2998751636000816\n",
      "training loss: 0.38819862826056123\n",
      "training loss: 0.37488336847003667\n",
      "training loss: 0.33313962551910664\n",
      "training loss: 0.24452171105498566\n",
      "training loss: 0.4654759141068098\n",
      "training loss: 0.22159083192917933\n",
      "training loss: 0.30963706405345875\n",
      "training loss: 0.32040044312169813\n",
      "training loss: 0.4806839556018895\n",
      "training loss: 0.3791680051062031\n",
      "training loss: 0.3388174187806544\n",
      "training loss: 0.4514588299790921\n",
      "training loss: 0.3198363638482988\n",
      "training loss: 0.29403935705195183\n",
      "training loss: 0.422499703472713\n",
      "training loss: 0.3317145472318316\n",
      "training loss: 0.40586966110787215\n",
      "training loss: 0.34195483293813594\n",
      "training loss: 0.35233985194030537\n",
      "training loss: 0.28974096918798753\n",
      "training loss: 0.3444667918822597\n",
      "training loss: 0.3927136676720147\n",
      "training loss: 0.31922694983581096\n",
      "training loss: 0.34548281875118847\n",
      "training loss: 0.28947675955151\n",
      "training loss: 0.3592694521102385\n",
      "training loss: 0.30943272639076896\n",
      "training loss: 0.29912062072425216\n",
      "training loss: 0.33324874088991235\n",
      "training loss: 0.5359184937570172\n",
      "training loss: 0.41667526174456726\n",
      "training loss: 0.3111849686026471\n",
      "validation_loss: 0.47334517504286944\n",
      "\n",
      "Epoch 56\n",
      "------------------------------\n",
      "training loss: 0.29885977503174216\n",
      "training loss: 0.3235778438214038\n",
      "training loss: 0.2838052538402917\n",
      "training loss: 0.3203249833492464\n",
      "training loss: 0.3600374113526436\n",
      "training loss: 0.3770960512649617\n",
      "training loss: 0.2648961659165798\n",
      "training loss: 0.46338028489324645\n",
      "training loss: 0.4503491386517999\n",
      "training loss: 0.274126402296497\n",
      "training loss: 0.22476992534779128\n",
      "training loss: 0.34284018427482804\n",
      "training loss: 0.3308680364189877\n",
      "training loss: 0.2666238805418925\n",
      "training loss: 0.36058454577557314\n",
      "training loss: 0.3513047593371084\n",
      "training loss: 0.5773085566564987\n",
      "training loss: 0.44897042972326745\n",
      "training loss: 0.31582909993972863\n",
      "training loss: 0.28713482476084207\n",
      "training loss: 0.42183542385355394\n",
      "training loss: 0.3761211242932404\n",
      "training loss: 0.3219067238950629\n",
      "training loss: 0.2676285777764861\n",
      "training loss: 0.33978140609942786\n",
      "training loss: 0.4603076887808857\n",
      "training loss: 0.46907091322347694\n",
      "training loss: 0.35153585196796483\n",
      "training loss: 0.38367927299726035\n",
      "training loss: 0.4152659691525332\n",
      "training loss: 0.426951214714536\n",
      "training loss: 0.3143956828263617\n",
      "training loss: 0.37624317971407434\n",
      "training loss: 0.306876909317225\n",
      "training loss: 0.31168782960114183\n",
      "validation_loss: 0.48114778149457094\n",
      "\n",
      "Epoch 57\n",
      "------------------------------\n",
      "training loss: 0.2822979848507384\n",
      "training loss: 0.40050151461866335\n",
      "training loss: 0.35670612629946846\n",
      "training loss: 0.39440628498668956\n",
      "training loss: 0.34348218458355406\n",
      "training loss: 0.28803503995528446\n",
      "training loss: 0.3737960583830136\n",
      "training loss: 0.3407845924654975\n",
      "training loss: 0.28660837347238155\n",
      "training loss: 0.37378617118112745\n",
      "training loss: 0.4211959277723508\n",
      "training loss: 0.35342046113521064\n",
      "training loss: 0.3283848170382043\n",
      "training loss: 0.4488052493696159\n",
      "training loss: 0.36501416265396985\n",
      "training loss: 0.326199543091534\n",
      "training loss: 0.33822148981643296\n",
      "training loss: 0.3851878035880691\n",
      "training loss: 0.3818105262338213\n",
      "training loss: 0.5790415816276799\n",
      "training loss: 0.28186299423803574\n",
      "training loss: 0.33546663798289955\n",
      "training loss: 0.36427509551576803\n",
      "training loss: 0.3709842270489389\n",
      "training loss: 0.32851308295940723\n",
      "training loss: 0.27412996411760104\n",
      "training loss: 0.3001665148855864\n",
      "training loss: 0.3555312755411819\n",
      "training loss: 0.2488468018477215\n",
      "training loss: 0.36737694253562947\n",
      "training loss: 0.39566594300908037\n",
      "training loss: 0.4522657476420864\n",
      "training loss: 0.3463469501109307\n",
      "training loss: 0.3725616779830307\n",
      "training loss: 0.2819554663072722\n",
      "validation_loss: 0.47404433714053323\n",
      "\n",
      "Epoch 58\n",
      "------------------------------\n",
      "training loss: 0.3209697606748182\n",
      "training loss: 0.35533691154540065\n",
      "training loss: 0.3733939460456531\n",
      "training loss: 0.30129526029661063\n",
      "training loss: 0.4313065382577133\n",
      "training loss: 0.3672430811358208\n",
      "training loss: 0.3857340659876354\n",
      "training loss: 0.44790303390866937\n",
      "training loss: 0.2880720055528218\n",
      "training loss: 0.3694673089684511\n",
      "training loss: 0.36647105894822746\n",
      "training loss: 0.3132925023354346\n",
      "training loss: 0.34318986557278547\n",
      "training loss: 0.24908997900552093\n",
      "training loss: 0.2739006366167996\n",
      "training loss: 0.22732937649258247\n",
      "training loss: 0.5164584580660448\n",
      "training loss: 0.39378995826504254\n",
      "training loss: 0.37576306801442116\n",
      "training loss: 0.3371863281846163\n",
      "training loss: 0.4321088458680606\n",
      "training loss: 0.3966502010802651\n",
      "training loss: 0.32338961443601877\n",
      "training loss: 0.38930202654053575\n",
      "training loss: 0.42508875836065274\n",
      "training loss: 0.3768966478943184\n",
      "training loss: 0.3751993547385791\n",
      "training loss: 0.37361010402439204\n",
      "training loss: 0.36323340168804863\n",
      "training loss: 0.31864468684096436\n",
      "training loss: 0.3258351254332979\n",
      "training loss: 0.40161832709098233\n",
      "training loss: 0.29044161076549246\n",
      "training loss: 0.35470580382505434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.29982960409659426\n",
      "validation_loss: 0.5091699443814554\n",
      "\n",
      "Epoch 59\n",
      "------------------------------\n",
      "training loss: 0.3245851691135067\n",
      "training loss: 0.3531976547000522\n",
      "training loss: 0.331503859691511\n",
      "training loss: 0.2774548890337246\n",
      "training loss: 0.33310071856307333\n",
      "training loss: 0.3236534757796653\n",
      "training loss: 0.32052322001662104\n",
      "training loss: 0.3113433923077264\n",
      "training loss: 0.32481780922240433\n",
      "training loss: 0.31129949368332743\n",
      "training loss: 0.3811192082571506\n",
      "training loss: 0.42921413082753135\n",
      "training loss: 0.35259379510663164\n",
      "training loss: 0.4018285898967679\n",
      "training loss: 0.4560484518468729\n",
      "training loss: 0.2710756092214433\n",
      "training loss: 0.3848918769042575\n",
      "training loss: 0.33751312151405727\n",
      "training loss: 0.26560520277445904\n",
      "training loss: 0.40741782791214065\n",
      "training loss: 0.35830661158097654\n",
      "training loss: 0.39499574129775283\n",
      "training loss: 0.39185035580674593\n",
      "training loss: 0.38485373875373624\n",
      "training loss: 0.37161346856950334\n",
      "training loss: 0.2541440410776795\n",
      "training loss: 0.4499956968690094\n",
      "training loss: 0.4083560797199607\n",
      "training loss: 0.28733356576295593\n",
      "training loss: 0.4380970048805466\n",
      "training loss: 0.28726368762290805\n",
      "training loss: 0.32260172118767516\n",
      "training loss: 0.38348811210752504\n",
      "training loss: 0.41222701262764533\n",
      "training loss: 0.3339337752048959\n",
      "validation_loss: 0.4863499261593962\n",
      "\n",
      "Epoch 60\n",
      "------------------------------\n",
      "training loss: 0.3725600936528122\n",
      "training loss: 0.42696581324431465\n",
      "training loss: 0.2508644286975323\n",
      "training loss: 0.27896739808071286\n",
      "training loss: 0.312845607152849\n",
      "training loss: 0.21467532235255932\n",
      "training loss: 0.43355346223339436\n",
      "training loss: 0.4004049203332397\n",
      "training loss: 0.4140729144638317\n",
      "training loss: 0.3725829746992167\n",
      "training loss: 0.3326992784901813\n",
      "training loss: 0.37336356969506596\n",
      "training loss: 0.32167336327723206\n",
      "training loss: 0.3537416187170311\n",
      "training loss: 0.36626319225157206\n",
      "training loss: 0.3074196960751215\n",
      "training loss: 0.47319784176929713\n",
      "training loss: 0.31285427193700344\n",
      "training loss: 0.48337659629717566\n",
      "training loss: 0.30541063801141716\n",
      "training loss: 0.2973403978953138\n",
      "training loss: 0.25345260795613284\n",
      "training loss: 0.3103159630131995\n",
      "training loss: 0.2856062615237897\n",
      "training loss: 0.4866784668393666\n",
      "training loss: 0.3448813078490275\n",
      "training loss: 0.3899470518296039\n",
      "training loss: 0.3573074251293383\n",
      "training loss: 0.41667886357883616\n",
      "training loss: 0.33433385714015457\n",
      "training loss: 0.3780666185023438\n",
      "training loss: 0.39846786461443345\n",
      "training loss: 0.3218476173303588\n",
      "training loss: 0.4572559025232658\n",
      "training loss: 0.33728942470421314\n",
      "validation_loss: 0.45750998016911365\n",
      "\n",
      "Epoch 61\n",
      "------------------------------\n",
      "training loss: 0.24804036405796068\n",
      "training loss: 0.3296852952234099\n",
      "training loss: 0.46269612109259467\n",
      "training loss: 0.3344818035949811\n",
      "training loss: 0.29108288025615364\n",
      "training loss: 0.36795939999399707\n",
      "training loss: 0.35747814909500447\n",
      "training loss: 0.27023248504541697\n",
      "training loss: 0.3325153256894555\n",
      "training loss: 0.3137448995759769\n",
      "training loss: 0.3522323991266603\n",
      "training loss: 0.27654050289937004\n",
      "training loss: 0.29245525601611005\n",
      "training loss: 0.44954584467544917\n",
      "training loss: 0.4266469530639006\n",
      "training loss: 0.42484066321663705\n",
      "training loss: 0.33564868078945437\n",
      "training loss: 0.38963224172424815\n",
      "training loss: 0.34179727255832404\n",
      "training loss: 0.3413979100810775\n",
      "training loss: 0.3513583511003526\n",
      "training loss: 0.3381028850020084\n",
      "training loss: 0.3999817282310687\n",
      "training loss: 0.2738092415181381\n",
      "training loss: 0.32320223546368654\n",
      "training loss: 0.43639303044477856\n",
      "training loss: 0.3013797429037822\n",
      "training loss: 0.42157499431068574\n",
      "training loss: 0.27417763711757287\n",
      "training loss: 0.2883309792731643\n",
      "training loss: 0.37393026664038187\n",
      "training loss: 0.3495593031364319\n",
      "training loss: 0.3487717990577221\n",
      "training loss: 0.4045156931645033\n",
      "training loss: 0.48039221103390445\n",
      "validation_loss: 0.5043520341691317\n",
      "\n",
      "Epoch 62\n",
      "------------------------------\n",
      "training loss: 0.3705930226741475\n",
      "training loss: 0.3830603150185198\n",
      "training loss: 0.35077851884547273\n",
      "training loss: 0.3447682497286223\n",
      "training loss: 0.415966851412777\n",
      "training loss: 0.3610091964234766\n",
      "training loss: 0.29087073388051066\n",
      "training loss: 0.4085584691538702\n",
      "training loss: 0.34923839758863323\n",
      "training loss: 0.3517236616843729\n",
      "training loss: 0.34673725603839556\n",
      "training loss: 0.3992838415267033\n",
      "training loss: 0.3473903171279471\n",
      "training loss: 0.37939753886208794\n",
      "training loss: 0.3488165202521486\n",
      "training loss: 0.42773926104375276\n",
      "training loss: 0.4691905944655082\n",
      "training loss: 0.4234221692992378\n",
      "training loss: 0.3248108889683499\n",
      "training loss: 0.28934068161790494\n",
      "training loss: 0.4355449164495803\n",
      "training loss: 0.31090149580555587\n",
      "training loss: 0.40210883257052044\n",
      "training loss: 0.29321174761687874\n",
      "training loss: 0.33869784747079396\n",
      "training loss: 0.40517926254775377\n",
      "training loss: 0.31389611853461247\n",
      "training loss: 0.3146853144443958\n",
      "training loss: 0.23978296707035043\n",
      "training loss: 0.30349520654469414\n",
      "training loss: 0.26787885228040975\n",
      "training loss: 0.30499147886214817\n",
      "training loss: 0.2994468949720067\n",
      "training loss: 0.255930171693326\n",
      "training loss: 0.4580184961804207\n",
      "validation_loss: 0.4668668708546318\n",
      "\n",
      "Epoch 63\n",
      "------------------------------\n",
      "training loss: 0.2943975912915721\n",
      "training loss: 0.3023497258673888\n",
      "training loss: 0.37819023812608066\n",
      "training loss: 0.2916477439663504\n",
      "training loss: 0.3366577406378747\n",
      "training loss: 0.3764403126816978\n",
      "training loss: 0.30693266167665567\n",
      "training loss: 0.3541195386107779\n",
      "training loss: 0.39509775373666345\n",
      "training loss: 0.29634409699603564\n",
      "training loss: 0.43260539944858467\n",
      "training loss: 0.33534334372456215\n",
      "training loss: 0.3979779621033231\n",
      "training loss: 0.4388896959973863\n",
      "training loss: 0.24986407610934294\n",
      "training loss: 0.3739011358562493\n",
      "training loss: 0.3166739046838484\n",
      "training loss: 0.46240274215619137\n",
      "training loss: 0.321514885718434\n",
      "training loss: 0.2726649255752636\n",
      "training loss: 0.3055000617255428\n",
      "training loss: 0.40708133417405634\n",
      "training loss: 0.40675077562453227\n",
      "training loss: 0.38123745750345733\n",
      "training loss: 0.32538595783902563\n",
      "training loss: 0.33971661365954786\n",
      "training loss: 0.27165193224326684\n",
      "training loss: 0.45248866133581034\n",
      "training loss: 0.38009724754589114\n",
      "training loss: 0.38152687332029017\n",
      "training loss: 0.4686460086418083\n",
      "training loss: 0.3754083521201574\n",
      "training loss: 0.39259395142726133\n",
      "training loss: 0.47499526076600884\n",
      "training loss: 0.33433754748359207\n",
      "validation_loss: 0.4674320696180196\n",
      "\n",
      "Epoch 64\n",
      "------------------------------\n",
      "training loss: 0.46037518905589425\n",
      "training loss: 0.2823099582241821\n",
      "training loss: 0.2739784896906713\n",
      "training loss: 0.307709309773054\n",
      "training loss: 0.3207670310133926\n",
      "training loss: 0.2933462301063446\n",
      "training loss: 0.3132122286314552\n",
      "training loss: 0.2785412974829887\n",
      "training loss: 0.30994364268220126\n",
      "training loss: 0.2974632763397676\n",
      "training loss: 0.3321933060332776\n",
      "training loss: 0.3073667494448455\n",
      "training loss: 0.3190227961161145\n",
      "training loss: 0.31755082272167784\n",
      "training loss: 0.2775878738684696\n",
      "training loss: 0.4347225331070513\n",
      "training loss: 0.3567087540991929\n",
      "training loss: 0.3768005938895294\n",
      "training loss: 0.42183419678003703\n",
      "training loss: 0.3734696996490311\n",
      "training loss: 0.3751447912247386\n",
      "training loss: 0.3093131541801267\n",
      "training loss: 0.45814837540601727\n",
      "training loss: 0.46436704684711005\n",
      "training loss: 0.3417849416268291\n",
      "training loss: 0.41870159792830236\n",
      "training loss: 0.39552779434736296\n",
      "training loss: 0.3507480568930623\n",
      "training loss: 0.3364682496165915\n",
      "training loss: 0.48482052851904883\n",
      "training loss: 0.4151788745931117\n",
      "training loss: 0.34090912865303835\n",
      "training loss: 0.370764736844867\n",
      "training loss: 0.4706286933359115\n",
      "training loss: 0.3304815683428751\n",
      "validation_loss: 0.4693265053971809\n",
      "\n",
      "Epoch 65\n",
      "------------------------------\n",
      "training loss: 0.3893702484712412\n",
      "training loss: 0.40616524547622246\n",
      "training loss: 0.3682196082478276\n",
      "training loss: 0.32725566811408496\n",
      "training loss: 0.3054750712283567\n",
      "training loss: 0.3553588192702227\n",
      "training loss: 0.4194557025784161\n",
      "training loss: 0.41045259023951985\n",
      "training loss: 0.3144114175248251\n",
      "training loss: 0.2701336229764479\n",
      "training loss: 0.3003727989763138\n",
      "training loss: 0.5251693874257216\n",
      "training loss: 0.41383738842277124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3957391994420323\n",
      "training loss: 0.3839134753903181\n",
      "training loss: 0.2929612053320307\n",
      "training loss: 0.29819605478900485\n",
      "training loss: 0.2617085124977166\n",
      "training loss: 0.3671551484128577\n",
      "training loss: 0.3796636117102025\n",
      "training loss: 0.4041732316331036\n",
      "training loss: 0.31357789338289876\n",
      "training loss: 0.34633823712327283\n",
      "training loss: 0.25046673344299963\n",
      "training loss: 0.32633972161784186\n",
      "training loss: 0.3452682755654678\n",
      "training loss: 0.3876707779714343\n",
      "training loss: 0.3181942880401402\n",
      "training loss: 0.3245947307907045\n",
      "training loss: 0.2998678021361411\n",
      "training loss: 0.3290510011609877\n",
      "training loss: 0.3698225365149847\n",
      "training loss: 0.36708492521611336\n",
      "training loss: 0.34075393974329926\n",
      "training loss: 0.4238271194572735\n",
      "validation_loss: 0.48724372252424974\n",
      "\n",
      "Epoch 66\n",
      "------------------------------\n",
      "training loss: 0.5034352531482\n",
      "training loss: 0.3789264238200872\n",
      "training loss: 0.27344892771739976\n",
      "training loss: 0.36314226834427243\n",
      "training loss: 0.33064796725200724\n",
      "training loss: 0.3292740786804643\n",
      "training loss: 0.3142320278022453\n",
      "training loss: 0.3687582730737267\n",
      "training loss: 0.44879550603940516\n",
      "training loss: 0.24548414548742584\n",
      "training loss: 0.3615812343501625\n",
      "training loss: 0.39646380691781813\n",
      "training loss: 0.3744631541959825\n",
      "training loss: 0.30789633853084525\n",
      "training loss: 0.42733413180187196\n",
      "training loss: 0.32618222785866235\n",
      "training loss: 0.3008688129861548\n",
      "training loss: 0.38082749136494387\n",
      "training loss: 0.41141351444268365\n",
      "training loss: 0.3410076608673262\n",
      "training loss: 0.32577307480307355\n",
      "training loss: 0.27397839926163214\n",
      "training loss: 0.3934992239893472\n",
      "training loss: 0.41890759219058965\n",
      "training loss: 0.3858383511280408\n",
      "training loss: 0.2800850971230193\n",
      "training loss: 0.3395595345650145\n",
      "training loss: 0.43244012723080233\n",
      "training loss: 0.3846232814941413\n",
      "training loss: 0.3137561757602452\n",
      "training loss: 0.28956204930276724\n",
      "training loss: 0.3645008030430472\n",
      "training loss: 0.28392751948267686\n",
      "training loss: 0.3666012709751908\n",
      "training loss: 0.38293962347205707\n",
      "validation_loss: 0.4577311350318989\n",
      "\n",
      "Epoch 67\n",
      "------------------------------\n",
      "training loss: 0.327966147970219\n",
      "training loss: 0.40738811463525054\n",
      "training loss: 0.41068866899557177\n",
      "training loss: 0.275206969630899\n",
      "training loss: 0.5789194821231649\n",
      "training loss: 0.345636545043526\n",
      "training loss: 0.36461690587363593\n",
      "training loss: 0.3978167155693518\n",
      "training loss: 0.36156097791157665\n",
      "training loss: 0.2840219799105489\n",
      "training loss: 0.4125330282410687\n",
      "training loss: 0.3373677898367532\n",
      "training loss: 0.4428360821913611\n",
      "training loss: 0.4081694213403898\n",
      "training loss: 0.3738668682493153\n",
      "training loss: 0.2438753203720262\n",
      "training loss: 0.2802574361984307\n",
      "training loss: 0.3463275728317967\n",
      "training loss: 0.2859035290093743\n",
      "training loss: 0.30541502340318405\n",
      "training loss: 0.3825995974030957\n",
      "training loss: 0.2809235879510129\n",
      "training loss: 0.35106454180429864\n",
      "training loss: 0.30070182530678724\n",
      "training loss: 0.41545884315643206\n",
      "training loss: 0.4084910639203008\n",
      "training loss: 0.3013270808325615\n",
      "training loss: 0.4889942641176458\n",
      "training loss: 0.39323667227447\n",
      "training loss: 0.42106419049618127\n",
      "training loss: 0.2881736269130852\n",
      "training loss: 0.27863665544133254\n",
      "training loss: 0.28854597718390324\n",
      "training loss: 0.2619582058089145\n",
      "training loss: 0.35902156902186105\n",
      "validation_loss: 0.44519693896358825\n",
      "\n",
      "Epoch 68\n",
      "------------------------------\n",
      "training loss: 0.3085374303078424\n",
      "training loss: 0.4931670466866308\n",
      "training loss: 0.24528042614829246\n",
      "training loss: 0.2760327394982596\n",
      "training loss: 0.32960272444674954\n",
      "training loss: 0.3654341415635281\n",
      "training loss: 0.36637332510726994\n",
      "training loss: 0.3018211042625626\n",
      "training loss: 0.43326269880351903\n",
      "training loss: 0.32369521497055986\n",
      "training loss: 0.3570368476184376\n",
      "training loss: 0.27555325333203656\n",
      "training loss: 0.5298140526371662\n",
      "training loss: 0.3305967289891851\n",
      "training loss: 0.36547500591550486\n",
      "training loss: 0.41015318383515703\n",
      "training loss: 0.35045566662383865\n",
      "training loss: 0.31634103145916015\n",
      "training loss: 0.31360825786007124\n",
      "training loss: 0.29842908019956665\n",
      "training loss: 0.29429081291027614\n",
      "training loss: 0.4384851828196702\n",
      "training loss: 0.3207493862841511\n",
      "training loss: 0.3988521757640774\n",
      "training loss: 0.47536110726941844\n",
      "training loss: 0.5144254583786824\n",
      "training loss: 0.4059608639055659\n",
      "training loss: 0.2464683133967378\n",
      "training loss: 0.2890666503008288\n",
      "training loss: 0.37397722600426275\n",
      "training loss: 0.338896438579759\n",
      "training loss: 0.33465231569396564\n",
      "training loss: 0.3131662935225995\n",
      "training loss: 0.30622086777175356\n",
      "training loss: 0.3801246854209603\n",
      "validation_loss: 0.45472014380398523\n",
      "\n",
      "Epoch 69\n",
      "------------------------------\n",
      "training loss: 0.28638558477377957\n",
      "training loss: 0.33192251459739053\n",
      "training loss: 0.3958533940414054\n",
      "training loss: 0.3171808873061673\n",
      "training loss: 0.4285575105297266\n",
      "training loss: 0.3243751176155638\n",
      "training loss: 0.3359297442553361\n",
      "training loss: 0.3047983318139268\n",
      "training loss: 0.2648073074124204\n",
      "training loss: 0.34286748791495714\n",
      "training loss: 0.41104209928231283\n",
      "training loss: 0.2862077912094537\n",
      "training loss: 0.2584527235812857\n",
      "training loss: 0.31957606204789957\n",
      "training loss: 0.36171467678184854\n",
      "training loss: 0.4090094563001185\n",
      "training loss: 0.3553706850632625\n",
      "training loss: 0.30474295447565963\n",
      "training loss: 0.43187556406119254\n",
      "training loss: 0.31938018902144905\n",
      "training loss: 0.26882425464340487\n",
      "training loss: 0.3710227399266296\n",
      "training loss: 0.3457135144882704\n",
      "training loss: 0.32885493153738937\n",
      "training loss: 0.37229747301971655\n",
      "training loss: 0.3863115472331265\n",
      "training loss: 0.4920635689220421\n",
      "training loss: 0.4266461197845092\n",
      "training loss: 0.41106680325272466\n",
      "training loss: 0.3710896998954036\n",
      "training loss: 0.3477241695813154\n",
      "training loss: 0.24267431181287974\n",
      "training loss: 0.5067688680235825\n",
      "training loss: 0.3690877077236655\n",
      "training loss: 0.43983157018774366\n",
      "validation_loss: 0.4891545261737761\n",
      "\n",
      "Epoch 70\n",
      "------------------------------\n",
      "training loss: 0.31844835374111424\n",
      "training loss: 0.31267641463768997\n",
      "training loss: 0.3882016835500326\n",
      "training loss: 0.2887198012609224\n",
      "training loss: 0.3174918167720989\n",
      "training loss: 0.3936729963055404\n",
      "training loss: 0.3621015275383252\n",
      "training loss: 0.39313442925609704\n",
      "training loss: 0.45370640644589\n",
      "training loss: 0.3513311693900323\n",
      "training loss: 0.3859386829302298\n",
      "training loss: 0.2978267948623761\n",
      "training loss: 0.26977980626720865\n",
      "training loss: 0.32483494567459276\n",
      "training loss: 0.3429156446417619\n",
      "training loss: 0.27269680723082274\n",
      "training loss: 0.36276001136241576\n",
      "training loss: 0.36181082169045115\n",
      "training loss: 0.3438311739941514\n",
      "training loss: 0.3777456597820856\n",
      "training loss: 0.3203875530830919\n",
      "training loss: 0.4178476417473939\n",
      "training loss: 0.28156773843355265\n",
      "training loss: 0.3874215618635935\n",
      "training loss: 0.39550457210018064\n",
      "training loss: 0.3024136054291739\n",
      "training loss: 0.3557018419525502\n",
      "training loss: 0.3070573673764011\n",
      "training loss: 0.4306582941283705\n",
      "training loss: 0.2999190895395168\n",
      "training loss: 0.3070291580882622\n",
      "training loss: 0.48755111376012794\n",
      "training loss: 0.2743324135566968\n",
      "training loss: 0.3875875737118622\n",
      "training loss: 0.4278004780872288\n",
      "validation_loss: 0.4689789447729639\n",
      "\n",
      "Epoch 71\n",
      "------------------------------\n",
      "training loss: 0.3031408102325804\n",
      "training loss: 0.4427866770995024\n",
      "training loss: 0.45255060397383207\n",
      "training loss: 0.43585999142451326\n",
      "training loss: 0.29338259971330444\n",
      "training loss: 0.3099948674814368\n",
      "training loss: 0.31895903681266646\n",
      "training loss: 0.350325896882714\n",
      "training loss: 0.4057017764514603\n",
      "training loss: 0.31716896989848464\n",
      "training loss: 0.3394674986438622\n",
      "training loss: 0.401622928367799\n",
      "training loss: 0.2956726722678286\n",
      "training loss: 0.3756048557139002\n",
      "training loss: 0.3108328080560023\n",
      "training loss: 0.40862773025895877\n",
      "training loss: 0.24977882940020207\n",
      "training loss: 0.39721235931436466\n",
      "training loss: 0.33403984611766646\n",
      "training loss: 0.3667725665595208\n",
      "training loss: 0.3077396807052719\n",
      "training loss: 0.28674071712572186\n",
      "training loss: 0.35899417154403634\n",
      "training loss: 0.34351732902272486\n",
      "training loss: 0.2925885911746263\n",
      "training loss: 0.33171587645978434\n",
      "training loss: 0.3259217329262901\n",
      "training loss: 0.30969192435135484\n",
      "training loss: 0.36679725689204135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.49421353001875107\n",
      "training loss: 0.35115237079109646\n",
      "training loss: 0.44103435157685456\n",
      "training loss: 0.3975281703469227\n",
      "training loss: 0.38076212980318813\n",
      "training loss: 0.35205204806504525\n",
      "validation_loss: 0.461098458274469\n",
      "\n",
      "Epoch 72\n",
      "------------------------------\n",
      "training loss: 0.34359978302019045\n",
      "training loss: 0.3258770421311056\n",
      "training loss: 0.4305655466891403\n",
      "training loss: 0.28301981550139316\n",
      "training loss: 0.3729706058674856\n",
      "training loss: 0.2567330307571683\n",
      "training loss: 0.46060038272753445\n",
      "training loss: 0.32368015906631853\n",
      "training loss: 0.33662480997286365\n",
      "training loss: 0.27495945001166544\n",
      "training loss: 0.2832954677366433\n",
      "training loss: 0.35626835204260715\n",
      "training loss: 0.3070142547787782\n",
      "training loss: 0.48272168370162943\n",
      "training loss: 0.30111991393023346\n",
      "training loss: 0.37642817929583544\n",
      "training loss: 0.3177445250216465\n",
      "training loss: 0.29498844214564085\n",
      "training loss: 0.3448284812545171\n",
      "training loss: 0.3699557975594871\n",
      "training loss: 0.361442254401336\n",
      "training loss: 0.25963035111880345\n",
      "training loss: 0.2883733956892502\n",
      "training loss: 0.3905589620306273\n",
      "training loss: 0.32009583592648594\n",
      "training loss: 0.34954430409408815\n",
      "training loss: 0.2430213422075599\n",
      "training loss: 0.3240359707342577\n",
      "training loss: 0.4186506002099486\n",
      "training loss: 0.37621604219748406\n",
      "training loss: 0.5762945672695059\n",
      "training loss: 0.3811104976007573\n",
      "training loss: 0.4369375614282762\n",
      "training loss: 0.4924752755394002\n",
      "training loss: 0.31772627450831353\n",
      "validation_loss: 0.44458881195559535\n",
      "\n",
      "Epoch 73\n",
      "------------------------------\n",
      "training loss: 0.323181819629026\n",
      "training loss: 0.37139744609696207\n",
      "training loss: 0.33004002683912403\n",
      "training loss: 0.2930043841194856\n",
      "training loss: 0.3668374106842748\n",
      "training loss: 0.393539198867511\n",
      "training loss: 0.2991965901978256\n",
      "training loss: 0.3983773029409713\n",
      "training loss: 0.34137174934717224\n",
      "training loss: 0.4095438719313825\n",
      "training loss: 0.2707754942440079\n",
      "training loss: 0.3025705225036654\n",
      "training loss: 0.4207573510585644\n",
      "training loss: 0.32033716449415806\n",
      "training loss: 0.34779797554475406\n",
      "training loss: 0.3410963560221717\n",
      "training loss: 0.3521593838627814\n",
      "training loss: 0.359413021664368\n",
      "training loss: 0.5349648664933556\n",
      "training loss: 0.3004732754646284\n",
      "training loss: 0.3112915335194339\n",
      "training loss: 0.35945299819726645\n",
      "training loss: 0.37315053193506176\n",
      "training loss: 0.3717467873787973\n",
      "training loss: 0.31030162406823364\n",
      "training loss: 0.4017778501383873\n",
      "training loss: 0.3476519410131732\n",
      "training loss: 0.37614312984952447\n",
      "training loss: 0.23285205193657021\n",
      "training loss: 0.33164685527808613\n",
      "training loss: 0.4299986784604698\n",
      "training loss: 0.27183127154319664\n",
      "training loss: 0.42153366204221127\n",
      "training loss: 0.2663325295859249\n",
      "training loss: 0.4952917519287803\n",
      "validation_loss: 0.49393755045513427\n",
      "\n",
      "Epoch 74\n",
      "------------------------------\n",
      "training loss: 0.27102757219283374\n",
      "training loss: 0.30286338581464406\n",
      "training loss: 0.3053075343755336\n",
      "training loss: 0.38019785583659543\n",
      "training loss: 0.3476250708442967\n",
      "training loss: 0.3770974609142149\n",
      "training loss: 0.36870058380038245\n",
      "training loss: 0.34575731880971033\n",
      "training loss: 0.2978618745183485\n",
      "training loss: 0.3935823486223399\n",
      "training loss: 0.36004609802479537\n",
      "training loss: 0.40532469992454023\n",
      "training loss: 0.3548226642191935\n",
      "training loss: 0.3107461666069139\n",
      "training loss: 0.2840582999027902\n",
      "training loss: 0.543746346468688\n",
      "training loss: 0.44959231265354904\n",
      "training loss: 0.3294662640086608\n",
      "training loss: 0.2565368022133771\n",
      "training loss: 0.3674004401642742\n",
      "training loss: 0.3445182669197675\n",
      "training loss: 0.3864766698389576\n",
      "training loss: 0.3208651967649348\n",
      "training loss: 0.2773385067848085\n",
      "training loss: 0.39155248326169384\n",
      "training loss: 0.3575082829620442\n",
      "training loss: 0.33149286280269735\n",
      "training loss: 0.39254176308779276\n",
      "training loss: 0.43212913818570087\n",
      "training loss: 0.42738269807108736\n",
      "training loss: 0.37204626476188424\n",
      "training loss: 0.2945875093950599\n",
      "training loss: 0.3134270364792428\n",
      "training loss: 0.3719033412795397\n",
      "training loss: 0.326197860282939\n",
      "validation_loss: 0.4510883609164783\n",
      "\n",
      "Epoch 75\n",
      "------------------------------\n",
      "training loss: 0.2666923886197128\n",
      "training loss: 0.28946769429006963\n",
      "training loss: 0.4259356792484505\n",
      "training loss: 0.40952154903501653\n",
      "training loss: 0.3707911874677757\n",
      "training loss: 0.34261402981712535\n",
      "training loss: 0.2930772342088312\n",
      "training loss: 0.2952621228879525\n",
      "training loss: 0.42940826598919557\n",
      "training loss: 0.3761510640430788\n",
      "training loss: 0.3633741315114912\n",
      "training loss: 0.4125269023362875\n",
      "training loss: 0.2866018320102103\n",
      "training loss: 0.25482860238989813\n",
      "training loss: 0.3467494592262665\n",
      "training loss: 0.2789072749033312\n",
      "training loss: 0.34801953418204223\n",
      "training loss: 0.4343769379914738\n",
      "training loss: 0.3759763223030518\n",
      "training loss: 0.2776458906332209\n",
      "training loss: 0.35027053533082836\n",
      "training loss: 0.284576841112721\n",
      "training loss: 0.28590607639576776\n",
      "training loss: 0.3647821104651666\n",
      "training loss: 0.4261540332953155\n",
      "training loss: 0.407620248383264\n",
      "training loss: 0.3154372510961548\n",
      "training loss: 0.45825584248595985\n",
      "training loss: 0.2789118341432186\n",
      "training loss: 0.35686804954020773\n",
      "training loss: 0.45702630863699595\n",
      "training loss: 0.335641752724041\n",
      "training loss: 0.31353982456024826\n",
      "training loss: 0.4167893380550959\n",
      "training loss: 0.4769920972446562\n",
      "validation_loss: 0.45318518833153354\n",
      "\n",
      "Epoch 76\n",
      "------------------------------\n",
      "training loss: 0.43190299572575896\n",
      "training loss: 0.34270325197660895\n",
      "training loss: 0.3661534990470682\n",
      "training loss: 0.25453665929931046\n",
      "training loss: 0.35891667194504406\n",
      "training loss: 0.3322737207074533\n",
      "training loss: 0.4358859229229893\n",
      "training loss: 0.41646558193257077\n",
      "training loss: 0.5301257331004672\n",
      "training loss: 0.324427223761013\n",
      "training loss: 0.4028189807868421\n",
      "training loss: 0.30098569359397515\n",
      "training loss: 0.3733997267662562\n",
      "training loss: 0.3737487486118698\n",
      "training loss: 0.3410142362501938\n",
      "training loss: 0.28780845781264364\n",
      "training loss: 0.2512731437361799\n",
      "training loss: 0.35087018175414414\n",
      "training loss: 0.3089180337144626\n",
      "training loss: 0.3410219024245453\n",
      "training loss: 0.3649500776703644\n",
      "training loss: 0.36788580642431046\n",
      "training loss: 0.416844906166898\n",
      "training loss: 0.3499356177107984\n",
      "training loss: 0.40110700187673504\n",
      "training loss: 0.5144134313345421\n",
      "training loss: 0.35053361140297057\n",
      "training loss: 0.32519598514176323\n",
      "training loss: 0.2702527881324204\n",
      "training loss: 0.3573129666431305\n",
      "training loss: 0.2614786648687732\n",
      "training loss: 0.33441324148705465\n",
      "training loss: 0.27934312504847186\n",
      "training loss: 0.30500041801220507\n",
      "training loss: 0.39465582696975615\n",
      "validation_loss: 0.4574447976871338\n",
      "\n",
      "Epoch 77\n",
      "------------------------------\n",
      "training loss: 0.24063609457662097\n",
      "training loss: 0.3767910595091075\n",
      "training loss: 0.466987386373803\n",
      "training loss: 0.4515043785314265\n",
      "training loss: 0.3522820448857965\n",
      "training loss: 0.4185388098377734\n",
      "training loss: 0.3368217442122477\n",
      "training loss: 0.2681997340860835\n",
      "training loss: 0.4348780036413518\n",
      "training loss: 0.24593556367559358\n",
      "training loss: 0.3482495856156402\n",
      "training loss: 0.3800141512143091\n",
      "training loss: 0.4206931884364917\n",
      "training loss: 0.2739633577517816\n",
      "training loss: 0.38288563571048373\n",
      "training loss: 0.33190672480108335\n",
      "training loss: 0.3279483743704986\n",
      "training loss: 0.38766179762868885\n",
      "training loss: 0.36487471248488873\n",
      "training loss: 0.33921237664020737\n",
      "training loss: 0.3923573738322011\n",
      "training loss: 0.28321670886465655\n",
      "training loss: 0.26971137590808214\n",
      "training loss: 0.3262609342877113\n",
      "training loss: 0.2515330850526516\n",
      "training loss: 0.3987618753886636\n",
      "training loss: 0.4001064115791814\n",
      "training loss: 0.35631214578590514\n",
      "training loss: 0.36861506670791644\n",
      "training loss: 0.31108465939973257\n",
      "training loss: 0.3265983582657009\n",
      "training loss: 0.32612631528903874\n",
      "training loss: 0.4417194205686974\n",
      "training loss: 0.5238832260813979\n",
      "training loss: 0.34404100511532304\n",
      "validation_loss: 0.49075597223095396\n",
      "\n",
      "Epoch 78\n",
      "------------------------------\n",
      "training loss: 0.3860903992506883\n",
      "training loss: 0.37613006288440376\n",
      "training loss: 0.31336346678464905\n",
      "training loss: 0.3536612801364595\n",
      "training loss: 0.3868589126344887\n",
      "training loss: 0.33980698724705005\n",
      "training loss: 0.34665199961491455\n",
      "training loss: 0.3040373883102802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.2636025418433928\n",
      "training loss: 0.3567244303608095\n",
      "training loss: 0.29568461834369375\n",
      "training loss: 0.34091637036952305\n",
      "training loss: 0.30703479596952093\n",
      "training loss: 0.49106754927401197\n",
      "training loss: 0.36114122868224513\n",
      "training loss: 0.3210407818799467\n",
      "training loss: 0.34754856350424235\n",
      "training loss: 0.3670552212855364\n",
      "training loss: 0.2886426626442517\n",
      "training loss: 0.342323036444252\n",
      "training loss: 0.4899620214238166\n",
      "training loss: 0.3353100951174565\n",
      "training loss: 0.4326669640324326\n",
      "training loss: 0.40776214989973597\n",
      "training loss: 0.4678504124854953\n",
      "training loss: 0.3592180903100416\n",
      "training loss: 0.35622346170101993\n",
      "training loss: 0.2853045638347339\n",
      "training loss: 0.35447081275109665\n",
      "training loss: 0.37026240503575536\n",
      "training loss: 0.41633007818272744\n",
      "training loss: 0.2779013462041621\n",
      "training loss: 0.27079603265828156\n",
      "training loss: 0.3305110124803468\n",
      "training loss: 0.28841695225833974\n",
      "validation_loss: 0.5119313984490413\n",
      "\n",
      "Epoch 79\n",
      "------------------------------\n",
      "training loss: 0.2952124590360313\n",
      "training loss: 0.36005350820502824\n",
      "training loss: 0.3159416303067701\n",
      "training loss: 0.2945563308931742\n",
      "training loss: 0.3972035601799144\n",
      "training loss: 0.31255822281877044\n",
      "training loss: 0.3630485277046682\n",
      "training loss: 0.31465177362093755\n",
      "training loss: 0.29951342652086166\n",
      "training loss: 0.4643887667579111\n",
      "training loss: 0.22963521259815026\n",
      "training loss: 0.3245452578675031\n",
      "training loss: 0.41062599418695755\n",
      "training loss: 0.28900516132817755\n",
      "training loss: 0.4697033276609545\n",
      "training loss: 0.38096248417994505\n",
      "training loss: 0.43773396867229164\n",
      "training loss: 0.42596963071453503\n",
      "training loss: 0.33997357122645555\n",
      "training loss: 0.45714263952730105\n",
      "training loss: 0.344111113795625\n",
      "training loss: 0.2996297123427212\n",
      "training loss: 0.3856255714526924\n",
      "training loss: 0.36034446904264766\n",
      "training loss: 0.22506158074011182\n",
      "training loss: 0.3464583356166622\n",
      "training loss: 0.3242222779383883\n",
      "training loss: 0.48028372561733707\n",
      "training loss: 0.3596868390693271\n",
      "training loss: 0.4112111955882938\n",
      "training loss: 0.3089841389184585\n",
      "training loss: 0.3896550254297017\n",
      "training loss: 0.2753987634463556\n",
      "training loss: 0.3221386722639204\n",
      "training loss: 0.36546327682677654\n",
      "validation_loss: 0.4762304350952848\n",
      "\n",
      "Epoch 80\n",
      "------------------------------\n",
      "training loss: 0.28527931334320783\n",
      "training loss: 0.4266072409924982\n",
      "training loss: 0.40357546579281917\n",
      "training loss: 0.39662901064628386\n",
      "training loss: 0.3704693490815407\n",
      "training loss: 0.40597547193130595\n",
      "training loss: 0.3821464490768267\n",
      "training loss: 0.4414905922509206\n",
      "training loss: 0.3937627700732719\n",
      "training loss: 0.31124317934813006\n",
      "training loss: 0.27628036020152646\n",
      "training loss: 0.3169101814761234\n",
      "training loss: 0.32560878798056364\n",
      "training loss: 0.4518223045043123\n",
      "training loss: 0.3710252602056425\n",
      "training loss: 0.31746619624707817\n",
      "training loss: 0.3060305339556362\n",
      "training loss: 0.2718099892286591\n",
      "training loss: 0.4330216918453152\n",
      "training loss: 0.3521583676441514\n",
      "training loss: 0.3699325161991874\n",
      "training loss: 0.39201886016140636\n",
      "training loss: 0.2929702936672038\n",
      "training loss: 0.3057656536022478\n",
      "training loss: 0.21784508618973633\n",
      "training loss: 0.3019522738330852\n",
      "training loss: 0.4124104842824454\n",
      "training loss: 0.32406964405909094\n",
      "training loss: 0.33109778980255444\n",
      "training loss: 0.3658910045352604\n",
      "training loss: 0.22231536939085345\n",
      "training loss: 0.3057498867911568\n",
      "training loss: 0.38616157375429794\n",
      "training loss: 0.28203256998866894\n",
      "training loss: 0.4193389698417741\n",
      "validation_loss: 0.5228503290429973\n",
      "\n",
      "Epoch 81\n",
      "------------------------------\n",
      "training loss: 0.49928839725653235\n",
      "training loss: 0.28918882196666573\n",
      "training loss: 0.37326348110771507\n",
      "training loss: 0.2607801976923656\n",
      "training loss: 0.3573086115615752\n",
      "training loss: 0.31386638245010545\n",
      "training loss: 0.2650233308574025\n",
      "training loss: 0.4609631603506568\n",
      "training loss: 0.4051430836360669\n",
      "training loss: 0.31933989008648495\n",
      "training loss: 0.28510178408643694\n",
      "training loss: 0.38586715033525254\n",
      "training loss: 0.4218002819629328\n",
      "training loss: 0.3810514405017602\n",
      "training loss: 0.3581469960292452\n",
      "training loss: 0.2915799479809834\n",
      "training loss: 0.4251235865417402\n",
      "training loss: 0.2950427896257611\n",
      "training loss: 0.3294787306601574\n",
      "training loss: 0.3275500624728738\n",
      "training loss: 0.3722501744008332\n",
      "training loss: 0.3165149666385696\n",
      "training loss: 0.2812858016293467\n",
      "training loss: 0.27782761454116556\n",
      "training loss: 0.283498427249433\n",
      "training loss: 0.33500508202021595\n",
      "training loss: 0.3793204807707298\n",
      "training loss: 0.3441806630436622\n",
      "training loss: 0.339751746339698\n",
      "training loss: 0.349906908350531\n",
      "training loss: 0.37176030159535\n",
      "training loss: 0.27860370816473734\n",
      "training loss: 0.45589377045006585\n",
      "training loss: 0.5030588977961452\n",
      "training loss: 0.38593343576550976\n",
      "validation_loss: 0.4974420009535161\n",
      "\n",
      "Epoch 82\n",
      "------------------------------\n",
      "training loss: 0.3542638100154181\n",
      "training loss: 0.3439142803204686\n",
      "training loss: 0.3264623571174161\n",
      "training loss: 0.3207372876637692\n",
      "training loss: 0.3735706181839123\n",
      "training loss: 0.38790688999986744\n",
      "training loss: 0.2779754351871816\n",
      "training loss: 0.34212334518160786\n",
      "training loss: 0.37483237259904856\n",
      "training loss: 0.3331714883769382\n",
      "training loss: 0.3434910695186409\n",
      "training loss: 0.2513566900530714\n",
      "training loss: 0.5164263934831251\n",
      "training loss: 0.35537152372600755\n",
      "training loss: 0.3965766404168448\n",
      "training loss: 0.3504012738435995\n",
      "training loss: 0.3971245896394976\n",
      "training loss: 0.292939116331836\n",
      "training loss: 0.33302829216980173\n",
      "training loss: 0.26524905388906517\n",
      "training loss: 0.43432733891278075\n",
      "training loss: 0.3612957107683178\n",
      "training loss: 0.3876792394147924\n",
      "training loss: 0.3537791989807738\n",
      "training loss: 0.32597550166753536\n",
      "training loss: 0.27138522134569937\n",
      "training loss: 0.31097954434155556\n",
      "training loss: 0.4147571238594196\n",
      "training loss: 0.30644957837153924\n",
      "training loss: 0.32036974309841754\n",
      "training loss: 0.38700791014009156\n",
      "training loss: 0.470038403428307\n",
      "training loss: 0.30211142734556234\n",
      "training loss: 0.3564612410973859\n",
      "training loss: 0.42088387526397125\n",
      "validation_loss: 0.47399510834819764\n",
      "\n",
      "Epoch 83\n",
      "------------------------------\n",
      "training loss: 0.30796888532302547\n",
      "training loss: 0.28389832535445975\n",
      "training loss: 0.24575587176041153\n",
      "training loss: 0.4194037395835221\n",
      "training loss: 0.3331420208990312\n",
      "training loss: 0.32480494663190485\n",
      "training loss: 0.3869785924578264\n",
      "training loss: 0.38458587818737444\n",
      "training loss: 0.4484836674404687\n",
      "training loss: 0.36926400669077564\n",
      "training loss: 0.4114104974667862\n",
      "training loss: 0.33467248750639556\n",
      "training loss: 0.32880441048240755\n",
      "training loss: 0.44649807784186124\n",
      "training loss: 0.3126289058383554\n",
      "training loss: 0.46082610530292184\n",
      "training loss: 0.3633760318638997\n",
      "training loss: 0.36550152189825896\n",
      "training loss: 0.3039504342599685\n",
      "training loss: 0.30678737222551716\n",
      "training loss: 0.3546069795304379\n",
      "training loss: 0.2881479321097868\n",
      "training loss: 0.3378863869975612\n",
      "training loss: 0.36911348262514365\n",
      "training loss: 0.29267557537466926\n",
      "training loss: 0.41802153896722305\n",
      "training loss: 0.5392401970225024\n",
      "training loss: 0.22196723801364895\n",
      "training loss: 0.3473458365997067\n",
      "training loss: 0.29517578969007446\n",
      "training loss: 0.30251996550123295\n",
      "training loss: 0.31555652598419326\n",
      "training loss: 0.3651097554264197\n",
      "training loss: 0.3557098344477799\n",
      "training loss: 0.34849501564225877\n",
      "validation_loss: 0.44922857745271416\n",
      "\n",
      "Epoch 84\n",
      "------------------------------\n",
      "training loss: 0.28932616918704296\n",
      "training loss: 0.32671516774423254\n",
      "training loss: 0.40355479757505236\n",
      "training loss: 0.29558794511249287\n",
      "training loss: 0.3035100215501825\n",
      "training loss: 0.2196740618815238\n",
      "training loss: 0.339515815681234\n",
      "training loss: 0.2789056402859296\n",
      "training loss: 0.3233823074516954\n",
      "training loss: 0.33506322243452813\n",
      "training loss: 0.30341925211861964\n",
      "training loss: 0.39976249057861424\n",
      "training loss: 0.33606827413972495\n",
      "training loss: 0.35155437118010924\n",
      "training loss: 0.26789914377182866\n",
      "training loss: 0.2903826563682287\n",
      "training loss: 0.37013904372572143\n",
      "training loss: 0.39204025805076526\n",
      "training loss: 0.3307493280344352\n",
      "training loss: 0.34880280357814625\n",
      "training loss: 0.43941910759467645\n",
      "training loss: 0.381018633597796\n",
      "training loss: 0.3716329294388197\n",
      "training loss: 0.3998263915694406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.38211790452116473\n",
      "training loss: 0.2912444472018615\n",
      "training loss: 0.42698337678776627\n",
      "training loss: 0.29598425356007285\n",
      "training loss: 0.2934633249014587\n",
      "training loss: 0.41973541254518976\n",
      "training loss: 0.43205933423702847\n",
      "training loss: 0.4605664990697551\n",
      "training loss: 0.5267769808496087\n",
      "training loss: 0.3477157587514375\n",
      "training loss: 0.46588887639455834\n",
      "validation_loss: 0.4700180492192232\n",
      "\n",
      "Epoch 85\n",
      "------------------------------\n",
      "training loss: 0.3500328624028407\n",
      "training loss: 0.3496176479910355\n",
      "training loss: 0.24004684812302912\n",
      "training loss: 0.41572905871842525\n",
      "training loss: 0.36806082340658575\n",
      "training loss: 0.3452776340857963\n",
      "training loss: 0.5587707757520548\n",
      "training loss: 0.300382103799202\n",
      "training loss: 0.3647814011051014\n",
      "training loss: 0.31847310198067136\n",
      "training loss: 0.3062997307803016\n",
      "training loss: 0.3215722443230425\n",
      "training loss: 0.3469118546737445\n",
      "training loss: 0.3857705373681256\n",
      "training loss: 0.47456144579184184\n",
      "training loss: 0.35479980815311135\n",
      "training loss: 0.3357301108027241\n",
      "training loss: 0.30992564685860996\n",
      "training loss: 0.31349605507836714\n",
      "training loss: 0.3762095970589962\n",
      "training loss: 0.29715590835862715\n",
      "training loss: 0.3294812755298699\n",
      "training loss: 0.3015665329726835\n",
      "training loss: 0.3348781508830234\n",
      "training loss: 0.33280935261940614\n",
      "training loss: 0.31541366936356097\n",
      "training loss: 0.403250321543419\n",
      "training loss: 0.3853506258288689\n",
      "training loss: 0.3638696426004026\n",
      "training loss: 0.38721778796936634\n",
      "training loss: 0.3885001508335699\n",
      "training loss: 0.3629400267513938\n",
      "training loss: 0.35873177594336086\n",
      "training loss: 0.42547955435104085\n",
      "training loss: 0.3360023317715968\n",
      "validation_loss: 0.4754334903601997\n",
      "\n",
      "Epoch 86\n",
      "------------------------------\n",
      "training loss: 0.30181902680167694\n",
      "training loss: 0.30191401770949594\n",
      "training loss: 0.35632846810025515\n",
      "training loss: 0.5026247701725538\n",
      "training loss: 0.2822135783325575\n",
      "training loss: 0.283641053299516\n",
      "training loss: 0.28445332862808753\n",
      "training loss: 0.26039200929368234\n",
      "training loss: 0.31645397537358805\n",
      "training loss: 0.3302232246222502\n",
      "training loss: 0.3334816382839199\n",
      "training loss: 0.4117991495415481\n",
      "training loss: 0.37946476153098047\n",
      "training loss: 0.38917264372932775\n",
      "training loss: 0.4180137818248113\n",
      "training loss: 0.3060368019387829\n",
      "training loss: 0.35582124539043436\n",
      "training loss: 0.3950513098132797\n",
      "training loss: 0.4079897771432297\n",
      "training loss: 0.39979271493502894\n",
      "training loss: 0.2987679943383046\n",
      "training loss: 0.43635493835114175\n",
      "training loss: 0.36271615815153607\n",
      "training loss: 0.35225018124154306\n",
      "training loss: 0.422686660100444\n",
      "training loss: 0.31578681618979315\n",
      "training loss: 0.3667824116451811\n",
      "training loss: 0.3280791278870311\n",
      "training loss: 0.32426904817068136\n",
      "training loss: 0.39317783737356876\n",
      "training loss: 0.2769794576059212\n",
      "training loss: 0.3227523534101783\n",
      "training loss: 0.40964981527315103\n",
      "training loss: 0.3486902774279588\n",
      "training loss: 0.3903876159864012\n",
      "validation_loss: 0.5222693618434731\n",
      "\n",
      "Epoch 87\n",
      "------------------------------\n",
      "training loss: 0.3764318419227493\n",
      "training loss: 0.4015948459703941\n",
      "training loss: 0.26254428337560964\n",
      "training loss: 0.3269248176857309\n",
      "training loss: 0.5121087271837678\n",
      "training loss: 0.39605643326002793\n",
      "training loss: 0.3255240008429973\n",
      "training loss: 0.3122738905465303\n",
      "training loss: 0.30864935041689023\n",
      "training loss: 0.3585642431976157\n",
      "training loss: 0.2645986966107739\n",
      "training loss: 0.3676932988223507\n",
      "training loss: 0.26674909967376154\n",
      "training loss: 0.4161703468707219\n",
      "training loss: 0.3027075751708253\n",
      "training loss: 0.32626361398005427\n",
      "training loss: 0.29171559194750446\n",
      "training loss: 0.41749437020233016\n",
      "training loss: 0.2729712899617152\n",
      "training loss: 0.32306008283572735\n",
      "training loss: 0.3665083998963746\n",
      "training loss: 0.3258713148228162\n",
      "training loss: 0.21608609204296955\n",
      "training loss: 0.39744737873450503\n",
      "training loss: 0.31187180348141735\n",
      "training loss: 0.40969941833132906\n",
      "training loss: 0.378022737371648\n",
      "training loss: 0.40612344374490933\n",
      "training loss: 0.33333114900742655\n",
      "training loss: 0.4328960856600315\n",
      "training loss: 0.39148294744977646\n",
      "training loss: 0.49160295934401804\n",
      "training loss: 0.37188011462058057\n",
      "training loss: 0.32344381423507\n",
      "training loss: 0.46881372963522155\n",
      "validation_loss: 0.46156915513491564\n",
      "\n",
      "Epoch 88\n",
      "------------------------------\n",
      "training loss: 0.38467177119164264\n",
      "training loss: 0.3305273597314226\n",
      "training loss: 0.5246556590605178\n",
      "training loss: 0.3216662348996124\n",
      "training loss: 0.41199372089141434\n",
      "training loss: 0.33221076241099806\n",
      "training loss: 0.30983019261399247\n",
      "training loss: 0.3722666687702531\n",
      "training loss: 0.42828161095844736\n",
      "training loss: 0.28017441336687626\n",
      "training loss: 0.271041875296396\n",
      "training loss: 0.3008349366020775\n",
      "training loss: 0.33902638240659144\n",
      "training loss: 0.2619139534782153\n",
      "training loss: 0.325748110443692\n",
      "training loss: 0.3746028315855892\n",
      "training loss: 0.2942954707239278\n",
      "training loss: 0.4274162108684141\n",
      "training loss: 0.2881139505574538\n",
      "training loss: 0.3618050259714073\n",
      "training loss: 0.3813846526163979\n",
      "training loss: 0.3093960816343315\n",
      "training loss: 0.451003642719088\n",
      "training loss: 0.3184463418728501\n",
      "training loss: 0.3473508030373705\n",
      "training loss: 0.2972885078970285\n",
      "training loss: 0.3978060781685963\n",
      "training loss: 0.39753325119159855\n",
      "training loss: 0.2921191836762591\n",
      "training loss: 0.3732048951002798\n",
      "training loss: 0.33748302419890025\n",
      "training loss: 0.38860938447716764\n",
      "training loss: 0.4566471470717806\n",
      "training loss: 0.2837689401768148\n",
      "training loss: 0.34633835554759573\n",
      "validation_loss: 0.5128012329787823\n",
      "\n",
      "Epoch 89\n",
      "------------------------------\n",
      "training loss: 0.30682338062046255\n",
      "training loss: 0.4015305821896618\n",
      "training loss: 0.40669159431047774\n",
      "training loss: 0.3989437056874522\n",
      "training loss: 0.4271554567379644\n",
      "training loss: 0.3364369291668117\n",
      "training loss: 0.30429152064870324\n",
      "training loss: 0.42860317437010964\n",
      "training loss: 0.37988514226326514\n",
      "training loss: 0.39808127712574787\n",
      "training loss: 0.3140404861324441\n",
      "training loss: 0.2986475845759196\n",
      "training loss: 0.3337555508538935\n",
      "training loss: 0.38149083426506875\n",
      "training loss: 0.2660171305021868\n",
      "training loss: 0.2967240578485507\n",
      "training loss: 0.2939596825623084\n",
      "training loss: 0.3833275938712177\n",
      "training loss: 0.32271565898889093\n",
      "training loss: 0.3135146129693203\n",
      "training loss: 0.2998812408818412\n",
      "training loss: 0.3252723407797112\n",
      "training loss: 0.32210649440356065\n",
      "training loss: 0.35069370190482\n",
      "training loss: 0.23299075381146395\n",
      "training loss: 0.3074964808028835\n",
      "training loss: 0.3757851406237751\n",
      "training loss: 0.3614375212571758\n",
      "training loss: 0.4415213367829892\n",
      "training loss: 0.44642339706280837\n",
      "training loss: 0.3834913271339792\n",
      "training loss: 0.38502986480438267\n",
      "training loss: 0.42529464435494446\n",
      "training loss: 0.46817712818309704\n",
      "training loss: 0.27819240165350495\n",
      "validation_loss: 0.478595661191316\n",
      "\n",
      "Epoch 90\n",
      "------------------------------\n",
      "training loss: 0.44019280882188466\n",
      "training loss: 0.25995288573087466\n",
      "training loss: 0.32832480143699283\n",
      "training loss: 0.3070434774430032\n",
      "training loss: 0.24215086863758187\n",
      "training loss: 0.3994893113641592\n",
      "training loss: 0.3736134646199571\n",
      "training loss: 0.2664514665120805\n",
      "training loss: 0.37697359965783106\n",
      "training loss: 0.43150379062251887\n",
      "training loss: 0.4004523452338617\n",
      "training loss: 0.3204369792605576\n",
      "training loss: 0.31366786878756103\n",
      "training loss: 0.33710063499137505\n",
      "training loss: 0.39965516794880385\n",
      "training loss: 0.3375709345757059\n",
      "training loss: 0.3267104863557324\n",
      "training loss: 0.38912204755939456\n",
      "training loss: 0.35399721791401134\n",
      "training loss: 0.39306841703695683\n",
      "training loss: 0.3727144380122468\n",
      "training loss: 0.34764204235907525\n",
      "training loss: 0.3892213409463147\n",
      "training loss: 0.4261883402784042\n",
      "training loss: 0.3675389290935163\n",
      "training loss: 0.24142298093647696\n",
      "training loss: 0.2994964775735571\n",
      "training loss: 0.32976257292874833\n",
      "training loss: 0.34909178037392846\n",
      "training loss: 0.44040782761177977\n",
      "training loss: 0.48467729919668273\n",
      "training loss: 0.33998629727786467\n",
      "training loss: 0.30040035944534793\n",
      "training loss: 0.24513259519878375\n",
      "training loss: 0.3695810840379272\n",
      "validation_loss: 0.46380476322459036\n",
      "\n",
      "Epoch 91\n",
      "------------------------------\n",
      "training loss: 0.2763502542219794\n",
      "training loss: 0.34657590115006315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.31005680525559\n",
      "training loss: 0.3507084643132112\n",
      "training loss: 0.300594661892319\n",
      "training loss: 0.33899486775539117\n",
      "training loss: 0.36966771915673463\n",
      "training loss: 0.38852835627323656\n",
      "training loss: 0.302660908163316\n",
      "training loss: 0.32379401008976855\n",
      "training loss: 0.3554329016621341\n",
      "training loss: 0.31724258768161234\n",
      "training loss: 0.3737885468141758\n",
      "training loss: 0.33809085259330457\n",
      "training loss: 0.3827271787715381\n",
      "training loss: 0.40635805983381035\n",
      "training loss: 0.3726099872592249\n",
      "training loss: 0.39821285989994976\n",
      "training loss: 0.3422797203711616\n",
      "training loss: 0.39127579762938924\n",
      "training loss: 0.4386330697678932\n",
      "training loss: 0.39642285823269047\n",
      "training loss: 0.4024488035147169\n",
      "training loss: 0.2817948677949607\n",
      "training loss: 0.32648270512421734\n",
      "training loss: 0.3325396229973558\n",
      "training loss: 0.47231930422014556\n",
      "training loss: 0.37405234453988895\n",
      "training loss: 0.370646883087029\n",
      "training loss: 0.3133003278719207\n",
      "training loss: 0.3863283978627078\n",
      "training loss: 0.2837227184378571\n",
      "training loss: 0.41213607884608794\n",
      "training loss: 0.29231913246673685\n",
      "training loss: 0.37823842810001224\n",
      "validation_loss: 0.4632854199411951\n",
      "\n",
      "Epoch 92\n",
      "------------------------------\n",
      "training loss: 0.32753079609261476\n",
      "training loss: 0.3223348249642004\n",
      "training loss: 0.3064964553697064\n",
      "training loss: 0.4215935340875239\n",
      "training loss: 0.3716818730902742\n",
      "training loss: 0.3173658453229291\n",
      "training loss: 0.41857002419944367\n",
      "training loss: 0.3605646272008016\n",
      "training loss: 0.32757816545485186\n",
      "training loss: 0.4015985507937603\n",
      "training loss: 0.3414039672153831\n",
      "training loss: 0.28958791942019163\n",
      "training loss: 0.31995964845307756\n",
      "training loss: 0.3766813539431314\n",
      "training loss: 0.40141047174553024\n",
      "training loss: 0.3058282754346965\n",
      "training loss: 0.23334157500081346\n",
      "training loss: 0.27267455906148824\n",
      "training loss: 0.39919382700238204\n",
      "training loss: 0.4479259506452217\n",
      "training loss: 0.27346493466073296\n",
      "training loss: 0.5696996121119446\n",
      "training loss: 0.3711722888587815\n",
      "training loss: 0.3317459905458236\n",
      "training loss: 0.32095631147710263\n",
      "training loss: 0.3125192194519332\n",
      "training loss: 0.31382849525864004\n",
      "training loss: 0.2982619727638667\n",
      "training loss: 0.35582589500714673\n",
      "training loss: 0.41860767079082506\n",
      "training loss: 0.3608032741918942\n",
      "training loss: 0.335910670166204\n",
      "training loss: 0.3126070321301813\n",
      "training loss: 0.3467030187445653\n",
      "training loss: 0.4071789702100091\n",
      "validation_loss: 0.4743650193643691\n",
      "\n",
      "Epoch 93\n",
      "------------------------------\n",
      "training loss: 0.2767908770777649\n",
      "training loss: 0.42741472169364897\n",
      "training loss: 0.3331456512006116\n",
      "training loss: 0.3222213508782443\n",
      "training loss: 0.315880699846457\n",
      "training loss: 0.5071164475960381\n",
      "training loss: 0.4070307731922367\n",
      "training loss: 0.3923724200122888\n",
      "training loss: 0.26943122289332677\n",
      "training loss: 0.42881656869572\n",
      "training loss: 0.3532023025194212\n",
      "training loss: 0.33896387256485466\n",
      "training loss: 0.30398832460583436\n",
      "training loss: 0.4127382295714779\n",
      "training loss: 0.3685668076373986\n",
      "training loss: 0.2942696240155419\n",
      "training loss: 0.2980470080587838\n",
      "training loss: 0.33813603030273953\n",
      "training loss: 0.3140163502583437\n",
      "training loss: 0.3736738625606085\n",
      "training loss: 0.31842960907088125\n",
      "training loss: 0.37286706232305733\n",
      "training loss: 0.28742733069317183\n",
      "training loss: 0.32929749917893786\n",
      "training loss: 0.44685926306585316\n",
      "training loss: 0.36678383071362075\n",
      "training loss: 0.3229966844451883\n",
      "training loss: 0.3751657769916437\n",
      "training loss: 0.4263847661604086\n",
      "training loss: 0.39434035443096943\n",
      "training loss: 0.4404133282622206\n",
      "training loss: 0.36494455922096675\n",
      "training loss: 0.3186028273867123\n",
      "training loss: 0.3467643841209792\n",
      "training loss: 0.3114788944449174\n",
      "validation_loss: 0.45545316817217274\n",
      "\n",
      "Epoch 94\n",
      "------------------------------\n",
      "training loss: 0.3929398343732464\n",
      "training loss: 0.4036479350342415\n",
      "training loss: 0.32777443327352557\n",
      "training loss: 0.360190421821535\n",
      "training loss: 0.41671355745589606\n",
      "training loss: 0.26935172517682077\n",
      "training loss: 0.2588755842149112\n",
      "training loss: 0.3793833499585162\n",
      "training loss: 0.40022891972002983\n",
      "training loss: 0.45816038051729263\n",
      "training loss: 0.30734776732571845\n",
      "training loss: 0.39701009766053175\n",
      "training loss: 0.30110954029456477\n",
      "training loss: 0.3193871923740153\n",
      "training loss: 0.41927282285090767\n",
      "training loss: 0.22282046282693047\n",
      "training loss: 0.37491694407669457\n",
      "training loss: 0.30362221687930285\n",
      "training loss: 0.4379529130621813\n",
      "training loss: 0.25989954802677173\n",
      "training loss: 0.2724895247220411\n",
      "training loss: 0.32539667662087596\n",
      "training loss: 0.3204042990299058\n",
      "training loss: 0.3378752072977022\n",
      "training loss: 0.3300059851613173\n",
      "training loss: 0.37524803795284245\n",
      "training loss: 0.2799373791070343\n",
      "training loss: 0.4185943801069516\n",
      "training loss: 0.32994920735744016\n",
      "training loss: 0.4252621170016937\n",
      "training loss: 0.34423711020271186\n",
      "training loss: 0.42740898031610414\n",
      "training loss: 0.37129420623732584\n",
      "training loss: 0.3018152361153716\n",
      "training loss: 0.37144360149385647\n",
      "validation_loss: 0.456510359390554\n",
      "\n",
      "Epoch 95\n",
      "------------------------------\n",
      "training loss: 0.5005262019818838\n",
      "training loss: 0.31439973601512067\n",
      "training loss: 0.2820206047944157\n",
      "training loss: 0.3380736222557607\n",
      "training loss: 0.4284826758936561\n",
      "training loss: 0.26835963371457183\n",
      "training loss: 0.2679674254988822\n",
      "training loss: 0.36741456753955615\n",
      "training loss: 0.3491087940673242\n",
      "training loss: 0.38975222282388133\n",
      "training loss: 0.37198513337815714\n",
      "training loss: 0.48823703490528714\n",
      "training loss: 0.3515493585782315\n",
      "training loss: 0.33619861965227754\n",
      "training loss: 0.29542122219820155\n",
      "training loss: 0.3234340018648072\n",
      "training loss: 0.2912056543686049\n",
      "training loss: 0.3255541433441249\n",
      "training loss: 0.4604009739276444\n",
      "training loss: 0.23956239231847576\n",
      "training loss: 0.3667924411763761\n",
      "training loss: 0.41505725947441535\n",
      "training loss: 0.33065945212081715\n",
      "training loss: 0.3730484344459114\n",
      "training loss: 0.3339382029286935\n",
      "training loss: 0.34260240385118096\n",
      "training loss: 0.3209288649831274\n",
      "training loss: 0.2975952396372395\n",
      "training loss: 0.38708961070152326\n",
      "training loss: 0.33478409744478993\n",
      "training loss: 0.3755739093435113\n",
      "training loss: 0.4248946081365284\n",
      "training loss: 0.38167864490405917\n",
      "training loss: 0.3213899250762188\n",
      "training loss: 0.43082591286569366\n",
      "validation_loss: 0.4681454110691494\n",
      "\n",
      "Epoch 96\n",
      "------------------------------\n",
      "training loss: 0.40753489191927655\n",
      "training loss: 0.3595138149171544\n",
      "training loss: 0.2654680775928318\n",
      "training loss: 0.3553407221008183\n",
      "training loss: 0.4370256058704399\n",
      "training loss: 0.3851216715660485\n",
      "training loss: 0.32197154441208115\n",
      "training loss: 0.31520962298120137\n",
      "training loss: 0.3911176754982353\n",
      "training loss: 0.4503902222571014\n",
      "training loss: 0.2854391874009161\n",
      "training loss: 0.22805868220777484\n",
      "training loss: 0.3386577576236687\n",
      "training loss: 0.3290946291556611\n",
      "training loss: 0.33102475501750633\n",
      "training loss: 0.37108949624206616\n",
      "training loss: 0.4279353289169376\n",
      "training loss: 0.4117935727830627\n",
      "training loss: 0.32374898461290286\n",
      "training loss: 0.4107162996538682\n",
      "training loss: 0.2843062380046558\n",
      "training loss: 0.4583321315655485\n",
      "training loss: 0.31209179323926944\n",
      "training loss: 0.3044354697021117\n",
      "training loss: 0.2424121048082543\n",
      "training loss: 0.4511590839112614\n",
      "training loss: 0.43665612825636346\n",
      "training loss: 0.3570441324186686\n",
      "training loss: 0.25959712254571515\n",
      "training loss: 0.36111090303908894\n",
      "training loss: 0.39293083395947176\n",
      "training loss: 0.2725879191700915\n",
      "training loss: 0.33564762323709146\n",
      "training loss: 0.37140817717016944\n",
      "training loss: 0.4344322375791671\n",
      "validation_loss: 0.46965309210395445\n",
      "\n",
      "Epoch 97\n",
      "------------------------------\n",
      "training loss: 0.32519690415831976\n",
      "training loss: 0.35166954033908043\n",
      "training loss: 0.27455073925317264\n",
      "training loss: 0.2824191853225784\n",
      "training loss: 0.32446626186305366\n",
      "training loss: 0.34835606318055395\n",
      "training loss: 0.2912479038388301\n",
      "training loss: 0.34444930929807016\n",
      "training loss: 0.2596096179301094\n",
      "training loss: 0.26766451601171865\n",
      "training loss: 0.4299954394622182\n",
      "training loss: 0.44276023548067317\n",
      "training loss: 0.2988341406049585\n",
      "training loss: 0.4297975144875636\n",
      "training loss: 0.3326795742740069\n",
      "training loss: 0.31041634543973484\n",
      "training loss: 0.3780201094492736\n",
      "training loss: 0.3793069468678732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 0.3719724951678654\n",
      "training loss: 0.34243620591587387\n",
      "training loss: 0.3765300841379212\n",
      "training loss: 0.3748100918783166\n",
      "training loss: 0.27189848174806686\n",
      "training loss: 0.3142065347887001\n",
      "training loss: 0.38506311641442154\n",
      "training loss: 0.4699122280545998\n",
      "training loss: 0.37027900767179744\n",
      "training loss: 0.3815181815879714\n",
      "training loss: 0.3374509779875007\n",
      "training loss: 0.3555446217866938\n",
      "training loss: 0.4184436791713233\n",
      "training loss: 0.3220272686194403\n",
      "training loss: 0.3049047154755681\n",
      "training loss: 0.4146848573772513\n",
      "training loss: 0.4852713480901366\n",
      "validation_loss: 0.4937342887821704\n",
      "\n",
      "Epoch 98\n",
      "------------------------------\n",
      "training loss: 0.418073859065953\n",
      "training loss: 0.35478758764573287\n",
      "training loss: 0.4168351628930395\n",
      "training loss: 0.3143255485565169\n",
      "training loss: 0.42451350181683667\n",
      "training loss: 0.36861669076868564\n",
      "training loss: 0.3970632985484917\n",
      "training loss: 0.28406222048219204\n",
      "training loss: 0.3955060384686294\n",
      "training loss: 0.4269387992874545\n",
      "training loss: 0.319312061778046\n",
      "training loss: 0.45541407297889236\n",
      "training loss: 0.2907014269657611\n",
      "training loss: 0.38214801506285084\n",
      "training loss: 0.4763887246752347\n",
      "training loss: 0.40287365193711594\n",
      "training loss: 0.41388667861843714\n",
      "training loss: 0.30096468428429946\n",
      "training loss: 0.34029550781765466\n",
      "training loss: 0.2500573544571853\n",
      "training loss: 0.3265374574100133\n",
      "training loss: 0.23990687333323876\n",
      "training loss: 0.29747239414155274\n",
      "training loss: 0.3073537555942676\n",
      "training loss: 0.29368008863384604\n",
      "training loss: 0.37430112179435127\n",
      "training loss: 0.3654643789408146\n",
      "training loss: 0.37147758203966075\n",
      "training loss: 0.2937397878932188\n",
      "training loss: 0.29881387183326297\n",
      "training loss: 0.3682204129674301\n",
      "training loss: 0.31928188638765276\n",
      "training loss: 0.3601719474112906\n",
      "training loss: 0.43280713342770466\n",
      "training loss: 0.3357855107173964\n",
      "validation_loss: 0.44856203244266074\n",
      "\n",
      "Epoch 99\n",
      "------------------------------\n",
      "training loss: 0.37355144895700504\n",
      "training loss: 0.4632433939888142\n",
      "training loss: 0.3606799104281527\n",
      "training loss: 0.4330064501882589\n",
      "training loss: 0.3595191976523074\n",
      "training loss: 0.4109984814524523\n",
      "training loss: 0.19165699455428695\n",
      "training loss: 0.40270927465557177\n",
      "training loss: 0.3197507922171735\n",
      "training loss: 0.33696492595206107\n",
      "training loss: 0.29016076198022345\n",
      "training loss: 0.3056208589261587\n",
      "training loss: 0.35793887003288544\n",
      "training loss: 0.4477738317445619\n",
      "training loss: 0.25471983210138205\n",
      "training loss: 0.2886211194272619\n",
      "training loss: 0.41267152236810944\n",
      "training loss: 0.22994611899026496\n",
      "training loss: 0.435999147825969\n",
      "training loss: 0.3412807852705373\n",
      "training loss: 0.38500999079893516\n",
      "training loss: 0.32908339385183355\n",
      "training loss: 0.368332610819034\n",
      "training loss: 0.369237844178715\n",
      "training loss: 0.3301007005215797\n",
      "training loss: 0.46210487444540377\n",
      "training loss: 0.35662577640847304\n",
      "training loss: 0.3371709180681501\n",
      "training loss: 0.3103050011515097\n",
      "training loss: 0.29916182833025234\n",
      "training loss: 0.43616883284194047\n",
      "training loss: 0.3535270390168898\n",
      "training loss: 0.3347314902462313\n",
      "training loss: 0.3162186906094121\n",
      "training loss: 0.3100492269836832\n",
      "validation_loss: 0.49871806248490325\n",
      "\n",
      "Epoch 100\n",
      "------------------------------\n",
      "training loss: 0.40192387039864114\n",
      "training loss: 0.3471178308373783\n",
      "training loss: 0.3399006347433533\n",
      "training loss: 0.3993064845474146\n",
      "training loss: 0.3146130503321183\n",
      "training loss: 0.3940241590100868\n",
      "training loss: 0.2579438229742118\n",
      "training loss: 0.2990499583021392\n",
      "training loss: 0.34735790563589036\n",
      "training loss: 0.29709841553698424\n",
      "training loss: 0.3365638856755686\n",
      "training loss: 0.4400443034170894\n",
      "training loss: 0.3946495376948678\n",
      "training loss: 0.2416070680948269\n",
      "training loss: 0.41941613310627873\n",
      "training loss: 0.3580079281999497\n",
      "training loss: 0.31148102940118405\n",
      "training loss: 0.24280190496212525\n",
      "training loss: 0.42796886441821697\n",
      "training loss: 0.2673030760645634\n",
      "training loss: 0.2798678921967303\n",
      "training loss: 0.41688410367957657\n",
      "training loss: 0.48869804198398925\n",
      "training loss: 0.3947680311672775\n",
      "training loss: 0.4387695794802858\n",
      "training loss: 0.33005698526569177\n",
      "training loss: 0.4204362490895437\n",
      "training loss: 0.2798178056289362\n",
      "training loss: 0.28576577955934224\n",
      "training loss: 0.37050473975577913\n",
      "training loss: 0.3753375466651778\n",
      "training loss: 0.3066787935509637\n",
      "training loss: 0.390968948416994\n",
      "training loss: 0.38112458315287084\n",
      "training loss: 0.3741522363149852\n",
      "validation_loss: 0.4753697246078306\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Run for number of epochs\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n------------------------------\")\n",
    "    \n",
    "    # Shuffle data\n",
    "    perm = torch.randperm(len(X_train_v))\n",
    "    X = X_train_v[perm]\n",
    "    y = y_train_v[perm]\n",
    "    \n",
    "    # Train and compute validation loss\n",
    "    train(X, y, model, loss_fn, optimizer, writer)\n",
    "    loss = val(X_val_v, y_val_v, model, loss_fn, writer, name)\n",
    "    \n",
    "    global_epoch += 1\n",
    "    print()\n",
    "    \n",
    "    # Update best model\n",
    "    if min_loss is None or loss < min_loss:\n",
    "        min_loss = loss\n",
    "        print(\"New best model.\")\n",
    "        torch.save(model, f'models/NN_{name}.pth')\n",
    "        \n",
    "print(\"Done!\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b8e892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4409450034889775\n",
      "0.38665653370889447\n",
      "0.4201456846405136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noahtopper/miniconda3/envs/ml/lib/python3.9/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Get best models\n",
    "model3 = torch.load('models/NN_model3.pth')\n",
    "model3_650 = torch.load('models/NN_model3_650.pth')\n",
    "model3_700 = torch.load('models/NN_model3_700.pth')\n",
    "# Show their validation losses\n",
    "print(val(X_val_v, y_val_v, model3, loss_fn))\n",
    "print(val(X_val_v, y_val_v, model3_650, loss_fn))\n",
    "print(val(X_val_v, y_val_v, model3_700, loss_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e384b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6141846375907507"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test error\n",
    "best_model = model3_650\n",
    "predictions = best_model(X_test_v).detach().numpy()\n",
    "mse = mean_squared_error(predictions, y_test)\n",
    "mse = np.sqrt(mse)\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
